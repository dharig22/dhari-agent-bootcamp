Company Legal Name:,"Project Summary: Describe the key business problem(s) or opportunity this project addresses. Include your industry, target audience, and relevant market context (e.g., size, regulatory environment, etc.).","What are the key objectives and success milestones of the proposed solution (e.g., automate a manual process, build a new product, improve decision support) and its importance (consequences of not solving the problem/what would happen if this project is delayed or not implemented)?","What specific outputs or deliverables do you envision for this project? Examples may include: a functional prototype, API, dashboard, chatbot, backend system, pipeline, or internal tool. Please describe what you expect the MLAs to help deliver within the scope of the cohort.","Who will be the primary users of this solution? Please identify key internal or external user groups (e.g., analysts, customers, clinicians, etc.).",How will this solution integrate with your existing operations or product offerings?,Please describe the dataset(s) you intend to use for this project.,What is the current state of readiness of this dataset?,Does the dataset contain any Personally Identifiable Information (PII),Please describe how PII is handled or anonymized:,How do you plan to evaluate the model or system’s performance?,"Have you already explored or are you considering techniques associated with Generative AI or Large Language Models (LLMs) for this use case? If so, please specify which specific models or approaches used or are under consideration.","Are there any ethical, legal, or societal considerations related to the use of Generative AI or autonomous agents in this context? (e.g., fairness, bias, data privacy, transparency, user trust)","How will you measure success for this project? Please include any baseline metrics, KPIs, or qualitative outcomes you hope to observe.",What infrastructure or platforms do you plan to use to support the development of this project?,"In one or two sentences, describe the ideal MLA candidate for this project.","Please describe your technical team (if any). Include the number of staff and relevant roles (e.g., ML engineers, data analysts, product managers, etc.).",Is there any additional context or background information you would like to provide to help us evaluate your use case application?
Company 1,"Project Summary

Company 1 is pioneering a new model of sustainable living powered by AI, XR, robotics, and digital twin technology. Traditional housing models are wasteful, static, and disconnected from both the environment and the emerging tools that can make living spaces smarter, cleaner, and more adaptable. There is a growing demand for homes and communities that are sustainable, tech-enabled, and future-proof.

Our solution creates modular, eco-conscious living spaces designed and operated through immersive XR and AI platforms. Using digital twin technology, residents can visualize, customize, and simulate their home environments before they are built. Once deployed, every space integrates robotics, automation, and AI-driven controls, allowing individuals to manage lighting, climate, energy production, water systems, and security all remotely from anywhere in the world.

The global modular housing market is projected to surpass $140B by 2030, and XR/AI adoption is forecast to exceed $200B. Company 1 positions itself at the intersection of these movements, targeting environmentally conscious individuals, forward-thinking families, and innovators seeking to live smarter, cleaner, and more sustainably.

This project will demonstrate how immersive design, agentic AI, and digital twins can reimagine not only housing but the future of living itself. Beyond business, Company 1 is a movement teaching the world to embrace sustainable, intelligent lifestyles that merge technology, community, and clean living.","Key Objectives and Success Milestones

The key objective of Company 1’s solution is to create a new model of sustainable, intelligent living by integrating AI, XR, robotics, and digital twin technology into modular eco-homes. Our goals are to:

Build a new product: Develop a proof-of-concept modular home with full XR/AI-enabled design, digital twin simulation, and remote control of all integrated systems (energy, water, climate, lighting, and security).

Improve decision support: Empower individuals and communities to visualize and customize sustainable living spaces before construction, using generative AI and digital twins to optimize layouts, energy efficiency, and resource use.

Automate processes: Incorporate robotics and AI-driven automation to reduce waste, improve maintenance, and streamline daily living operations.

Establish scalability: Demonstrate a replicable model that can scale globally as a blueprint for sustainable, tech-enabled communities.

Success Milestones

Completion of the digital twin platform for immersive design and simulation.

Deployment of a prototype modular unit with integrated AI/robotics control systems.

Validation of energy and resource efficiency improvements versus traditional housing.

Pilot testing with early adopters, generating measurable data on user satisfaction, sustainability outcomes, and cost efficiency.

Importance and Consequences of Delay
Without this project, individuals and communities will remain tied to outdated, unsustainable housing models that waste resources and fail to leverage emerging AI/XR capabilities. Delaying implementation means missing a critical opportunity to prove that housing can be both sustainable and intelligent, widening the gap between environmental urgency and technological adoption. It would also slow Canada’s leadership in sustainable AI/XR innovation at a time when global demand for clean, tech-enabled living is accelerating.","Specific Outputs and Deliverables

For this project, the envisioned outputs will include both a functional prototype and supporting digital systems that demonstrate the integration of AI, XR, robotics, and digital twin technology into sustainable modular living. Within the scope of the cohort, we expect MLAs to help deliver:

Functional Prototype: A working proof-of-concept of a modular sustainable unit, represented as a digital twin, showcasing how spaces can be visualized, customized, and optimized in XR before being built.

AI-Driven Design Tool: A generative AI interface that allows users to simulate layouts, furnishings, and sustainable configurations in real time.

Control Dashboard / API Layer: A unified system for managing and remotely controlling all modular home functions (energy, climate, water, lighting, and security), built to scale into a broader smart-living platform.

Automation Modules: Integration of basic robotics and automation features for routine living operations (e.g., energy optimization, predictive maintenance alerts).

Data Pipeline: Collection and organization of simulation and usage data to measure sustainability outcomes, efficiency gains, and user engagement.

These deliverables will serve as the foundation for Company 1’s larger vision a replicable, intelligent, and sustainable living model that merges immersive XR, agentic AI, and clean technology.","Primary Users of the Solution  End Consumers / Residents: Individuals and families seeking sustainable, intelligent, and customizable living spaces. They will use the XR/AI design tools, digital twins, and smart-home systems to shape and manage their modular homes.  Hotels & Hospitality Providers: A key industry partner eager to deliver sustainable, tech-enabled guest experiences. Hotels can deploy modular units as showcases, allowing the public to experience AI, XR, and robotics firsthand before considering adoption in their own lives. This accelerates education and builds demand.  Sustainability Innovators & Early Adopters: Environmentally conscious users and communities who value clean living and want to demonstrate new models of eco-conscious housing.  Industry & Ecosystem Partners: Developers, architects, designers, and sustainability researchers who will use the platform and its data to inform future housing and urban planning models.  company 1 Internal Team: Founders, designers, and engineers who will test, deploy, and refine the system as the platform evolves.",New standalone tool or feature,"Datasets for the Project
1. Design & Sustainability Dataset
    Contents / Relevance: Architectural layouts, material specifications, energy usage benchmarks, and sustainable building standards relevant to modular construction. This dataset supports the creation of accurate digital twins and generative AI design recommendations.
    Source: Publicly available sustainability databases (e.g., LEED, Energy Star), modular housing case studies, and company 1’s curated internal design library.
    Collection & Format: Sourced from published standards, reports, and structured datasets in CSV/JSON formats. Some unstructured design files (CAD, BIM).
    Key Features: Floor plan dimensions, insulation types, HVAC efficiency ratings, solar/renewable energy options, embodied carbon values, material costs.
    Size: 5,000+ building material entries; 100+ modular case studies; 50–100MB of structured/tabular files.
2. User Interaction Dataset
    Contents / Relevance: Data on how users interact with XR design tools and digital twins choices of layouts, finishes, automation preferences. This will train generative AI to recommend sustainable, user-preferred options.
    Source: company 1 prototype sessions (XR/VR builds), survey inputs, and simulated user interaction logs.
    Collection & Format: Collected during design trials, stored as tabular logs (CSV) and event sequences (JSON).
     Key Features: User-selected layouts, energy system choices, automation preferences, interaction timestamps.
    Size: Early-stage dataset of 500–1,000 interactions; projected to scale to 50,000+ as user testing expands.
3. Environmental & IoT Dataset
    Contents / Relevance: Sensor and IoT data from modular living environments energy consumption, water use, climate control efficiency, occupancy patterns. Used to optimize robotics and AI control modules.
    Source: IoT APIs, open smart home datasets, and pilot modular unit deployments.
    Collection & Format: Structured time-series data collected via sensors; stored in CSV/Parquet files.
    Key Features: kWh consumption, water flow rate, temperature, humidity, occupancy metrics, device control logs.
    Size: Small-scale pilot data (weeks to months of logs,  (10–50MB). Projected to expand to multi-GB scale with full deployment.",Partially annotated,FALSE,,"We plan to evaluate the system’s performance through a combination of quantitative metrics and human-in-the-loop validation.

Golden Dataset / Benchmarks: We will curate a benchmark dataset combining sustainability standards, IoT sensor data, and XR interaction logs. This dataset will serve as a baseline for evaluating AI predictions and recommendations.

Human-in-the-Loop Validation: Designers, sustainability experts, and end-users (including test groups in modular housing and hospitality pilots) will review system recommendations in real-time to validate accuracy, usability, and practical relevance.

Heuristics and Business Rules: We will apply domain-specific rules (e.g., energy efficiency thresholds, safety standards, or sustainability compliance frameworks) to cross-check AI outputs.

Pre/Post Comparisons: Pilot deployments will track changes in energy consumption, cost savings, and user satisfaction before and after AI system integration. This will help us quantify direct impact.

Dashboards & Metrics Tracking: A real-time dashboard will monitor KPIs such as prediction accuracy, energy optimization scores, system uptime, and user engagement in XR interfaces.

Qualitative Indicators of Success: User feedback (e.g., from hotel operators, property owners, and guests) will be collected to measure trust, adoption, and perceived value of the system.

Success will be defined by measurable reductions in energy usage and operational costs, increased sustainability compliance, and high user satisfaction with XR-driven interaction and automation.","Yes – we are actively exploring Generative AI and LLM techniques to power company 1’s platform.

LLM Integration (Decision Support & Guidance): Considering models such as GPT-4, Claude, and LLaMA-based open-source variants to provide real-time design guidance, sustainability recommendations, and hospitality guest support. These models will be fine-tuned or prompted with domain-specific datasets (e.g., energy benchmarks, hospitality best practices).

Generative AI for Design & Simulation: Using diffusion models (e.g., Stable Diffusion, RunwayML Gen-2) for generating architectural layouts, interior design variations, and sustainable material visualizations within XR environments.

Multi-Modal AI: Exploring vision-language models (e.g., OpenAI CLIP, Gemini, LLaVA) to interpret XR interactions, IoT sensor data, and visual datasets, enabling AI-driven optimization of both digital twins and real-world modular homes.

Agentic AI: Evaluating frameworks where LLMs act as autonomous agents to coordinate smart home robotics, manage hospitality services (climate, lighting, appliances), and suggest sustainability improvements in real time.

We see these techniques as essential to bridging XR, robotics, and sustainability ensuring guests, homeowners, and hotel operators can interact naturally with advanced AI systems while improving energy efficiency and reducing environmental impact.","Yes, there are several ethical, legal, and societal considerations we are actively addressing in this project:

Fairness & Bias: Generative AI and LLMs can reflect biases present in training data. We will mitigate this through curated, diverse datasets and human-in-the-loop validation to ensure recommendations (e.g., for design, sustainability, or hospitality services) are inclusive and equitable.

Data Privacy & Security: Smart modular homes, hotels, and XR environments involve sensitive user data (energy usage, preferences, behavior). We will apply privacy-by-design principles, anonymization, and strict compliance with Canadian and international data regulations (e.g., GDPR, PIPEDA).

Transparency & User Trust: Users must understand how AI is influencing recommendations or controlling devices. We will integrate explainable AI features, dashboards, and transparent logging of system actions to build user confidence.

Autonomy vs. Human Oversight: While agentic AI can manage environments autonomously (e.g., energy optimization, robotics coordination), human override and consent will always be prioritized. Users will maintain full control of their spaces and data.

Sustainability & Societal Impact: Our use of AI is directly tied to reducing environmental impact by optimizing energy use, material selection, and lifecycle management. This aligns with global sustainability goals, ensuring AI deployment is a net positive for society and the planet.","We will measure success through a combination of quantitative KPIs and qualitative outcomes that reflect both technological performance and societal impact.

Baseline Metrics & KPIs

Energy Efficiency Gains: % reduction in energy and water consumption compared to traditional housing baselines.

Automation Impact: Accuracy and reliability of AI/robotics automation in managing climate, lighting, and maintenance tasks.

User Engagement: # of XR design sessions completed; average time spent in digital twin environments; % of design iterations optimized through AI recommendations.

Sustainability Compliance: Alignment with LEED/Energy Star benchmarks and reductions in carbon footprint of selected materials.

System Performance: Latency, uptime, and accuracy of AI-driven controls and predictive recommendations.

Qualitative Outcomes

User Trust & Satisfaction: Feedback from early adopters, residents, and hotel partners on usability, transparency, and comfort with AI-enabled living.

Adoption & Awareness: Evidence that hotels and hospitality partners can successfully showcase XR/AI/robotics to the public, increasing demand for sustainable living solutions.

Movement Building: Community interest and positive press around company 1 as a model for clean, intelligent living.

Overall Success Definition
Success will be defined by delivering a functional proof-of-concept modular unit + digital twin system that demonstrates measurable sustainability improvements, strong user adoption, and the potential to scale into both residential and hospitality markets.","Google Cloud Platform (GCP), Amazon Web Services (AWS), Microsoft Azure, Vector-provided infrastructure (if applicable), Company-hosted development environment (e.g., GitLab, JupyterHub), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Unity & Unreal Engine","The ideal candidate for this project would have experience with Generative AI, digital twins, and autonomous agent design, along with strong skills in data cleaning, model training, and integration of XR/robotics systems. A background in sustainability-focused AI applications and comfort working with cloud platforms (AWS, GCP, or Azure) would be highly valuable.","Currently, company 1 is led solely by myself as founder. I have been independently trained in Unity development, XR prototyping, and product innovation, and I have built the early-stage prototypes and vision on my own without a technical team. While I do not yet have dedicated staff, I am actively seeking to expand with ML engineers, data analysts, and product managers through programs like Vector Institute, accelerators, and future funding opportunities.","company 1 is a founder-led startup reimagining sustainable modular living through the integration of AI, XR, robotics, and digital twins. My vision is to create a new global standard for how people live, travel, and experience technology blending smart, eco-friendly housing with immersive, AI-driven interaction.

As a solo founder, I have independently built prototypes in Unity and developed the early foundation for this concept. However, the scope of this project requires advanced expertise in AI and machine learning that programs like Vector’s MLA can uniquely provide.

Participation in this cohort would not only accelerate the technical development of company 1 but also provide credibility and validation critical to securing partnerships with industry leaders, governments, and hospitality groups. This project is not simply a short-term business opportunity it is about pioneering a sustainable future of living and teaching the public how to adopt responsible, AI-driven technologies."
Company 2,"company 2 is an AI-driven mapping company. Our core product, RealSimE, is a cloud-based SAS platform that automates the generation of simulation-ready HD maps for Autonomous Driving (AD) and Advanced Driver-Assistance Systems (ADAS). The business challenge we are addressing is the cost, complexity, and manual effort involved in creating simulation environments at scale—an issue that limits both safety validation and rapid deployment in the autonomous systems sector.

The opportunity is rooted in the urgent market need for scalable, realistic, and customizable simulation environments. The global simulation market for AD/ADAS is expected to exceed USD $10B by 2030, driven by increasingly stringent regulatory and safety validation requirements. Our target audience includes automotive OEMs, Tier 1 suppliers, simulation platform providers, and governments/regulators. These stakeholders require high-fidelity environments to test driving policy, edge cases, and control logic—especially in dynamic, unstructured urban settings.","The primary objective of this project is to integrate Agentic AI into RealSimE, our simulation-ready HD mapping platform, to optimize accuracy, scalability, and interactivity. By deploying domain-specific agents, we aim to automate four key components in the simulation map creation process:
	1. Geometry corrections: auto-adjusting geometry and topology to fit real-world constraints.
	2. Semantics: enriching maps with contextual annotations like road rules and asset types.
	3. QA/QC Validation:  identifying and correcting inconsistencies in geometry or metadata.
	4. Edge Case Scenario Generation: creating synthetic, high-risk scenes for robust AV/ADAS testing.

Major milestone: 
	•	MVP deployment of multi-agent orchestration for map QA/QC 
If this project is delayed or not implemented, RealSimE risks falling behind industry expectations for automation and interactivity. Agentic automation is essential to meet these market demands, reduce operational bottlenecks, and maintain our competitive advantage as a next-generation simulation-enablement platform.","We envision three main deliverables resulting from this project, all aimed at advancing RealSimE into an agent-assisted simulation platform for AV/ADAS development. Specific outputs may include:
	1.	Autonomous Agent Framework:
A modular backend system with domain-specific AI agents capable of handling tasks such as semantic QA, scene editing validation, and edge-case generation.
	2.	Scene Intelligence API:
An internal or public-facing API that allows automated or user-queried semantic map assessments, such as detecting missing elements (e.g., crosswalks, signage) or generating suggestions for scene completion.
	3.	Edge Case Scenario Generator Module:
A tool that leverages agentic decision-making to synthesize high-risk driving situations (e.g., jaywalking VRUs at unmarked intersections)","Internal Users: 	•	GIS Developers – Validate and format simulation-ready maps with increased automation and speed. 	•	Cartographers / Annotation Specialists – Benefit from AI-assisted semantic labeling and reduced manual workload. 	•	Software Developers – Integrate domain-specific agents into RealSimE’s backend, streamlining workflows.  External Users: 	•	Simulation Engineers at Tier 1 Suppliers – Customize, test, and iterate scenes faster for AV/ADAS development. 	•	QA Teams at OEMs and Tier 1s – Perform automated quality checks to ensure scalable, accurate simulation map deployment.",New standalone tool or feature,"Dataset Content & Relevance
This project does not rely on image-based datasets. Instead, we will use internal geometric vector map datasets from our RealSimE platform. These maps are structured representations of road networks including lanes, intersections, signs, and traffic elements. The goal is to evaluate these maps against predefined simulation standards using AI agents—to detect errors (e.g., disconnected lanes, missing signals) and recommend scenarios for validation. This is highly relevant for automating quality control and test generation in AD/ADAS simulation workflows.
Source of the Dataset
The data comes from company 2’s internal simulation mapping pipeline. For this project, only the final vector map layers will be used—no aerial imagery or annotations are required.",Fully labeled for ML tasks,FALSE,,"To ensure the reliability and effectiveness of Agentic AI integration in RealSimE, we will use a multi-pronged evaluation approach:

 Golden Dataset for QA Validation

We will create a small set of manually verified simulation maps representing ideal geometries and standards. These “golden” maps will serve as a benchmark for validating the agent’s ability to detect geometry inconsistencies, identify violations, and propose corrections.

 Heuristic Rules Based on Simulation Standards

Agents will operate on embedded business rules and heuristics derived from domain-specific simulation guidelines (e.g., valid lane connectivity, presence of required traffic control elements). Evaluation will include accuracy in rule compliance and the rate of false positives/negatives.

Pre/Post Comparison of Edge Case Generation

To assess the agent’s impact on scenario generation, we will compare edge case coverage before and after agent integration:
	•	Number of edge cases generated per map
	•	Novelty and diversity of cases
	•	Time-to-generate edge cases

Human-in-the-Loop Verification

Cartographers and simulation engineers will review agent recommendations via a visual dashboard. Feedback will be used to:
	•	Score agent outputs (e.g., correct, incorrect, uncertain)
	•	Fine-tune agent behavior
	•	Track agreement rates over time

Performance Metrics & Dashboards

Success indicators will be monitored through internal dashboards showing:
	•	Reduction in QA/QC processing time (baseline vs. agent-assisted)
	•	Number of manual interventions required
	•	Agent recall and precision in error detection
	•	Scene completion rates","No, we have not yet explored or implemented Generative AI or Large Language Models (LLMs) for this use case. While we are aware of their potential, our current focus is on developing domain-specific agentic systems based on structured simulation standards and rule-based reasoning, rather than using generative models or LLMs.","No, we do not anticipate any ethical, legal, or societal concerns at this stage, as the system does not process personal data or sensitive user information. The autonomous agents will operate within clearly defined rule-based parameters derived from simulation standards, minimizing risks related to bias, fairness, or transparency.","We will measure success through a combination of quantitative KPIs and qualitative feedback from internal and external stakeholders.

Key Metrics & Baseline Comparison
	•	QA/QC Efficiency:
Baseline: 100% manual validation.
Target: ≥50% reduction in human time spent on geometry validation tasks.
	•	Semantic Labeling Automation:
Baseline: Fully manual annotation by cartographers.
Target: Automate ≥60% of semantic layer generation using agents.
	•	Scenario Generation Time:
Baseline: Several hours of manual review and scene crafting.
Target: Generate valid edge-case scenarios in under 10 minutes using agentic queries.",Amazon Web Services (AWS),"The ideal candidate for this project would have experience with autonomous agents, rule-based validation systems, and simulation data workflows. Familiarity with geospatial data structures, QA automation, and integration of intelligent agents into complex pipelines (e.g., for scene editing or semantic reasoning) is highly desirable. Knowledge of AD simulation software like CARLA and Unreal Engine is a strong asset.","Our technical team consists of 11 full-time staff including:
	•	1 Software Developer specializing in backend systems and cloud infrastructure
	•	2 GIS Developers working on geospatial data processing pipelines
	•	1 Computer Vision Engineer focused on AI-driven feature extraction and semantic segmentation
	•	4 Cartographers/GIS Technicians supporting annotation, QA, and data validation
	•	2 Founders (CTO and CEO) with extensive backgrounds in geospatial AI and simulation
	•	1 Product Designer supporting UX and system integration",
Company 3,"Project Summary: Describe the key business problem(s) or opportunity this project addresses. Include your industry, target audience, and relevant market context (e.g., size, regulatory environment, etc.).
Our organization trains nurses and pharmacists for their licensing exams. We wish to create tools for managing, improving and revising large volumes of questions -- which is a significant problem in the medical education space.

The reason for this problem are manifold

RECENT TRENDS FORCE INSTITUTIONS TO MAINTAIN LARGE INVENTORIES OF QUESTIONS


This is true for a variety of reasons: (1) computer adaptive tests (CATs), where the difficulty of the test changes while you do the test are increasingly being used. But in a CAT if the student sees 75 questions and you have 10 possible difficulty levels, you need 750 questions (2) many institutions and standardized tests allow students to do tests at home using their own computers; but even with remote proctoring, the questions are considered compromised when used, necessitating new ones to take their place (3) owing to changes in the nursing and pharmacy exams,  many new questions need to be created. For example, in the nursing licensing exam they have 23 different types of questions with multiple choice being just 1 of them, with 13 of those types never seen before in any other test (which means institutions cannot previously have a supply of them); and (4) some US states have laws against standardized exams from external vendors (as the reused questions are considered compromised) necessitating institutions to create their own questions.

THESE LARGE QUESTION BANKS BECOME UNWIELDY TO MAINTAIN
In medicine nothing is static. The way a disease is supposed to be treated is defined by clinical practice guidelines which tend to continually change. In an average month 2 to 5 clinical practice guidelines will change and this necessitates changes to course notes, questions, flashcards and other educational materials. In addition the needs of exams change in a phenomenon some call “drift” whereby the exam makers will gradually make changes such as raising the level of difficulty or the level of thinking required (as classified by a theory called “Bloom’s taxonomy”). But when these changes happen there comes a need to review all questions to make sure they conform with these changes.

PURPOSE OF PROJECT AND OPPORTUNITY

The purpose of this project is to create a tool for college and university faculty as well as standardized testing agencies to identify which questions need to be changed, and to help them implement those changes and to improve those questions for better student outcomes. 

Among the criteria the system will use to identify questions that are candidate for change are (1) conformance to a learning objectives, (2) clarity (3) fairness (e.g. one clear answer) (4) non-obvious answers (5) conform to the latest clinical guidelines (6) conformance with exam formatting and other exam requirements (6) are tagged correctly according to a hierarchical taxonomy (7) test the right level of thinking (as classified by Bloom’s Taxonomy). 

Once a question has been identified that it needs to be changed, recommended actions will be provided to the user on how to improve the question. Additionally the human can direct the AI to change certain aspects of a question; for instance, by  highlighting an answer choice and indicating to the AI that the answer choice is “too obvious” to rework the question to make the answer choice less obvious. Additionally, the tool will identify gaps in the questions to achieve the hierarchical curriculum learning objectives (e.g. by reporting that “there is no question that tests this aspect of your curriculum”)

Currently no such tool exists in the market, and human judgement alone is being relied on. This tool will assist humans to become at least 10 times more productive by approving the AI’s recommendations while also directing the AI (for example, by highlighting a question choice and indicating to the AI that the answer choice is too obvious, and requesting that it be re-written).

MARKET SIZE

Nurses in Canada and the USA share a common licensing exam called the NCLEX. The market size for nursing licensing exam prep is estimated to be over USD $418 million and has grown between 9.5% (2023) to 15% (2022) annually (the 2 latest years available) and is expected to accelerate as existing nurses are mass-retiring and governments are making investments to increase nursing enrollment to make up for the gap. The nursing education market is estimated to be $6.8 billion annually, and this would be a segment of that. The pharmacy licensing exam prep market is estimated to be $74 million in Canada and the USA growing at about 15% annually.

PAST PROJECT


This work builds on our past work in:


Automated question writing and associated patents
Past Vector MLA (as part of cohort 8) that resulted in a working remediation system and patent application 
Identifying strengths and weaknesses",Build a prototype that will result in a new product. If this product is not implemented students will not have optimal outcomes and faculty will simply not be able to manage their inventories of questions (which often number in the thousands to tens of thousands).,"We envision a prototype being built that via API accesses our existing inventory of 15,000 questions, download the clinical practice guidelines and to write a series of ""detectors"". Each detector will detect a if a specific issue (for instance, non-conformance with the latest clinical treatment guidelines) is present in a question, explain the reason, and the suggested change (similar to how a spelling or grammar check helps a person quickly become aware of changes needed to a document, but the type of changes and issues detected would be much broader). A question may have many reasons necessitating that it be updated. These reasons form the basis for prioritizing which questions need to be changed first. This project would result in the full gamut of identify and fixing and result in a user interface suitable for use by faculty.",College and university faculty and our own internal use at company 3 and NurseAchieve,Enhancement to an existing system,"1. QUESTIONS

We have a bank of 15,000 questions in pharmacy and nursing. They are all the original work of our pharmacists and nurses who work at company 3 and NurseAchieve. 

The questions data exists in XML and is also available as JSON via an API. 

Features include:
-the first part of the question (called the question stem)
-answer choices
-whether each answe choice is right or wrong
-detailed rationale (which includes learning objective, background, why each answer choice is right or wrong, and references)

2. QUIZ DATA

We have grades on individual questions, and quizzes for about 100,000+ students, including the dates they were administered as well as the privacy permissions to collect non-identifiable statistics. 

Each quiz consists of the questions. Associated with questions are attempts, which include the student’s grade on each question (ranging from 0 to 1) as well as the date/time each question was attempted.

Each question has a unique ID and is classified into topics by being associated with tags.  Although tags are not inherently hierarchical we have made our tags hierarchical by strictly tagging our questions based on the first and second level of multiple topic hierarchies. For example, a question may be tagged with the topic “Cardiovascular Disorders” indicating it is a Cardiovascular question as well as being tagged with the sub-topic “Heart Failure”.  We tag questions to classify them in multiple topic hierarchies including:

* System (e.g. Endocrine)

* Concern (e.g. Diabetes) - These function like topics and sub-topics

* Examination Board Test Plan Areas - this is a 3 level hierarchy with approximately 6 level-2 topics, 76 level-3 topics and over 700 level 4 topics.

* Bloom’s Taxonomy Level (Bloom Taxonomy is a system used to categorize the level of thinking required to solve a problem, with the lowest level being able to recall facts without understanding them, and the highest level being to able to create something new).

* Exam (the specific exam it is for in pharmacy or nursing e.g. PEBC Evaluating Exam, PEBC MCQ, NCLEX, REx-PN)

* Author (who wrote the question)


The grading data is used to calculate specialized statistics for testing; e.g. 

-Difficulty index (this is the percentage of students who got the question wrong)",Fully labeled for ML tasks,FALSE,,Human in loop,"Yes, fine tuning, prompt engineering. We use Gemma3, MedGemma, Gemini 2.5 Pro and ChatGPT.",No,"Qualitative - that our staff use it internally to surveil and update questions.  
Quantitative - number of questions they update using the tool.","Google Cloud Platform (GCP), Amazon Web Services (AWS), Internal on-premise or virtual private servers, Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Company-hosted development environment (e.g., GitLab, JupyterHub), Microsoft Azure","Experience with LLMs. Can write good prompts. Can write good UIs (e.g. using React). Good communication skills. Creativity.

We had a good experience in the previous MLA cohort and hired on of our MLAs full time to join our AI team.","3 AI Engineers. We also have 5 other Computer Scientists working on our platform, but they do not work on ML except to provide needed APIs consumed by the ML.","Our Co-Founder/CTO,  has been working in probabilistic text-based machine learning since 2008 (when he was a Research Scholar in Machine Learning at CMU), and went on to get 2 MIT Masters while serving as an RA at the MIT CSAIL and Harvard Med School with his masters thesis being on how to de identify data while maximizing its usefulness for training ML models. He is named as an inventor or cited in 15 US patents including those associated with Google, Twitter, IBM. We have leveraged the in-house expertise of 3 in-house AI engineers. Our team has 7 US Patent submissions in progress, including 1 recently granted patent representing the state-of-the-art in medical question writing. Our previous MLA project also resulted in a prototype and a 104 page US patent application."
Company 4,"company 4 is tackling a major trust and product-discovery gap in the direct-to-consumer (DTC) e-commerce industry, a market projected to exceed $250 B globally by 2026. Today, authentic product reviews are locked inside individual Shopify and DTC storefronts, fragmented and inconsistent in quality. Shoppers struggle to confidently compare products across brands, while independent merchants pay rising acquisition costs to compete with the recommendation engines of dominant marketplaces like Amazon.

Our opportunity is to create the first cross-brand review search engine powered by Generative and Agentic AI. company 4 aggregates millions of customer reviews, applies retrieval-augmented generation (RAG) and multi-agent reasoning to summarize sentiment and surface trusted product recommendations in a conversational interface.

Target users are online shoppers seeking authentic guidance and DTC merchants who need a new, high-trust discovery channel. By enabling transparent, data-driven product discovery, company 4 lowers acquisition costs for brands and gives consumers a trusted, AI-native alternative to closed marketplace ecosystems.","The primary objective is to build and validate a cross-brand review intelligence engine that transforms fragmented Shopify and DTC reviews into trusted, AI-generated product recommendations. Our goals include:

Automate unstructured review analysis – convert millions of raw customer reviews into structured sentiment insights using RAG and agentic multi-agent reasoning.

Deliver a conversational discovery experience – enable shoppers to query for the “most recommended” products across brands with explainable, transparent AI responses.

Provide brands with actionable analytics – surface competitive sentiment and category trends to reduce acquisition costs and improve product strategy.

Success Milestones

Completion of a production-ready proof of concept demonstrating real-time retrieval and multi-agent recommendation.

Pilot launch with a select group of Shopify merchants and at least 1,000 active consumer queries.

Documented uplift in shopper trust and measurable decrease in brand customer acquisition cost.

If this project is delayed, small independent DTC brands remain disadvantaged, forced to rely on expensive ad networks and opaque marketplaces, while shoppers continue to face information overload and unreliable reviews, leaving a growing gap for trusted, AI-driven commerce discovery.","Building on the proof of concept successfully developed in the Vector Institute’s DaRMoD Cohort 5—where we completed data readiness, model development, and model deployment—our FastLane project will focus on taking the next step from prototype to a scalable, real-time platform. Key deliverables include:

Functional prototype enhancement: Expand the existing Streamlit conversational search interface into a production-grade experience that supports broader categories of DTC products, higher query volumes, and seamless integration with the company 4 Reviews Shopify app.

Real-time review data access: Create a live data pipeline that continuously ingests and updates customer reviews across Shopify-powered online stores, ensuring up-to-the-minute product insights and recommendations.

Retrieval-augmented generation (RAG) pipeline: Harden the backend for large-scale ingestion, de-duplication, and multi-agent reasoning to produce trustworthy, explainable product recommendations.

Specialized Small Language Model (SLM): Lay the foundation for a domain-specific SLM purpose-built for review collection and search chatbots, enabling scalable deployment across the wider DTC e-commerce landscape beyond Shopify.

Analytics dashboard/API: Provide DTC merchants with live competitive sentiment analytics and actionable insights.

We expect the Machine Learning Associates (MLAs) to drive development of the real-time ingestion layer, assist in early SLM architecture experiments, and optimize the RAG infrastructure and evaluation metrics to ensure accuracy, scalability, and trustworthiness for commercial rollout.","* Online Shoppers (External) – Consumers looking for trusted, cross-brand product recommendations. They will use company 4’s conversational search interface to quickly compare products and understand authentic sentiment across multiple DTC brands.  * DTC Merchants & Shopify Store Owners (External) – Independent e-commerce brands seeking greater product visibility and actionable insights. They will access the company 4 Reviews Shopify app and accompanying analytics dashboard to monitor competitive sentiment, track their own review performance, and engage new customers through company 4’s discovery engine.  * Internal Product & Data Teams (Internal) – company 4’s own engineers and data scientists, who will maintain the real-time review ingestion pipeline, fine-tune the specialized Small Language Model (SLM), and ensure the quality and reliability of AI-generated recommendations.  These user groups collectively drive the marketplace effect—shoppers benefit from transparent recommendations while merchants gain a high-trust acquisition channel—forming the core ecosystem company 4 aims to scale.",Enhancement to an existing system,"- Content & Relevance:
In our initial proof of concept (completed in Vector’s DaRMoD Cohort 5), we utilized a recent Amazon reviews dataset focused on the Beauty products category to validate our retrieval-augmented generation (RAG) and multi-agent reasoning approach. These reviews provided authentic, large-scale customer sentiment data—directly relevant to building a cross-brand product-recommendation engine.

- Sources:
The dataset was drawn from publicly available Amazon product review corpora (Beauty category) and processed in compliance with data-use terms. For the FastLane project we will expand our training corpus to include reviews from 5–7 major e-commerce categories (e.g., Beauty, Home, Electronics, Apparel, Health & Wellness, Sports, and Pet Supplies).

- Collection Method & Format:
Data is acquired from open Amazon review datasets and stored in structured JSON/CSV format with raw text fields for review bodies and metadata.

- Key Features / Fields
*Review text and review title
*Star rating and average product rating
*Product and category metadata (name, SKU, category)
*Review timestamp and language
*Helpful vote (Yes/No) and Verified purchase (Yes/No)
*Reviews count per product
*Anonymized reviewer identifier (non-PII)

- Approximate Size:
The next training phase targets at least ~350 million reviews across these categories (hundreds of GB of text and metadata).

This expansion will enable multi-category model training and lay the foundation for a specialized Small Language Model (SLM) to power cross-category review discovery and review-search chatbots.",Partially annotated,FALSE,,"To ensure both technical accuracy and real-world impact, we will use a multi-layered evaluation approach that leverages agentic AI to scale search testing:

- Golden Dataset & Benchmarking – Maintain a held-out “golden set” of Amazon and Shopify reviews, manually labeled for sentiment, category, and key product attributes. This benchmark will drive precision/recall, F1 score, and ranking-quality assessments.

- Agentic AI–Driven Search Testing – Deploy autonomous AI agents to simulate diverse shopper queries and automatically test retrieval quality across millions of reviews. These agents continuously probe the system, stress-testing real-time search and recommendation accuracy at scale.

- Human-in-the-Loop Validation – Domain experts and pilot DTC merchants will review AI-generated summaries and recommendations for factual accuracy, bias, and clarity.","Yes. company 4 has already explored and implemented multiple Generative AI and Large Language Model (LLM) techniques in developing our review-intelligence platform. During the Vector DaRMoD Cohort 5, we successfully built and deployed a proof of concept using retrieval-augmented generation (RAG) and agentic AI workflows to summarize and rank large-scale e-commerce reviews.

We have tested and continue to evaluate a range of frontier and open-source models, including Google Gemini 2.5 Pro, Flash LLMs for low-latency tasks, and Meta’s LLaMA-3 family for fine-tuning. Our next phase involves experimenting with specialized Small Language Models (SLMs) trained on a multi-category corpus (~350 million Amazon reviews) to create domain-specific review search and recommendation chatbots.","company 4 is intentionally designing its generative and agentic AI systems to address key ethical, legal, and societal issues:

- Data Privacy & Consent – All review data is sourced from publicly available or merchant-authorized datasets (e.g., Amazon open data, Shopify merchants via the company 4 Reviews app). We ingest only non-PII text, apply strict access controls, and comply with applicable privacy regulations (e.g., Canadian PIPEDA, GDPR for EU users).

- Fairness & Bias Mitigation – We actively monitor models for category or brand bias by using balanced training corpora and performing regular bias audits on generated recommendations and sentiment summaries.

- Transparency & User Trust – AI-generated summaries and recommendations will be clearly labeled as machine-generated, with explainable reasoning (e.g., surfaced key review signals) to build shopper confidence.","We will measure success through a mix of technical, user, and business outcomes:

- Model Accuracy & Retrieval Quality – Achieve ≥85% F1 score on the held-out golden dataset for sentiment classification and product-category matching. Track mean reciprocal rank (MRR) and precision@k to confirm high-quality cross-brand recommendations.

- Agentic AI Search Performance – Using agentic AI–driven automated search testing, target >90% successful query coverage and maintain real-time retrieval latency <1.5 seconds.

- User Engagement & Trust – Baseline: pilot testers average 1–2 queries/session. Goal: ≥3 queries/session and Net Promoter Score (NPS) > 30 by the end of the cohort.

- Merchant Impact – Demonstrate at least a 15% reduction in customer acquisition cost and improved review engagement metrics (e.g., higher helpful votes on surfaced reviews) for pilot DTC brands.

- Foundation for SLM – Establish measurable progress toward training a multi-category Small Language Model (SLM), including completing data ingestion of ≥350 M reviews and producing an initial evaluation report on SLM performance.","Google Cloud Platform (GCP), Amazon Web Services (AWS), Company-hosted development environment (e.g., GitLab, JupyterHub)","Have strong experience in large language models (LLMs) and agentic AI workflows, including prompt engineering, retrieval-augmented generation (RAG), and real-time data pipeline development. Familiarity with training or fine-tuning small language models (SLMs), scalable cloud infrastructure (GCP/AWS), and data cleaning of large-scale e-commerce review datasets is highly valued.","company 4’s core technical team consists of four dedicated members:

Founder & CEO– Leads product vision, technical strategy, and AI roadmap, with hands-on experience in LLM integration and retrieval-augmented generation (RAG).

Machine Learning Engineer (1) – Specializes in large language models, agentic AI workflows, and real-time data pipelines for multi-category review ingestion and search.

Full-Stack Developers (2) – Responsible for backend APIs (FastAPI), cloud infrastructure (GCP/AWS, Docker/Kubernetes), and front-end interfaces (Streamlit, Next.js/React).

Data Analyst (1) – Focuses on dataset preparation, large-scale review data cleaning, and evaluation metrics.",
Company 5,"- Key business problem: Government procurement employees require specialized knowledge to navigate challenges related to compliance, policies, media scrutiny, shortage of staff, legal cases, audits, proving productivity, & delivering goals with limited support.  Meeting compliance requirements towards spending taxpayer's dollars wisely is critical in Government procurement which is a $9.5 Trillion industry. Else, it leads to audits, negative media coverage, and job losses at the cost of taxpayer's money.
- Industry: GovTech AI SaaS
- Target Audience: Procurement department of Government organziations in Canada (Federal, Provincial, Municipal)
- Relevant Market context: (1) Behavioral: Govt employees are asked to submit proof of their productivity., (2) Regulatory/Legislative: U.S. Govt. is using AI tools for $1 a year from Open AI & Anthropic, (3) Heat in this space: Startups such as Procurement Sciences raised $10 Mn in Series A.","- Key objectives: Build a new product that offers Productivity savings of up to 90% in time and 50% in cost in Government procurement documentation
- Importance: Current solutions offer technical solutions and do not focus on content quality. This makes solutions hard to understand, less practical, and contain technical legal references that are not useful to users. This impacts ability of target users to conduct Government procurement within the time, cost, and quality and leads to inefficient and ineffective use of taxpayer's money.","(1) Functional prototype for pilot testing with customers, (2) Leverage feedback from pilot testing to build a Beta version of the product for formal launch and testing with customers and prove that the technology works successfully in real life.","Primary users: Individuals and teams that work in Procurement department of Canadian Government organizations (Federal, Provincial, Municipal)",New standalone tool or feature,"1) What the dataset contains and its relevance to the use case: It includes proprietary data in the form of Word, PPT, PDF, Excel, and website content. This data will be used for Small Language Model.

2) Source(s) of the dataset (e.g., internal CRM, third-party APIs, web scraping, user-generated input, etc.) - We have proprietary data of 45 years created by industry experts on Government procurement. 

3) How the data was collected and its current format (structured, unstructured, tabular, text, etc.). - This data was created by Government procurement experts in the last 45 years. This is structured data and we have exclusive proprietary rights to use this data.

4) Indicate the key features or fields present in the dataset. - Proprietary content written in a simple language about Government procurement vetted by hundreds of customers and includes experiential knowledge, perspective, and insights from industry experts.

5) Approximate size (e.g., number of records, file size, time range). - Approximately 200 hours of data that users can read.",Fully labeled for ML tasks,FALSE,,"(1) Human-in-the-loop validation, (2) Use of a “golden dataset”, (3) Net promoter Score","Yes, we have explored and are  considering techniques associated with Generative AI or Large Language Models (LLMs) for this use case. We have explored Anthropic (Claude) and its AI models and an open source chat company that syncs data with Anthropic. However, the main use case is not offered, is not available open source, and is not present as an existing product for which we are seeking collaboration with Vector.",Yes. The output should be Fair and bias equivalent to a human industry expert. Proprietary data and user inputs should not be used by third party LLM's to update their models under any circumstances. This is importance for compliance and trust as our target customer is Canadian Government.,"(1) Net Promoter Score of 9 and above, (2) Compliance rate of 90% and above, (3) Source listing with 100% accuracy, (4) Time on task with 90% reduction in time, (5) Abandonment rate of 5% or fewer, (6) User error rate of fewer than 0.1%, (7) User retention rate of 85% and higher, (8) Ease of use 90% and higher (9) Task success rate of 95% and higher.","Microsoft Azure, Vector-provided infrastructure (if applicable), Internal on-premise or virtual private servers, Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases)","The ideal candidate for this project would have strong theoretical knowledge in AI & ML, has at least 2 years of relevant industry experience with LLMs, data science, prompt engineering, NLP, and data engineering.","4 external talent: AI Engineer, AI Architect, Web developer, Product Manager","Our intended product will be extremely useful and first of its kind in Canada for Government. As per a target user, ""Thats a huge undertaking! Even more, impressive that you can pull this together without VC or a big team."" If we are able to crack the new technical product with pilot customers, then it will make us attractive to raise external funding to hire inhouse talent as we have (1) Proprietary data flywheel + performance lifts create defensibility, (2) Packaged into workflows users already need towards task outcomes, (3) Delightful, frictionless, & opinionated UX"
Company 6,"The key business problem this project addresses is the limited scalability of our current teacher-dependent training model, coupled with the shortage of qualified educators in Tier 2 and Tier 3 cities and the challenges underserved communities face in adopting STEM programs. As a provider of robotics, coding, and space (rocketry) education for K–12 schools, Company 6 relies on resource-intensive teacher preparation: up to 40 hours of in-person training plus annual recertification. This model constrains our ability to expand, as each new school requires significant investment of time and staff, while many regions lack access to trained educators altogether.

At the same time, the global EdTech market—projected to exceed $400 billion by 2030 is growing rapidly, with STEM education among its fastest-rising segments. Yet, demand for hands-on, future-ready learning far outpaces the number of skilled teachers available, especially in underserved communities where resources and adoption are already limited.

To address this gap, we propose developing an AI-powered agentic tutoring system that provides students with real-time guidance in troubleshooting and programming. Though models are becoming powerful, a practical path may be to first solve for narrower use cases such as:

1.	Syntax error correction
2.	Basic logic error detection
3.	Explaining programming concepts
4.	Suggesting code structure improvements
5.	Answering “how do I?” questions from the curriculum

By shifting support from teachers to intelligent tutoring, we can double our effective student-to-educator ratio (from 4:1 to 8:1), reduce reliance on lengthy teacher training, and expand curriculum access into regions where educator shortages are most acute. This solution empowers students with immediate, personalized assistance, while lowering barriers for underserved schools and enabling sustainable, scalable growth in STEM education delivery.","The main objective is to reduce our dependency on manual staff support, which is currently a major barrier to scaling our business. By providing an AI-powered agentic tutoring system for student support, we can expand into new schools, tier 2 and tier 3 markets and underserved communities while significantly reducing the effort and time required for each partnership.

If we do not implement this project, our growth will stall. We won't be able to handle an increased number of schools and students, leading to lost revenue and a decline in our competitive position.

•	Proof of Concept (PoC) & Minimum Viable Product (MVP): We will develop a PoC to validate the core AI functionality and an MVP to demonstrate basic student assistance, with the goal of handling at least 50% of common student queries without requiring human teacher support. This will establish the feasibility and scalability of the solution.
•	Integration with LMS/Portal: We will integrate the solution into the existing Company 6 teacher portal. This integration is crucial for seamless user experience and data collection.
•	User Acceptance Testing (UAT) Environment: This is a critical step to ensure the solution is robust, user-friendly, and ready for a pilot launch with a small group of users.","Key Deliverables & Outputs:

The core deliverable of this project is an Agentic Tutoring System integrated into the Company 6 LMS. This system will combine agentic processes with Company 6’s proprietary curriculum to deliver proactive, personalized support that adapts to each student’s progress in real time.

The key functionalities and outputs are:
•	Proactive AI Assistant: Using an agentic architecture, the system will actively monitor lesson progress and content on screen. It will proactively offer context-sensitive tips, hints, and suggestions either on demand or initiated by the system itself, to help students overcome challenges and accelerate learning.
•	Context-Aware Response Engine: Built on Company 6’s proprietary curriculum and technical documentation, the AI will provide real-time, contextually accurate answers that are fully aligned with established teaching methodologies. This ensures instructional consistency and reliability.
•	Real-Time Code Analysis & Feedback: Leveraging Computer Vision (CV) and Optical Character Recognition (OCR), the system can interpret a student’s programming environment from live screens or image captures. It will detect syntax errors, logical gaps, and performance bottlenecks, then provide targeted, actionable feedback to improve student coding outcomes.
•	Feedback Loop System: A structured feedback loop will enable continuous system improvements. Insights from student interactions and performance data will be incorporated back into the system, refining the tutoring model over time for greater accuracy and personalization.
•	Guardrails for Safe Interaction: A stateful, conversational interface with embedded guardrails will ensure that interactions remain safe, transparent, and educationally appropriate, while also preserving conversation context for a more natural learning experience.
•	Teacher Dashboard: A monitoring interface will provide teachers with visibility into student interactions, query resolution patterns, and progress metrics. This allows teachers to intervene when necessary and track how the AI assistant enhances learning outcomes.

Contribution of ML/AI Specialists (MLAs):

We expect the MLAs to be instrumental in the following areas:
●	Training & Grounding the Language Model: The MLAs will be responsible for adapting and optimizing a Large Language Model (LLM) or similar model to align with Company 6’s proprietary curriculum and troubleshooting data. Instead of relying on full-scale fine-tuning, which is costly and requires frequent retraining as the curriculum evolves, we will leverage proven methods such as Retrieval-Augmented Generation (RAG), instruction fine-tuning, and prompt engineering to ground the model in Company 6’s content. This approach ensures higher accuracy, relevance, and scalability while reducing long-term maintenance overhead.
●	Developing the Code Analysis Engine: This is a crucial technical component. MLAs will design and implement the computer vision or OCR algorithms to interpret and analyze code from visual inputs. They will also build the logic to identify common programming errors and suggest improvements.
●	Integrating the AI with Existing Infrastructure: The MLAs will work with our development team to build the API for seamless integration of the AI system into the Company 6 portal. This includes setting up the necessary backend systems and pipelines for data flow.","The primary users of this solution are twofold: 1.	Students (75% usage volume): The direct end-users who will interact with the AI teaching assistant for real-time support and troubleshooting during their robotics and programming sessions. 2.	Company 6' Clients (20% usage volume): This includes our franchisees and partner schools. The solution will serve as a key resource to enhance their educational offerings and streamline their operational support, allowing them to focus on instruction rather than technical support. While not direct users of the chatbot interface itself, our internal Zebra corporate and support teams will be key beneficiaries of the solution's success, as it will reduce the need for manual support and provide valuable data for continuous curriculum improvement.",New standalone tool or feature,"For this project, we'll use a unique, proprietary dataset consisting of our extensive educational materials. This data is crucial for training our conversational AI to provide context-aware and accurate responses based on our curriculum.

1. Course Content & Curriculum Data This dataset contains our full multimedia course materials. Its relevance is paramount, as it will serve as the core knowledge base for the AI, enabling it to answer questions and provide guidance that's consistent with our teaching methodology.
Source: The data is internally sourced from our Canvas Learning Management System (LMS).
●	Collection & Format: It will be exported in a standardized IMS Common Cartridge (IMSCC) format, a type of unstructured data. We will also use video and image files from our courses.
●	Key Features: This dataset includes course modules, lesson plans, instructional text, diagrams, and multimedia files.
●	Approximate Size: For the initial pilot, we'll use a few select courses, including all their modules.

2. Student Assignment & Code Data This dataset will contain historical student assignments and working code examples. This data is essential for teaching the AI how to analyze and correct common errors.
●	Source: This will be internally sourced from student submissions within our learning portals.
●	Collection & Format: The data is a combination of unstructured student submissions (text, code files) and structured data related to assignments (e.g., assignment names, dates, student IDs).
●	Key Features: It includes student-written code, textual explanations, and, where available, feedback or grades.
●	Approximate Size: We can provide a sample of assignments from past student cohorts for the initial training phase.

3. ""Golden Standard"" Code Library This dataset contains high-quality, verified code that successfully achieves specific robot programming objectives. It will serve as a reference for the AI to understand ""correct"" code and provide accurate, effective suggestions.

●	Source: Internally created and curated by our subject matter experts.
●	Collection & Format: This is a structured dataset of code files, potentially with accompanying metadata.
●	Key Features: The dataset includes various programming challenges, their corresponding verified solutions, and comments explaining the code logic.
●	Approximate Size: A library of working codes for all our core robot missions.",Raw/unprocessed,FALSE,,"Our primary evaluation method will be      manual validation, supported by a ""golden dataset"" and ongoing performance metrics.

●	Golden Dataset: We will use a pre-established benchmark dataset of common student issues and programming challenges. This will allow us to quantitatively measure the bot's accuracy and effectiveness.
●	Human Validation: Our expert curriculum developers and teachers will serve as validators. They will interact with the bot to troubleshoot various coding problems, reviewing its suggestions and providing feedback for continuous improvement. This qualitative feedback is critical for ensuring the bot's recommendations are educationally sound.
●	Key Performance Indicators (KPIs): We will monitor performance through a dashboard that tracks metrics such as issue resolution time, response accuracy, and user satisfaction, which will be measured through surveys or feedback forms.","Yes, we have explored techniques associated with Generative AI and Large Language Models (LLMs).

We initially evaluated using Gemini's LLM with a standard Retrieval-Augmented Generation (RAG) pipeline. While this approach would be effective for knowledge retrieval from our curriculum documents, it lacks the capability to analyze a student's live coding environment.

To solve our core use case, which requires real-time troubleshooting based on a student’s work, we are pursuing a multimodal LLM solution. This approach leverages the model’s ability to analyze the screen directly, extract and interpret the code, and then provide contextual, intelligent feedback based on the actual code and its state.","Transparency: Students should be clearly informed when they are interacting with an AI system to build trust and set appropriate expectations.

Privacy: Although no personally identifiable information (PII) will be collected, usage data and interaction patterns may still be sensitive and require strong safeguards for protection.

Accessibility: The solution must ensure equitable access, including appropriate accommodations for students with disabilities, so that all learners can benefit.","We'll measure this project's success through a combination of quantitative and qualitative metrics.

     Quantitative KPIs

●	Resolution Rate: Our primary metric will be the percentage of student queries the AI teaching assistant can resolve without requiring a teacher or staff member to step in. A successful outcome will be an increase in this rate over time, which directly reflects a decrease in the burden on our support teams. We anticipate improving student query resolution times by 25%, thereby enhancing the overall learning experience and enabling students to complete additional learning challenges more effectively.
●	Accuracy of Suggestions: We'll measure the number of correct suggestions provided by the bot as a percentage of requests made, which is your initial metric. We'll track this using a ""golden dataset"" of known problems and solutions, along with human-in-the-loop validation to ensure accuracy.
●	User Engagement: We'll track user adoption and session duration within the bot to ensure it's a valuable and engaging tool for students.
●	Student-to-teacher ratio: We will track changes in the average number of students supported per teacher in classrooms using the AI teaching assistant, comparing against historical baselines. Improvements in this ratio will demonstrate the system’s ability to enhance instructional capacity and free up teacher time for higher-value student interactions.

Qualitative Outcomes

●	Improved Student Confidence: We anticipate a qualitative improvement in student confidence and independence as they receive immediate, effective support. We'll measure this through student and teacher surveys.
●	Streamlined Teacher Workflow: Teachers and staff should feel that the AI bot has significantly reduced their workload, allowing them to focus on teaching rather than troubleshooting. We'll confirm this through interviews and feedback sessions with our partners and internal teams.",Amazon Web Services (AWS),"The ideal candidate for this project would have hands-on experience with multimodal AI systems integrating NLP and computer vision, strong expertise in instruction-tuning LLMs for educational applications, and practical knowledge of real-time inference optimization. They should also have experience with OCR and code analysis, building production-ready APIs, and familiarity with cloud platforms such as AWS SageMaker and Bedrock.","Our technical, business and management teams are well-equipped to execute this project, with a blend of specialized and full-stack expertise. The team currently includes:

●	One Machine Learning Engineer: Focused on the core AI model development, including data processing, model training, and performance tuning.
●	One AI Consultant: Provides strategic guidance and ensures the solution aligns with the latest advancements in AI, offering a high-level perspective on the technical approach.
●	Two Full-Stack Developers: Responsible for building and integrating the solution into our existing platform, managing both the front-end user interface and the back-end infrastructure. ReactsJs and NodeJs with MySQL on AWS is our tech stack.
●	One Product Delivery Lead: Oversees the end-to-end delivery of product, including project management, business analysis, and quality assurance responsibilities.","Company 6 is a rapidly expanding company with a strong presence in the educational technology sector. We currently operate 25 retail locations in Canada and the US, with a growing number in these countries. Our proprietary curricula is a key differentiator, and it's been adopted by over 25 schools across Canada and Jamaica, with further expansion planned into other Caribbean islands.

Our core mission is to equip students with the skills required for the next decade and democratize access to robotics, coding and space (rocketry) education. Partnering with schools is central to this mission. However, our current operational model is not scalable due to the high cost and resource intensity of teacher training and support.

This project is a strategic initiative to overcome that bottleneck. By leveraging AI, we aim to significantly reduce the cost of our education delivery and support, enabling us to scale our operations rapidly. This will enable us to make our courses more accessible to a broader student base, including Tier 2 and Tier 3 cities and underserved communities, where STEM program adoption is growing but limited by the availability of qualified educators and resources. Through this approach, we can advance our goal of democratizing innovation sustainably while unlocking opportunities for students who have historically had limited access to such programs."
Company 7,"Company 7.ai is a Canadian healthtech startup building ambient, voice-enabled AI health assistants that relieve documentation, front-office and back-office burdens in outpatient and inpatient care settings. 

Clinics face unsustainable admin loads: physicians spend large portions of their day on notes while staff are overwhelmed by call triage, email, scheduling, and voicemail backlogs. These are problems most “AI scribe” tools don’t solve because they stop at documentation and don’t close the loop on inbound communications, patient intake, scheduling, or clinical encounter follow up.

We operate in healthcare IT (clinical workflow automation). The opportunity is to move beyond a single scribe into a voice-driven, multi-agent workflow system that automates call handling, triage, structured clinical intake, and direct EMR entry, with Canadian privacy compliance and bilingual (EN/FR) support. No major competitor currently offers this integrated, Canadian-compliant, connected solution.

Our product implements a modular architecture:
1. Administrative Agent: access and coverage verification, referrals and prior authorization, triage, scheduling, escalation, waitlist and template optimization, inbound communications management, digital check-in and forms, bilingual EN/FR communications, revenue cycle tasks including clinical documentation improvement and coding, claims scrubbing and submission, capacity and ADT throughput tracking.
2. Clinical Agent: structured intake, pre-visit summaries, real-time summarization and gap spotting, decision support, medication reconciliation, empathy and communication coaching, orders and task orchestration, rounding and handover, discharge planning and transition, after-visit summaries and patient education, care navigation and social supports, targeted post-encounter follow-up, EMR write-back via SMART on FHIR and HL7. 
3. Regulatory and Safety Agent: compliance and privacy-by-design, consent and preference management, PHI minimization, evaluation and performance monitoring, drift and safety monitoring, A/B testing with guardrails, quality management system activities, and post-market surveillance.
4. Usability-first dialog flows validated with staff in a real pilot primary care clinic.

Target Audience
Primary users are time-constrained clinicians and clinical administrators seeking efficiency, accuracy, and burnout reduction.

Our first commercialization vertical is Canadian primary care clinics that can implement our solution using an EMR gateway.

Go-to-market focuses on outpatient clinics, clinic chains, group practices, and health authorities in Canada, leveraging provincial pilot programs and a clinic-friendly subscription model. Having successfully been accepted to the Canada Health Infoway and Supply Ontario pre-qualified vendor programs and the BC AI SCRIBE trial, we have a dedicated user base that is excited to have us expand our solutions to bring clinical and administrative agents to practitioners' offices.  

Market Context
Based on a blended top-down and bottom-up view, the global market for clinical documentation, intake, and front-office workflow automation is approximately US$10B to US$15B in 2024 with low double-digit growth. Using a conservative serviceable obtainable market at 0.05 percent global penetration, this represents roughly US$13M in ARR potential. In Canada, with about 90,000 physicians and more than 100,000 allied health professionals, the serviceable obtainable market remains approximately US$200M annually at modest adoption.

Regulatory & Interoperability Environment
The solution is designed for Canadian and North American health privacy: HIPAA, PHIPA, PIPEDA (with SOC 2 readiness and Canadian data residency) and targets SMART on FHIR/HL7 interoperability. The project goal includes achieving a Health Canada Software as a Medical Device Class I level for the production-ready assistant using multiple sub-agents, a quality management system, verified evaluation frameworks and a post-market surveillance plan.","The key objectives are as follows: 
1. Automate inbound administrative communications for health facilities, including call triage, escalation, and appointment booking, with a non-integrated fallback mode for message requests.
2. Standardize clinical intake and pre-visit preparation by generating structured intake and pre-visit summaries that hand-off directly to the Company 7.ai Health Assistant AI scribe.
3. Deliver a production-ready assistant with interoperability and safety: SMART on FHIR/HL7 connectivity, modular multi-agent architecture with role-based access, and SaMD Class I readiness with privacy-by-design.
4. Track meaningful KPIs aligned to the business case, including accuracy, latency, voicemail reduction, patient activation, and capacity uplift.

Success milestones
1. Architecture and readiness: 
1.1. Target reference architecture selected with rationale
1.2. C4 and data-flow diagrams completed
1.3. RBAC and least-privilege enforced
1.4. SOC 2-mapped compliance artifacts produced.

2. MVP build and integration
2.1. Admin Agent callable service live with booking and graceful human handoff
2.2. Clinical Intake Agent generates pre-visit summaries with valid FHIR bundles.
2.3. EMR gateway and non-integrated relay operational; end-to-end EMR write-back proven with audit trail.
2.4. Usability dry-runs completed with ≥90 percent staff task success
2.5. pilot Go/No-Go passed with no Severity-1 issues and a rollback plan.

3. Discovery and safety groundwork
3.1. Clinic discovery and value-stream mapping baseline captured in ≥2 clinics; top workflows prioritized.
3.2. Intent taxonomy v1 covers ≥95 percent of historical intents
3.3. synthetic testbed covers ≥90 percent of prioritized intents with no PHI.

4. Pilot outcome targets
4.1. ≥99 percent pre-visit note-completion accuracy; 4.2. ≤60 seconds end-to-end latency from end of patient input to AI scribe output; 
4.3. ≥80 percent patient activation within three months.

5. Program-aligned deliverable
5.1. A working Proof of Concept plus documentation at program end.

Importance and consequences if delayed or not implemented
1. Operational strain persists: Without automation, clinics continue to absorb high documentation and front-office loads, exacerbating clinician burnout and limiting access. The project is explicitly designed to reduce this burden and improve care quality.
2. Lost efficiency and patient experience gains: Latency, voicemail backlog reduction, and patient activation improvements will not be realized, undermining the ROI case captured in the pilot metrics plan.
3. Interoperability and compliance gaps remain: Without the planned SMART on FHIR/HL7 adapters, RBAC, and privacy-by-design artifacts, safe at-scale deployment is delayed and integration opportunities with EMRs and pilot sites stall.
4. Commercial momentum slows: Delays risk deferring a production-ready assistant and reference architecture that are central to partnerships and near-term pilots.

In sum, the project’s objectives and milestones directly target automation of high-cost manual workflows, delivery of a production-ready, interoperable assistant, and rigorous KPI validation. Delays would prolong administrative bottlenecks, defer measurable capacity gains, and slow a safe, compliant path to deployment.","The 6 high-level outputs and deliverables include:
1. Functional prototype
1.1. Voice-enabled Administrative Agent that performs call triage, escalation, booking, and a non-integrated fallback that generates actionable message requests with an audit trail.

1.2. Clinical Intake and Pre-visit Summarizer that produces structured intake and pre-visit summaries, output as valid FHIR bundles ready for clinician review and AI Scribe write-back.

1.3. Pilot-ready package with observed usability dry-runs, issue log, and a Go/No-Go memo and rollback plan.

2. APIs and integrations
2.1. SMART on FHIR adapter for the AI Scribe / EMR connection, HL7 v2 bridge where required, and a non-integrated relay for clinics without immediate EMR integration.

2.2. Versioned API contracts including REST or GraphQL schemas, FHIR resource mapping, webhook events, idempotency rules, and rate limits with sample clients that pass conformance checks.

3. Conversational UI and dashboard
3.1. IVR or voice chatbot entry point and a web console for agent supervision and graceful human handoff.

3.2. Operational and evaluation dashboards that instrument KPIs such as accuracy, latency, voicemail reduction, patient activation, and capacity uplift, live before pilot.

4. Data, evaluation, and ML pipelines
4.1. Intent taxonomy and conversation design v1 with slot schemas, escalation rules, and error-recovery paths, covering at least 95 percent of historical intents.

4.2. Synthetic testbed and redaction policy for safe experimentation without PHI, aligned with cohort expectations that data be void of PII.

4.3. Reference implementations leveraged for agentic AI, prompt engineering, RAG, LLM fine-tuning, and experimental design to accelerate delivery.

5. Safety, compliance, and architecture assets
5.1. C4 and data-flow diagrams for Admin and Clinical agents, with RBAC model and secrets management documented.

5.2. Security and privacy-by-design artifacts including threat model, DPIA, audit logging spec, retention policy, consent and identity strategy, and SOC 2 control mapping.

5.3. Compliance artifact package with policies, runbooks, and vendor risk checklist mapped to SOC 2 and ISO controls.

6. Documentation and handover
6.1. Proof of concept codebase and documentation delivered within the 16-week execution phase with bi-weekly supervision and knowledge transfer, IP retained by the company.

6.2. Final presentation and deployment plan consistent with cohort outcomes for POC development and knowledge transfer.

What we hope Vector MLAs may help deliver within the cohort:
1. Two FTE MLAs for 16 weeks working with our team to build, integrate, evaluate, and document the above deliverables.
2. Work (remotely) primarily in our secure environment with company-provided compute and data, with access to Vector resources as needed.
3. Active development of the PoC under Vector’s technical supervision, including coding, prototyping, and testing.

By the end of the cohort we aim to have a working, instrumented PoC that books appointments, handles intake, escalates, writes back to the AI Scribe through a standards-based adapter, and includes the APIs, dashboards, evaluation harness, and compliance artifacts required to proceed to an expanded pilot. 

Of note, we have already identified a pilot clinic and secured funding for this phase of the project.","Primary customers are chains of clinics, with individual clinics as additional early sites. Health authorities will also become customers as we progress. Primary end users include clinicians (physicians, NPs, PAs, nurses, allied health), clinical administrators, practice managers, IT administrators, and compliance and privacy officers; secondary users include patients and caregivers and operations analysts. We already have users in Canada and the United States who are excited to trial the solution. Initial pilots will run with clinic chains and individual clinics due to faster procurement and lower regulatory complexity, while we have established pathways in several provinces to pilot within more regulated health authority facilities. We have also begun an exchange with Health Canada regarding Software as a Medical Device classification to support safe pilots and subsequent scale-up.",Enhancement to an existing system,"The datasets that we will use include, but are not limited to, the following: 
1. Real-world clinical call corpus 
1.1. Approximately 1,000 hours of anonymized, exported call audio from a Canadian primary care clinic as the seed dataset for intent taxonomy, routing, latency, and human-in-the-loop QA.
1.2. A synthetic medical records dataset of 1,000 patients with multiple encounters per record for the most common primary care consultation reasons. 
1.3. Governance includes PHI-free verification, storage in a secure environment, and documented sampling for train, validation, and blind test splits.

2. Synthetic and programmatic augmentation 
2.1. Generation of multi-turn EN/FR call simulations covering rare edge cases, escalation paths, and accessibility variants, with prompt templates and noise profiles aligned to clinic telephony. 
2.2. Synthetic transcripts paired with TTS-based audio for controlled ablations and error injection to test robustness and redaction policies.

3. Open ASR and diarization benchmarks for robustness 
3.1. Mozilla Common Voice for multilingual and accent diversity. 
3.2. AMI Meeting Corpus for far-field, multi-speaker, meeting-like conditions that mirror front-desk reality.

4. Task-oriented dialogue and scheduling benchmarks
4.1. Schema-Guided Dialogue for API-oriented flows across many services. 
4.2. Taskmaster-1 for realistic booking and support conversations.

5. Clinical style, empathy, emotion and summarization resources
5.1. MedDialog to align medical conversation style and turn-taking.  
5.2. EmpatheticDialogues to train and evaluate empathetic responses.  
5.3. IEMOCAP for emotion recognition signals in speech. 
5.4. DialogSum for real-life dialogue summarization. 

7. Grants and partner resources 
7.1. ElevenLabs Grants program acceptance provides year-long credits and tooling that can be integrated into evaluation pipelines for multilingual, expressive voice experiments.  
7.2. Ongoing dialogue with Providence Health Care Ventures in British Columbia to support pathway-to-pilot evaluation within more regulated facilities.",Annotation pipeline is in place,FALSE,,"1. Golden datasets and benchmark suites
1.1. “Gold Set v1” for access and scheduling built from 1,000 hours of anonymized Canadian primary-care call audio, plus synthetic EN/FR augmentations. Stratified by intent, language, and acoustics; split into train/dev/blind-test with leakage checks.
1.2. Task-oriented dialogue benchmarks to stress-test booking and escalation policies before pilot, using Schema-Guided Dialogue and Taskmaster-1, mapped to your intent/slot schema.
1.3. De-identification and PHI leakage checks using a synthetic redaction suite aligned to Health Canada SaMD expectations for privacy-by-design.

2. Human-in-the-loop validation
2.1. Clinician review breakpoints for pre-visit summaries and booking confirmations, with structured rubrics on correctness, completeness, and clinical relevance.
2.2. Front-office staff QA on triage and escalation outcomes, with disagreement adjudication and calibration sessions.
2.3 Shadow-mode trials in a partnered clinic in British Columbia, then staged rollout with safe fallbacks and human override.

3. Heuristics and business rules
3.1 Guardrails for consent capture, identity checks, clinic-specific scheduling constraints, and escalation to human when confidence is low or P95 latency is exceeded.
3.2 Policy engine with versioned rules for hours, providers, templates, and referral requirements with rule coverage reported alongside performance metrics.
3.3 Safety filters for PHI minimization, profanity/abuse escalation, and hard blocks for restricted actions.

4. Pre/post comparisons and A/B testing
4.1 Operational baselines collected 2–4 weeks pre-pilot: voicemail backlog, abandonment rate, average handle time, booking lead time, schedule utilization, and no-show rate.
4.2 Difference-in-differences analysis across pilot vs. control sites; A/B or phased rollouts where feasible.

Program targets (initial): pre-visit note completion ≥ 99 percent accuracy; end-to-end latency from patient input to scribe output ≤ 60 seconds; ≥ 80 percent patient activation within 3 months; material reduction in voicemail backlog and abandonment.

5. Metrics and dashboards
5.1 Speech/turn-taking: Automatic speech recognition word error rate, diarization, speaker-turn accuracy.
5.2 Dialogue quality: intent, slot exact-match, escalation precision/recall, booking success rate, fallback rate, safe-completion rate.
5.3 Summarization: factuality and consistency scores with human rubric audits.
5.4 Safety and privacy: PHI leakage rate, hallucination rate, harmful-content flagging precision/recall.
5.5 Fairness/robustness: performance parity across EN/FR, accent clusters, and call types, with gaps ≤ 5 percentage points where feasible.
5.6 Real-time dashboards for ops and executive rollups for return on investment (capacity uplift, time saved, access metrics).

6. Evaluation pipelines and documentation
6.1 Reproducible pipelines for data curation, labeling, model training, and report generation, with versioning, seed control, and immutable artifacts.
6.2 C4 and data-flow diagrams that link evaluation touchpoints to trust zones and a threat model updated as models change.
6.3 Post-market surveillance plan for continuous monitoring, issue taxonomy, and rollback criteria aligned to SaMD Class I readiness.

7. Resources and partnerships that strengthen evaluation
7.1 Pilot clinic in Canada already engaged to trial the system and provide human-in-the-loop feedback.
7.2 ElevenLabs Grants program resources for robust text-to-speech and speech-to-speech synthetic augmentation in EN/FR and diverse voices with 33 million credits secured.
7.3 Ongoing dialogue with Providence Health Care Ventures (BC) to support pathway-to-pilot evaluation in more regulated facilities.","Yes. Our stack is GenAI-native and we’re already using and evaluating LLM techniques across the speech, orchestration, and safety layers:
1. Azure AI Foundry for orchestration, summarization, and tool-calling to scheduler/EMR: GPT-4o family and o-series models via Azure AI Foundry with content filtering and model routing. We also use the Azure Speech Studio and its developing features to be paired with Azure Communications Services. 

2. Medical ASR: We have explored and have the option to install a self-hosted trial of Deepgram Nova-3 Medical for high-accuracy, low-latency transcription in clinical settings; also evaluating Azure Speech as a fallback. 

3. Domain LLMs (medical): We are experimenting with Google’s MedGemma (open models for medical text/image comprehension; 4B multimodal and 27B text-only) and tracking Med-Gemini research; used for clinical text understanding and guideline-aware checks.

4. Open/self-hosted instruct models from Hugging Face (e.g., Gemma/Gemma-based, Llama, Mistral variants) for intent/slot extraction, redaction assistance, and cost-efficient on-prem inference when PHI exposure must be minimized.

5. Voice synthesis / speech-to-speech: ElevenLabs (via our grants program) for multilingual EN/FR voices and accent coverage in synthetic data generation and usability testing. 

Our approach patterns for the development of the above into the outputs and deliverables previously described, include: 
1. Retrieval-augmented generation over policy/clinic playbooks.
2. Structured function/tool-calling for system of record (our AI Scribe / an EMR) write-back and eligibility checks. 
3. State-machine action graph to keep agents deterministic for safety
4. Evaluation with golden sets and human-in-the-loop review.","Our first duty is patient safety and clinician enablement. Agentic systems that schedule care, triage symptoms, or summarize encounters can create real harm if they misroute urgent cases, fabricate facts, or erode clinician vigilance. We mitigate this with human-in-the-loop gates at high-impact moments, deterministic action graphs rather than free-form autonomy, confidence thresholds that trigger escalation to staff, and continuous post-deployment monitoring tied to a documented risk register.

Fairness and bias: Speech and language performance can vary by accent, language, and noise conditions. We design datasets and evaluations to measure parity across EN and FR, accent clusters such as en-CA and fr-CA, and acoustic settings such as handset versus speakerphone, then set parity targets and corrective retraining where gaps exceed thresholds. We also monitor intent and slot accuracy across demographic proxies gathered ethically, and we publish evaluation summaries to build user trust.

Privacy, data minimization, and consent: We collect only what is necessary for the task, keep PHI out of analytics through de-identification, and enforce role-based access with audit trails. For Canada, we align consent practices with PHIPA’s requirements for knowledgeable consent and the “circle of care,” and with PIPEDA’s principles for meaningful consent and limiting collection. We disclose recording at call start, provide opt-outs, and maintain retention and deletion schedules.

Regulatory pathway and correctness of claims: We are preparing for Health Canada oversight as Software as a Medical Device. Our initial scope targets Class I readiness, with quality management, post-market surveillance, and documented human oversight. As functionality expands, we will reassess classification using Health Canada’s SaMD guidance and examples, and we will adjust processes if risk class increases. We are using some of the more involved NHS and FDA guidance where Health Canada guidance is lacking. 

Transparency, accountability, and auditability: We maintain dataset cards, model cards, decision logs, and immutable audit trails mapped to SOC 2 logging controls via our Vanta GRC platform so that actions are explainable to clinicians, privacy officers, and regulators. We align our risk program to NIST’s AI Risk Management Framework and its Generative AI profile, and we operate with explicit service level objectives for accuracy and latency.

Security and misuse: We protect recordings, transcripts, and model prompts with encryption in transit and at rest, enforce least privilege, and monitor for prompt injection, data exfiltration, and harmful content. We include red-teaming for jailbreaks and abuse, and we maintain incident response procedures consistent with SOC 2 expectations for logging, detection, and reporting.
Scope of practice and user trust. The assistant never makes diagnoses or overrides clinician judgment. Outputs are marked as drafts for review, with visible confidence signals, model provenance, and links to underlying evidence where applicable. We provide clear user education, disclosures about synthetic voices, and guardrails to prevent voice impersonation.

Societal impact and workforce effects: We design the system to reduce clerical burden rather than displace clinical roles. We publish change-management materials, measure staff experience, and include a structured channel for feedback and refusal. We build bilingual access that supports equity for English and French speakers and we design for accessibility.

External guardrails and safety research: We plan to initiate a dialogue with LawZero, the new nonprofit launched by , to explore integrating a “safe-by-design” guardrail framework inspired by their Scientist AI approach and benchmarks for trustworthy agents. The goal is to increase transparency about agent goals, to detect deceptive or harmful behaviors, and to block unsafe actions before they reach production systems.

Governance and continuous improvement: We run formal Transparency, Equity, Value, and Voice practices that include golden datasets, blind tests, and pre/post comparisons at pilot sites, and we publish an evaluation plan that names owners, thresholds, and rollback criteria. We review drift, error taxonomies, and bias metrics on a regular cadence, and we plan to re-certify releases against Health Canada SaMD guidance when functional scope changes.","We will measure success with a pre-defined baseline (2–4 weeks before pilot), followed by continuous dashboards and a formal end-of-pilot read-out. Baselines will be captured from call logs, scheduling systems, and AI Scribe or EMR audit trails, then compared to pilot-period metrics using A/B or difference-in-differences where feasible.

Operational access and admin efficiency: We aim to reduce voicemail backlog and call abandonment relative to baseline and to shorten average handle time and time-to-booking. We expect higher first-contact resolution and improved schedule utilization. Success will be evidenced by a material decline in backlog and abandonment, meaningful reductions in handle time, and measurable lifts in utilization and same-day/next-day access.

Clinical intake and documentation quality: We will track accuracy and completeness of pre-visit summaries and structured intake against a clinician rubric, with a target of ≥99 percent pre-visit note-completion accuracy. We will also measure time-to-close the chart from EMR logs and aim for a clear reduction versus baseline.

Patient engagement: We will measure patient engagement (e.g., completion of digital check-in, confirmations, and follow-ups) with a target of ≥80 percent within three months at pilot sites, alongside concise micro-surveys after calls.

Safety, privacy, and compliance: We will track PHI leakage rate (target zero in redacted analytics), safe-completion rate for guarded flows, and incident counts/severity; we will require zero critical safety incidents and maintain documented human-in-the-loop approvals at designated checkpoints.
Fairness and robustness. We will monitor parity across English/French and accent clusters (e.g., en-CA, fr-CA) for key metrics such as intent F1, slot exact-match, and escalation precision/recall, aiming for performance gaps ≤5 percentage points; we will also track robustness across acoustic/noise profiles.
Interoperability and workflow completion. We will validate that FHIR Bundles pass conformance checks (target 100 percent valid in production paths) and that bookings, tasks, and write-backs complete successfully end-to-end with auditable IDs.

Human factors and adoption: We will collect clinician and staff usability scores and short burnout/effort proxies, plus narrative feedback from debriefs. Success looks like high usability, clear perceived time savings, and growing voluntary use without mandate.

Program-level KPIs and Go/No-Go criteria: At minimum, we expect: ≥99 percent pre-visit note accuracy; ≤60 seconds P95 latency; ≥80 percent patient activation; a material decline in voicemail backlog and abandonment; demonstrable reductions in time-to-booking and time-to-close the chart; zero critical safety incidents; and ≥95 percent interoperability validations passed. Meeting or exceeding these thresholds, with neutral-to-positive fairness checks and positive staff/patient feedback, will constitute a Go for expanded pilots.","Microsoft Azure, Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), ElevenLabs, Deepgram, n8n / Langchain","The ideal candidate for this project would be a hands-on ML engineer with experience shipping LLM- and agent-powered workflows (prompt engineering, tool-calling/RAG), strong NLP and data cleaning skills, and familiarity with speech pipelines (ASR/diarization), Azure AI Foundry, and open models (Hugging Face). They should be comfortable building evaluation pipelines and dashboards, working with healthcare standards (SMART on FHIR/HL7), and applying privacy-by-design practices for PHI. It would be a bonus if they have English and French capability and, especially, prior work in clinical settings.","We have a 7-person core team: two QA specialists focused on test automation, safety checks, and regulatory validation (one who is solely focused on clinical QA); a clinical lead / Chief Medical Officer who define requirements, evaluation rubrics, and human-in-the-loop review; one ML engineer responsible for LLM/ASR pipelines, RAG, and metrics; one clinical interoperability engineer owning SMART on FHIR/HL7 adapters, EMR integrations, and data security; and one product manager coordinating roadmap, pilots, and stakeholder feedback. We are also onboarding product designers to strengthen conversation design and end-to-end UX.",We're very excited to get started!
Company 8,"Company 8 proposes an AI-powered multi-agent scheduling assistant to streamline hospital staff scheduling by replacing the current manual Excel and email workflows. The system uses generative AI in a conversational interface to gather clinician preferences, availability, and constraints in natural language. Instead of schedulers sending spreadsheets or numerous emails, the AI assistant engages with clinicians in a dialogue to collect their scheduling needs, clarifying ambiguities and ensuring all inputs are understood. The assistant then validates these requests against scheduling rules (e.g. shift limits, skill mix requirements) and flags or helps resolve any conflicts or overlaps. All collected data feeds directly into Company 8’s existing scheduling platform. This solution will reduce administrative burden on department schedulers, shorten the schedule creation cycle, and improve accuracy and staff satisfaction by collaboratively resolving scheduling issues in real-time.


Our proposed solution is an Agentic Generative AI system that serves as a conversational scheduling coordinator. It interacts with multiple stakeholders through natural language, effectively becoming a scheduling “co-pilot” for the department:

Interactive Data Intake and smart import tool: The AI agent initiates or responds to clinicians via a chat-based interface (which could be a web portal or messaging platform). It asks for their availability, preferred shifts, vacation requests, and other constraints in a friendly, structured dialogue. For example, a clinician can simply say, “I prefer morning shifts next week and need Friday off,” and the assistant will interpret and record this input. It asks follow-up questions if details are missing or unclear (e.g., “Do you have any preference between Monday and Tuesday for your morning shifts?”), mimicking how a human scheduler would clarify ambiguous requests.  In parallel, the Smart Import Tool enables schedulers to upload Excel or CSV files containing availability, constraints, and preferences directly into Company 8 Scheduling. Using a mapping UI, planners can align column headers (e.g., physician name, date, shift type, constraint type) with Company 8’s data schema. The tool performs basic validation and error checking, flags inconsistencies before import, and provides a preview mode so users can confirm accuracy. Historical import templates can be stored for recurring use, accelerating the process for each new scheduling period.

Together, these two intake modes — conversational AI and structured file import — give organizations flexibility to work within their current habits while transitioning toward full platform adoption. Data gathered through chat or import is tagged, logged, and made traceable in Company 8, creating a single source of truth for constraints and preferences. This dual approach not only reduces manual data entry and errors but also bridges the gap between offline workflows (Excel, email, paper) and the digital scheduling environment.

Constraint Validation: As inputs are gathered, the system automatically checks them against institutional rules and individual contract constraints. It knows, for instance, how many hours per week a clinician can work, mandatory rest periods, or specific skill requirements per shift. If a clinician’s request violates a rule (for example, too many consecutive night shifts), the assistant immediately notifies them and suggests alternatives (e.g., “You have requested four night shifts in a row, which exceeds policy. Would you like to reduce this to 2 or 3 nights?”). This real-time validation ensures that most issues are resolved upfront, rather than discovered later by the scheduler.

Conflict Resolution: One of the most challenging aspects of scheduling is handling conflicts (such as two clinicians requesting the same limited slot or too few people available for a critical shift). The AI assistant will detect these conflicts as the data comes in. It can proactively address them by alerting the affected users or negotiating solutions. For example, if two surgeons both request the same week off, the assistant can inform them of the overlap and gather additional input: perhaps one is willing to swap vacation weeks after a conversation mediated by the AI. In cases where an immediate resolution isn’t possible, the system flags the conflict clearly for the human scheduler and even provides context or suggestions (like a ranked list of possible resolutions based on past resolution patterns or fairness criteria). The goal is to handle simpler conflicts autonomously and assist the scheduler in making final decisions on tougher conflicts.

Automated Schedule Assembly: Once inputs are collected and refined, the system compiles them into a structured format compatible with Company 8’s scheduling engine. Essentially, the AI translates the conversational inputs into scheduling data (shift preferences, availability calendars, etc.). The Company 8 scheduling system can then automatically generate draft schedules using this data, or the scheduler can use the compiled inputs to manually finalize a schedule with confidence that all preferences and constraints are up to date.","Our project has clear objectives to drive efficiency and align with Company 8’s strategic goals of operational excellence and user-centric innovation:

Automate Manual Workflows: Eliminate the need for manual spreadsheets and email threads by providing a one-stop conversational interface for scheduling data collection. This directly reduces administrative overhead for schedulers.

Improve Accuracy and Compliance: Ensure scheduling constraints (working hour limits, coverage requirements, union rules, etc.) are adhered to automatically through built-in validations. This reduces human error and compliance issues, aligning with our mission to relieve administrative burden while maintaining fairness and safety.

Accelerate Schedule Finalization: Shorten the end-to-end scheduling cycle. By gathering complete and correct information in a single interactive session (rather than over days of email exchanges), initial schedules can be generated faster. This means departments can release final schedules earlier, improving operational predictability.

Enhance Stakeholder Satisfaction: Engage clinicians in a collaborative manner, giving them confidence that their preferences are heard and respected. A more transparent, interactive process leads to higher clinician satisfaction and engagement. At the same time, schedulers benefit from a co-pilot that reduces their workload and stress, improving job satisfaction.

Align with Intelligent Workflow Strategy: This project advances Company 8’s strategic goal of intelligent care coordination. By embedding a generative AI assistant into the scheduling workflow, we demonstrate Company 8’s commitment to innovative solutions that streamline healthcare operations. The solution not only modernizes a tedious process but also showcases AI-driven transformation in day-to-day clinical operations, reinforcing Company 8’s reputation as a leader in healthcare innovation.

In summary, the Conversational Scheduling Assistant will modernize the scheduling process by leveraging advanced AI to interact, understand, and help coordinate schedules as a human would – but faster and with fewer errors. This aligns directly with Company 8’s broader mission to provide intelligent solutions that increase efficiency, reduce burnout, and empower healthcare teams with better tools. It addresses a real pain point in primary and acute care settings, and its successful implementation would be a tangible step toward smarter, more adaptive healthcare operations.

Milestones:

Conversational intake MVP;

Rules/constraint validator;

Conflict-resolution agent;

API integration to Company 8 Scheduling;

Pilot metrics showing reduced cycle time and fewer post-publish trades.","Working PoC: multi-agent conversational assistant (web chat) that collects availability/preferences/constraints.

Validation & conflict modules: programmatic rules checks (hours, rest, skills mix) and agent-assisted conflict handling.

Integration layer (APIs) to push structured intake to Company 8 Scheduling and pull rules/rosters/templates.

Config & admin dashboard (lightweight) for schedulers: status of submissions, flagged conflicts, suggested resolutions.

Docs & deployment plan: architecture, ops runbooks, Responsible AI notes, production pathway on Azure.","The primary users of the proposed solution are hospital department schedulers and the clinicians they schedule.  Department Schedulers (Administrators): These are the staff responsible for creating and managing schedules (for example, a scheduling coordinator in an ER department). For them, the AI assistant will serve as a powerful co-worker. It handles the tedious parts of collecting inputs and ensures rules are followed, allowing the human scheduler to focus on higher-level decisions and final approval. Schedulers will interact with the system through an administrative interface where they can oversee the conversation progress (e.g., see which clinicians have submitted info, view conflict alerts generated by the AI, and intervene or adjust as needed). This helps schedulers work more efficiently and confidently, reducing burnout from the formerly repetitive communication tasks.  Clinicians (Doctors, Nurses, and Other Healthcare Providers): These are the individuals whose availability and preferences are being scheduled. Instead of filling out Excel templates or emailing requests, clinicians will interact with the conversational AI (via chat or a guided form that feels like chat). This offers them a convenient, user-friendly experience – they can quickly provide their availability in plain language at any time, on any device. The AI ensures they are immediately notified of any issues with their requests (like overlapping commitments or rule violations) and can adjust in the moment. This interactive process gives clinicians more transparency (they know their requests are received and valid) and potentially more input into resolving scheduling conflicts fairly. Engaging clinicians in this manner is likely to improve their satisfaction with the scheduling process, as it fosters a sense of collaboration and fairness (everyone is talking to the same unbiased assistant).",Enhancement to an existing system,"What it contains & relevance: Past shift assignments, roles, locations, dates/times; recorded constraints/preferences; common exceptions/changes; scheduler notes (where available). Used to model typical patterns, validate rules, and design conflict strategies.

Sources: Company 8’s scheduling.


Approx. size/time range: Multi-year history across several departments; tens to hundreds of thousands of shift rows (varies by site), plus smaller tables for constraints/preferences. Exact volumes available at kickoff.",Raw/unprocessed,FALSE,,"Human-in-the-loop: Scheduler reviews suggestions/resolutions before finalize/publish.

Implement openevals testing including 

Evaluators LLM-as-Judge Extraction and tool calls 

Evaluating structured output with exact match 

Evaluating structured output with LLM-as-a-Judge 

RAG Correctness Helpfulness Groundedness 

Retrieval relevance 

Retrieval relevance with LLM as judge 

Retrieval relevance with string evaluators

Pre/Post metrics & dashboards:

Reduction in Scheduling Cycle Time: A primary goal is to reduce how long it takes to go from initiating schedule collection to finalizing the schedule. We will measure the total turnaround time for schedule creation before and after implementing the AI assistant. Our target might be, for example, a reduction by 30-50% in the scheduling cycle. Faster schedule completion not only saves administrator time but also gives clinicians their rosters sooner.

Decrease in Communication Rounds: We expect far fewer back-and-forth emails or revisions. As a proxy, we can count the average number of touchpoints or revisions needed per scheduling period. Success would mean that many schedules are completed in one cycle of AI-facilitated input, rather than multiple correction cycles. A concrete metric could be “number of corrections or re-run schedules needed” – with the AI assistant, this should drop significantly (ideally to near zero for routine periods).

Scheduler Time Spent: We will track how many hours the scheduling staff spend on schedule coordination. If the AI assistant is effective, the human scheduler’s active involvement time (excluding waiting for inputs) should decrease. For instance, if a scheduler currently spends 10 hours per week on communications and data entry, we aim to cut that by at least half. This time savings can be quantified through time logs or self-reported estimates and is directly tied to cost and efficiency improvements.

Adherence to Constraints: Measure the incidence of scheduling rule violations or overlooked preferences in the final schedule. We anticipate that with automated validation, compliance with all constraints (legal, contractual, personal preferences) will be near 100%. Any violations or mismatches found after schedule publication (e.g., someone scheduled on a day they asked off) would indicate a failure. The goal is to eliminate such errors entirely.

User satisfaction (clinician & scheduler CSAT/NPS, qualitative feedback).","At Company 8 we started exploring different use cases around Gen AI and LLMs, we are building a platform where we can create reusable components and enable fast productization, including: 

LLM-driven conversational intake (prompt-engineered; few-shot with scheduling exemplars).

Agentic multi-agent orchestration (e.g., one agent for dialogue, one for validation/tools, one for conflict suggestions).","Privacy & residency: Canadian data residency; no PHI; de-identified staff data for modeling.

Racial and violence filters","Operational: post-publish trades/corrections, scheduler hours on intake.

Experience: Clinician & scheduler CSAT/NPS.

Adoption: % departments using assistant after pilot; time to first schedule draft.","Microsoft Azure, Models :  Azure open AI hosted in canada with gpt 4.1  Azure Open AI embedding Large as embbeding  Data :  Snowflake  Graphql  Pgvector  GeAI Frameworks:  langchain  langgraph  Monitoring :  langsmith  Server :  Langgraph  Kubernetes  Scheduling API  UI / interaction :  Typescript  ReactJs","The ideal candidate for this project would be an AI engineer with strong software engineering fundamentals (Python) and hands-on experience building LLM applications: prompt engineering, retrieval-augmented generation (RAG), and tool integration via MCP servers.","Principle engineer Data & AI: 1

AI engineers: 2

Data and analytics engineers: 4

Data analytics:3 

Product manager:1",
Company 9,"Key Business Problem

Fundamental physics has slowed in making breakthrough discoveries. While more physicists than ever are working, progress is limited because today’s questions are far harder and less intuitive. Human reasoning struggles with ideas like quantum fields or ten-dimensional space. Academic systems were built for an earlier era and depend on individual brilliance instead of scalable methods. Meanwhile, AI and distributed computing are advancing quickly, but these tools have not yet been applied in a systematic way to fundamental science. The result is a growing gap: the biggest scientific questions are stalling just as technology capable of helping is taking off.

Opportunity

This is the moment to create a new kind of research organization. AI is now able to reason, not just recognize patterns. Applied to fundamental science, it can explore theories beyond human cognitive limits. Company 9 aims to combine deep scientific expertise with advanced AI engineering and a mission-aligned non-profit model, operating globally and free from the constraints of traditional institutions. The market for AI-driven scientific discovery is wide open - there is no established player with this mix of capabilities.

Industry & Target Audience

Company 9 works at the intersection of advanced AI and fundamental science. Its audience includes academic researchers, scientific institutions, philanthropic funders, and global science policy makers who share the goal of accelerating discovery. Interest in AI for science is growing across both private and public sectors, and regulatory frameworks are still forming, leaving room for early movers to shape the field.","Key Objectives

Create AI systems for scientific reasoning: Build technology that can generate hypotheses, perform multi-step symbolic reasoning, and evaluate scientific models beyond what human intuition can handle.

Accelerate fundamental physics research: Remove the bottleneck caused by limited human cognitive bandwidth and outdated academic processes.

Establish a new research model: Operate as a global, independent non-profit that merges the speed and engineering focus of a tech company with the depth of a research lab.

Open and scale knowledge creation: Design systems that capture, refine, and share evolving insights to make basic science faster and more collaborative.

Success Milestones

Early platform releases: Deliver the first working versions of the AI research system (starting with physics) and demonstrate its ability to generate and test novel scientific ideas.

Growing research impact: Produce results recognized by the scientific community—new models, insights, or validated hypotheses that would have been difficult for humans alone.

Community adoption: Build partnerships with universities, research institutes, and funding bodies who use and contribute to the platform.

Global network of contributors: Attract and maintain a diverse team of engineers, scientists, and collaborators who expand the system’s capabilities over time.

Importance of Timely Execution

If this project is delayed or not implemented, the slowdown in fundamental physics discovery will continue. Human-only research will remain constrained by cognitive limits and outdated academic structures, and the accelerating power of modern AI will be left untapped. This risks missing a once-in-a-generation opportunity: AI is just now reaching the level where it can reason and build theories. Waiting means losing momentum, letting others define the standards for AI-driven science, and postponing potential breakthroughs that could reshape our understanding of the universe and drive long-term technological and societal progress.","Company 9 is interested in the MLA program to work on internal tools to contribute towards the development of the AI Physicist:

Core Reasoning Engine - A central model that coordinates scientific reasoning and integrates outputs from specialized modules such as symbolic regression, question formulation, and model evaluation. This is the foundation for scalable theory-building 

Specialized Modular Components - Independent but interconnected models that handle tasks like hypothesis generation, multi-step symbolic reasoning, and formal model building. These modules must exchange information dynamically and adapt through reinforcement learning.

Knowledge Capture Layer - An internal repository to record evolving insights, enabling reuse and refinement of ideas over time. This ensures the organization can build on previous work without repeating experiments.

Automation Pipelines - Infrastructure to run simulations and testing workflows automatically, feeding results back into the reasoning engine for continuous improvement 

These deliverables are critical for enabling scalable, AI-driven scientific discovery and will directly determine the success of the cohort’s work.",Scientists and universities will access the system to generate and evaluate new ideas in fundamental physics and related fields,Enhancement to an existing system,"1. Content and Relevance

The project will use large collections of scientific literature and related physics data. These include published physics papers, experimental data, mathematical formulations, and structured metadata (citations, references, equations). This material is essential for training and evaluating AI systems that can reason about physical theories and generate new hypotheses.

2. Sources

The data will come from a mix of public scientific repositories (such as arXiv and other open-access physics archives), partner institutions that provide curated datasets, and internally generated research notes and outputs from the Company 9 team and collaborators.

3. Collection Method and Format

These data are gathered through publicly available APIs and bulk-download services from open-science archives, plus direct contributions from research partners. The current formats are mixed:

Structured: bibliographic metadata (titles, authors, references, citation graphs).

Semi-structured: LaTeX/Markdown representations of equations and models.

Unstructured: full-text scientific papers, diagrams, and research notes.

4. Key Features or Fields

Paper metadata: title, authors, institution, publication date, references, citation network.

Full text: abstracts, main body text, and mathematical equations.

Figures and tables describing experimental data or theoretical models.

Derived features: symbolic expressions, extracted physical parameters, and model relationships built during preprocessing.

5. Approximate Size

The combined dataset is expected to cover millions of scientific papers spanning several decades of physics research, amounting to terabytes of raw text and figures.",Partially annotated,FALSE,,"Evaluation Plan:

Benchmark tests: Physics specific benchmarks and standard reasoning benchmarks.

Expert review: Physicists validate generated hypotheses and derivations.

Metrics: Track reasoning accuracy, number of validated new insights, and time to produce/test hypotheses.

Dashboards: Monitor hypothesis quality and validation turnaround over time.

Impact: Success measured by adoption and recognition from the scientific community.","Yes. The project actively explores large language models and related generative techniques:

Foundation models such as Qwen, ChatGPT, Claude, and open-source LLMs (e.g., LLaMA family) as starting points for scientific text understanding and hypothesis generation.

Domain-specific fine-tuning on physics literature and mathematical corpora to improve symbolic reasoning and scientific accuracy.

Hybrid architectures combining LLMs with symbolic reasoning modules and reinforcement learning to guide multi-step derivations and formal model building.

These approaches are core to enabling AI systems that can reason about complex physical theories and propose new scientific insights.","Scientific integrity and bias - Models trained on existing literature may inherit historical biases or overlook less-cited but valuable work. Regular expert review and transparent validation are needed to prevent distorted conclusions.

Transparency and reproducibility - All generated hypotheses and reasoning steps must be documented so researchers can audit and reproduce results.

Intellectual property and data rights - Use of scientific papers must respect licensing and copyright terms of open-access repositories and partner datasets.

User trust and oversight - Human experts must remain in the loop to avoid over-reliance on automated reasoning and to guard against misleading or incorrect outputs.","Measures of Success:
Adoption - Regular use of the internal tools by AI researchers, physicists, and partner institutions.

Efficiency gains - Reduction in time to generate and test hypotheses compared to current manual workflows.

Community impact - Citations, collaborations, and recognition from the broader scientific community.

System performance - Accuracy on reasoning benchmarks and stability of the core platform over successive releases.","Amazon Web Services (AWS), Internal on-premise or virtual private servers, Company-hosted development environment (e.g., GitLab, JupyterHub)","The ideal candidate for this project would be a strong machine learning engineer with hands-on experience in large language models, prompt engineering, and symbolic or scientific reasoning. They should be comfortable building internal research tools, integrating structured and unstructured data, and collaborating with scientists to validate and refine AI-driven hypotheses. They should also have a passion for science and scientific discovery, with a specific focus on physics or fundamental science.","15 full time data scientists, software engineers, researchers, etc.",
Company 10,"Company 10 operates at the convergence of three rapidly evolving sectors: events & conferences, community management platforms, and professional networking platforms. These industries collectively shape the landscape of professional engagement and connection, yet they often fall short of sustaining meaningful relationships across hybrid and in-person experiences. Company 10 addresses a critical market gap: it creates a new category for intelligent, values-based networking—bridging the divide between static directories, impersonal social platforms, and ephemeral event interactions. Company 10’s proposition transforms professional connectivity into a continuous, data-enriched experience for individuals and organizations.","Key objectives: 
Automate a current manual process: finding the right match is relatively easy in professional settings: we already implemented it in our MVP by scrapping available professional information and matching individuals based on their professional background by using prompt engineering in existing LLM. But in order to improve matching and relationship building based on multiple data sources, prompt engineering is not sufficient, nor the fine tuning of existing models. We need to translate what we believe to be the effective process for social & professional matches and relationship building we currently implement in our experiential events. Doing it manually does not allow us to scale. 
Developing a proprietary data set (vectorization and bucketization of proprietary data in order to feed and train a new model, to execute our vision of ""Intentional Serendipity”
Develop and train a propriety model of “intentional Serendipity”

If this project is delayed - we won’t be able to scale beyond our own events, not to bigger international conferences due to the limiting nature of extensive manual work.","At best: A functional & operational prototype, with a dashboard and supporting Bakend system 
At minimum: propriety dataset  - vectorization and bucketization of proprietary data and an AI workflow pipeline - ready to train",The first users will be those attending events we organize or support with the current MVP,New standalone tool or feature,"1) What the dataset contains and its relevance to the use case.
The dataset contains records of event participants, their professional background, past matches  and past events they participated in, and additional data they included during registration. 
2) Source(s) of the dataset (e.g., internal CRM, third-party APIs, web scraping, user-generated input, etc.).
Sources of the data: internal CRM, web scrapping and user generated input
3) How the data was collected and its current format (structured, unstructured, tabular, text, etc.).
The data was collected during field testing we did with MVP prior (during registration) and during live events (interaction of participants with their matches)
The data is both structured and unstructured:
Structured - user generated input, internal CRM and matches
Web Scrapping - semi structured (the scrapped file is structured as a jason, but include unstructured data as a text) 
4) Indicate the key features or fields present in the dataset.
Current key features are professional data, matches and reasonings. 
5) Approximate size (e.g., number of records, file size, time range).
Each record is approximately 0.5M. We currently have a few thousand records from past year events, and from this October are going to accumulate thousands more during multiple events due to a new partnership with an event place.",Raw/unprocessed,FALSE,,"Existing methods and planned approaches to evaluate the model/system performance:
Currently with MVP and prompt engineering the evaluation was based on personal interviews, and before/after impressions (Qualitative and human validation). 
For the evaluation of the developed model and system, we plan to use the following: 
 - Use of “golden” data set of what we consider as good matches and principles of good relationship building to train and supervise results
- Human-in-the-loop validation - based on user input
- Pre/post comparisons and dashboard indicators
- Quantitative indicators of success, such as number of meetings during events, matches rating, number of logins, number of premium upgrades, churn, etc.","We are currently in the initial phase of exploration and consideration, and still need support to finalize the workflow. We plan to create a high-quality dataset of what makes a great match, using Vertex AI Data labeling and AI pipelines, and add logged user feedback (such as ""good match,"" ""bad match"") into BigQuery. Once we have thousands of labeled examples in BigQuery, train the model on the specific nuances of our matching and relationship building logic. We use LLM and generative AI for curated conversation starters between recommended matches, for follow ups and relationship building pipelines.","We develop the model based on our own biases of what is considered as a good/bad match and need to keep in mind to make it unbiased as possible, using the implementation of user feedback loops as well as to develop various indications to alert and expose alarming patterns. When matching people, we need to take in consideration gender, sexual, cultural and geographical consideration and socially accepted behaviours, as well as implementing barriers to misuse or alarming behaviours to keep the safety of users. To increase user trust we need to be as transparent as possible and share reasonings for recommended actions. We need to implement feedback loops in order to identify issues in real time. And we need to verify the implementation of privacy and safety of data.","Qualitative outcome:
Dataset ready for model training
AI model development workflow for social and professional matches including personally curated follow up
Definition of successful match (today with MVP, match rating is based on an algorithm  we defined that leads to the creation of a  good match, with no implementation of feedback loop. With user feedback, we need to consider user biases for good/bad match - and we need to further develop it and decide the weight given to each consideration (experiment with it))","Google Cloud Platform (GCP), Vector-provided infrastructure (if applicable)","technically strong AI practitioner with hands-on expertise in large language models (LLMs), natural language processing, data cleaning, and model fine-tuning. They should have practical experience in building and deploying machine learning systems, including recommendation or matching models, and be able to translate research into scalable solutions that connect people in meaningful ways. Strong programming skills (e.g., Python, PyTorch/TensorFlow), data engineering capabilities, and the ability to work collaboratively in an interdisciplinary environment","Our tech team (advisors and founders) include the following tech skills: A data analyst, product manager, solution architect, QA, programming. We are currently in the process of recruitment of two PT full stack developers. We are missing on ML engineers but have the upper level knowledge base to develop the model framework.","We hope to work with Vector and get the support we need in order to transform what we do in the real world into automation in the machine world - something that will allow us to scale. Currently, although we have the tech understanding of what we want to build - we are missing a deeper ML/AI hands on experience and expertise. We are looking for MLA that would like to build a platform that will bring people closer to each other in the real world."
Company 11,"Company 11, through its RISTs platform (Real-Time Immersive Simulation Training), is tackling critical workforce shortages in Canada’s energy, nuclear, skilled trades, and emergency response sectors. Current training methods rely on static classrooms and non-adaptive simulations that fail to address individual learning differences. Our solution deploys Agentic AI-driven non-player characters (NPCs) capable of reasoning, planning, and adapting their behaviour in real time by integrating Cognitive3D telemetry (motion, gaze, task flow), trainer dashboard inputs, and optional biometric signals (e.g., EEG headbands such as Muse) to capture attention, stress, and cognitive load. This multi-stream approach ensures NPCs respond dynamically while instructors retain oversight and actionable analytics. Trainees and instructors at Durham College’s CoreLab and partner TACs will use this system to accelerate onboarding, reduce errors, and improve retention. The Canadian nuclear sector alone faces 10,000+ skilled worker shortages by 2030, and the skilled trades are forecast to require over 700,000 new workers in the same timeframe, while the global simulation training market exceeds $11B. This project builds on prior work developed as Texture Dynamics (TDX) during Vector’s DaRMoD program, now formalized under Company 11’s RISTs division, ensuring continuity, technical maturity, and readiness for scale.","Our objective is to develop a scalable proof-of-concept where Agentic AI-driven NPCs dynamically adapt to trainees in immersive training environments. These NPCs will reason, plan, and adjust behaviours in real time by integrating Cognitive3D telemetry (motion, gaze, task flow), trainer dashboard inputs, and optional biometric signals such as EEG headbands (e.g., Muse) to capture attention, stress, and cognitive load. The goal is to enhance decision support for instructors, reduce training time, and improve knowledge retention across the energy, nuclear, skilled trades, and emergency response sectors.

Key milestones are staged across two phases. AI Readiness (Oct–Nov 2025): finalize use case with Durham MRC and Cognitive3D, define evaluation metrics, complete dataset inventory, and deliver an AI Solution Roadmap. Execution (Jan–May 2026): develop NPC Prototype v1 with dialogue and short-term memory, advance to NPC v2 with multi-step reasoning and telemetry integration, deploy the system in Durham’s CoreLab with instructors and students, and conclude with a packaged NPC + dashboard module and evaluation report for scaling across TAC sites.

If delayed or not implemented, training programs will remain static, costly, and slow to deliver, leaving employers unable to meet urgent workforce demand. This increases safety risks, extends onboarding time, and widens gaps in critical sectors already facing acute labour shortages.","We expect the cohort to deliver a functional proof-of-concept that integrates Agentic AI-driven NPCs with real-time data streams and instructor oversight. Specific deliverables include:
•	NPC Prototype: An adaptive NPC agent with dialogue, short-term memory, and multi-step reasoning, fine-tuned on training dialogues and SOPs.
•	Telemetry Integration: Real-time ingestion of Cognitive3D motion/gaze/task flow data, plus extensibility for trainer dashboard inputs and biometric signals (e.g., EEG).
•	Trainer Dashboard v1: A live monitoring and analytics tool where instructors can observe NPC–trainee interactions and review post-session metrics.
•	Reusable Module: A packaged NPC + dashboard system that can be deployed at CoreLab Durham and extended to partner TAC sites.
•	Evaluation Report: Metrics on training time reduction, error rates, and engagement improvements, validated through instructor feedback and Cognitive3D analytics.

These outputs will demonstrate feasibility, provide instructors with actionable tools, and establish a foundation for scaling across trades and energy training environments.","Trainees and apprentices in immersive energy, nuclear, and skilled trades programs, including learners from Indigenous, rural, and equity-seeking communities.    	Instructors and training supervisors at Durham College’s CoreLab and partner TACs who use the trainer dashboard to monitor, assess, and guide learners.    	•	Employer partners (utilities, construction unions, emergency response organizations) who require scalable, adaptive training solutions to accelerate workforce readiness. Applied research partners (Durham MRC, Conestoga SMART Centre, Cambrian, La Cité) who will validate, refine, and extend deployment across multiple training environments.",Enhancement to an existing system,"Dataset(s) Description:
1.	Content & relevance: Dialogue datasets from existing immersive training modules (trainee–instructor exchanges, NPC scripts); domain-specific materials such as nuclear SOPs, safety manuals, and emergency procedures; motion and behaviour datasets (e.g., M-Body project and open-source corpora) for human movement; and real-time telemetry streams (motion, gaze, task flow) via Cognitive3D. These datasets are directly relevant for modelling NPC reasoning, adaptive responses, and evaluating trainee performance.
2.	Sources: Internal training dialogues developed within RISTs modules; structured SOPs and manuals provided by sector partners; open-source human motion datasets and M-Body project contributions; Cognitive3D’s telemetry platform integrated in immersive environments.
3.	Collection & format: SOPs/manuals are structured text; dialogues are semi-structured scripts in tabular/text format; open-source motion datasets in JSON/CSV; Cognitive3D telemetry in structured real-time event streams (JSON).
4.	Key features: Dialogue turns, task flow steps, safety procedures, gaze vectors, movement traces, completion times, attention markers, error events.
5.	Size: Training dialogues (~5,000+ annotated exchanges, growing); SOP/manual corpus (~hundreds of documents); motion datasets (tens of thousands of records across multiple subjects); Cognitive3D telemetry (millions of events collected per training session, continuous stream).",Partially annotated,FALSE,,"Our evaluation framework will be co-designed with instructors and sector partners to ensure outcomes are meaningful and measurable. We will establish pre- and post-training benchmarks (time-to-completion, error rates, knowledge retention) and map them against NPC interactions and telemetry signals. Trainers will be able to associate specific NPC behaviors with individual learner progress, creating the foundation for each trainee’s personalized learning LLM.

The plan includes:
•	A golden set of scripted training interactions to provide a baseline.
•	Human-in-the-loop validation by instructors at Durham MRC’s RISTs CoreLab.
•	Telemetry analytics (engagement, gaze, task flow, error frequency) via Cognitive3D.
•	Instructor dashboards to capture live feedback and post-session review.
•	Industry observation: OPG’s training department will be invited to review late-stage pilots and comment on industry relevance.

Rather than fix arbitrary numeric goals now, success will be defined by measurable improvement between pre- and post-training benchmarks, instructor validation, and alignment with industry training standards.","Yes. We are actively exploring generative AI and LLM-based techniques for this use case. Specifically, we plan to fine-tune open-source models such as Mistral and Llama-2/3, augmented with retrieval-augmented generation (RAG) pipelines built on LangChain/AutoGen. These models will be trained on domain-specific SOPs, safety manuals, and annotated training dialogues to power NPC reasoning and dialogue. We are also considering lightweight fine-tuning methods (LoRA/PEFT) to make models efficient for real-time interaction.

In parallel, we are designing an agentic architecture where NPCs use short-term and long-term memory modules, allowing them to reason across multi-step tasks while adapting to telemetry signals (motion, gaze, task flow). The MLA cohort will help refine this orchestration and ensure responsible use of generative AI in immersive training.","Yes. We are addressing several ethical, legal, and societal considerations as part of this project:
•	Fairness & Bias: NPC behaviour will be validated with input from instructors and advisors representing diverse learner groups, including Indigenous and equity-seeking communities in the trades.
•	Transparency & Trust: All NPC interactions will be auditable. Trainers can review decisions in real time via the dashboard, ensuring human oversight and avoiding “black box” AI.
•	Data Privacy: No personally identifiable information (PII) is used. All datasets are synthetic, anonymized, or derived from non-sensitive training content. Biometric signals, if integrated, will be optional and fully consent-based.
•	Human-Centric Design: NPCs are designed to augment, not replace instructors. The system strengthens trainer effectiveness while ensuring learners remain guided by qualified humans.
•	Responsible AI Principles: We are aligning with Canadian standards for responsible AI (fairness, accountability, transparency, safety), and we will continue engaging applied research, training, and community advisors to ensure societal values are embedded throughout development.","Success will be measured through a combination of baseline metrics, instructor validation, and learner outcomes. We will establish pre- and post-training benchmarks (time-to-completion, error rates, knowledge retention) and compare improvements once NPCs and dashboards are integrated. Instructor dashboards and Cognitive3D analytics will provide quantitative measures of engagement, task accuracy, and attention. Human-in-the-loop validation will ensure NPC behaviour supports learning goals and aligns with training standards.

By operating within the RISTs CoreLab, we can iterate measurements in a controlled environment, refining evaluation criteria with instructors and learners until they align with the expectations of industry partners such as OPG and Bruce Power, trade unions, and regulators including nuclear safety commissions. This approach ensures outcomes are not only technically valid but also match the compliance and workforce readiness needs of safety-critical sectors.

Ultimately, success will be demonstrated by measurable improvements against baseline training results, combined with validation from instructors, industry partners, and regulatory stakeholders that the solution is ready for scaling.","Internal on-premise or virtual private servers, Vector-provided infrastructure (if applicable), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Google Cloud Platform (GCP)","The ideal candidate for this project would have experience with LLM fine-tuning, prompt engineering, and retrieval-augmented generation (RAG), along with skills in simulation/NPC modelling, real-time data integration, and multimodal AI (e.g., video/vision analysis and speech/voice generation). Familiarity with tools such as LangChain, Hugging Face, Unity, and telemetry platforms would be valuable. The candidate will work as part of the RISTs CoreLab team, collaborating with developers, instructors, and industry partners to apply AI in immersive training and workforce development contexts.","Our technical capacity is structured around a lean internal team supported by strong applied research and industry partners. The team includes an immersive developer, a backend engineer, and an AI/ML technical lead guiding model development and data strategy.

The RISTs CoreLab @ Durham MRC/AI Hub will provide an additional AI layer during the MLA use case, ensuring integration with applied research. Cognitive3D is also contributing in-kind support and deploying their telemetry/analytics solution within CoreLab, giving us access to their platform and AI expertise.

We are prepared to bridge resources and shuffle financial priorities to ensure the MLA engagement is fully supported, even if external funding timelines shift. This approach, combined with consulting revenue opportunities and IRAP support already in development, ensures we can sustain the work and expand into a dedicated team in 2026.","Company 11 RISTs is building a replicable CoreLab model for immersive training, starting with energy and nuclear and expanding to trades and other safety-critical sectors. The MLA engagement will accelerate the development of our AI/NPC engine and help establish success criteria that align with industry and regulatory partners.

This work directly connects to our Skills Development Fund (SDF) focus and ongoing engagement with energy sector companies (OPG, Bruce Power) and trade unions (CBTU) to shape workforce training initiatives. By aligning AI-enabled training with both employer and labour needs, we ensure solutions are technically sound, industry-relevant, and workforce-validated.

We are supported by partnerships with Durham MRC/AI Hub, Conestoga SMART Centre, and Cognitive3D, with in-kind contributions and applied research resources already in place. As founder and technical lead, I am committed to bridging resources and shuffling priorities to ensure MLA success.

This timing is perfect within our R&D roadmap, providing the missing AI layer to RISTs CoreLab activities and positioning us to scale into larger SDF, IRAP, and industry-backed initiatives."
Company 12,"“Current practices of phenotypic term extraction in the absence of automation are labor-intensive and put a significant burden on subject matter experts in the diagnostic lab. This is inherently at odds with the increasing utilization of genomic sequencing across many healthcare specialties which requires scalability of solutions to meet market demands.” (Albayrak et al 2025) 

Genomic medicine is entering mainstream care, but interpretation remains the bottleneck: accurate variant analysis depends on structured phenotypic data, yet requisitions are often incomplete, unstructured, and still on paper. Additionally, manual entry of features and symptoms as Human Phenotype Ontology (HPO) codes, the preferred format for laboratory analytics platforms, are slow and inconsistently reported by clinicians, delaying care and limiting scale. 

Our solution, SmartRequisition, is an AI-powered phenotyping engine that automatically extracts HPO terms from multimodal sources (3D scans, dictation, free text) and outputs structured, lab-ready requisitions. Integrated within Company 12s’ clinical genomics workflow software, the tool reduces clinician burden, ensures high-quality requisition data ready for downstream analysis, and accelerates genetic test interpretation.

Impact:
Clinicians → reduced burden of data entry.
Laboratories → structured requisitions that improve test accuracy.
Patients → faster, more accurate diagnoses.

The focus of the Vector Fastlane 4-month project will be building the clinician-in-the-loop UI and developing an AI model that continuously learns from this feedback, directly tackling clinician burden and ensuring more accurate, structured requisitions.","SmartRequisition is part of a two‑year larger project vision with clinical rollouts at hospitals across the county. However, within the shorter Fastlane 4-month timeline, we will concentrate on a subset of the project goals, specifically advancing the clinician-in-the-loop UI and developing an AI model that continuously learns from this feedback. The Fastlane project will:

1. Build the clinician‑in‑the‑loop UI to enable validation and feedback on AI suggestions.

2. Deliver an AI model that continuously/regularly learns from this feedback, ensuring measurable improvement over time.

3. Enable multi‑modal extraction, including free text, clinician note uploads, and dictation.

4. Produce a functional prototype integrating backend pipeline and clinician interface.","Within the 4-month Vector Fastlane cohort, the focus will be on producing a functional prototype of SmartRequisition that builds on our existing Vector Fastlane prototype. This includes a RAG-based pipeline for HPO extraction from free text, clinician note uploads, and dictation, with performance targets of ≥81% precision, ≥.76 recall, and ≥0.78 F1-score. A clinician-in-the-loop UI will enable users to review, validate, and provide feedback on AI-suggested terms, while adaptive learning capabilities will ensure the model continuously improves from this input. An integration-ready backend will demonstrate how structured, lab-ready requisitions can be output for downstream analysis.",Clinicians and geneticists completing genomic test requisitions.,Enhancement to an existing system,"CHU-50 Dataset: 
Contents & Relevance: 50 curated, de-identified clinical summaries that have been manually coded with HPO terms. These are particularly challenging cases, making them valuable for stress-testing the extraction model.
Source: Partner access
Format: Unstructured clinical text paired with structured HPO annotations.
Key Features: Narrative summaries of patient histories, symptoms, and clinical observations; corresponding HPO-coded phenotypic terms.
Size: 50 records, highly curated and high value for benchmarking.

Synthetic Rare Disease Dataset (Generated via GenAI)
Contents & Relevance: Synthetic clinical notes generated from ~6,000 example rare disease cases. Provides scale for developing and refining extraction pipelines without exposing sensitive patient data.
Source: Synthetic data generation using LLM-based tools seeded with open clinical knowledge bases.
Format: Free-text clinical notes with phenotype descriptors; paired with automatically or semi-automatically generated HPO annotations.
Key Features: Phenotype descriptions, demographic context, clinical indications.
Size: ~6,000 synthetic records.

Reference Knowledge Bases
Human Phenotype Ontology (HPO): Open-source ontology (>15,000 phenotype terms) used as the target vocabulary for extraction.
Other Genetics Resources (e.g., ClinVar, Orphanet, OMIM, Monarch Initiative): Structured phenotype-genotype associations used to augment model accuracy.",Fully labeled for ML tasks,FALSE,,"Evaluation will combine benchmarking, clinician validation, heuristics, and continuous monitoring. 

We will use the CHU-50 dataset as a gold standard, targeting ≥81% precision, ≥.76 recall, and ≥0.78 F1-score, and validate consistency across ~6,000 synthetic rare disease cases to test robustness across free text, notes, and dictation. 

Clinicians will provide feedback through a prototype UI that allows them to accept, reject, or edit AI-suggested terms, with metrics such as acceptance rates, correction times, and usability feedback. 

Heuristics like confidence thresholds and plausibility checks will reduce errors, while dashboards will track accuracy, feedback trends, and iterative model improvement over time.","Yes. SmartRequisition leverages techniques from Generative AI and Large Language Models (LLMs) to support accurate extraction of Human Phenotype Ontology (HPO) terms from clinical text. 

Specific approaches for this project include: Retrieval-Augmented Generation (RAG) Pipeline, Generative AI for Synthetic Data Augmentation and Evaluating multi-modal LLMs for handling dictation and note uploads alongside free-text.","Yes. Patient privacy is protected by keeping all raw data under hospital custodianship and using only de-identified or synthetic data for development. Bias risks are addressed by incorporating diverse datasets, while transparency is ensured by grounding outputs in the Human Phenotype Ontology (HPO) and allowing clinicians to review and edit AI suggestions. The system is an assistive tool, not a replacement for clinical judgment, supporting user trust. Development follows applicable privacy laws (PHIPA, HIPAA, GDPR) and Canadian ethical standards (TCPS2), ensuring compliance and societal acceptability.","For the 4‑month Vector Fastlane cohort, success will focus on advancing the clinician-in-the-loop UI and developing an AI model that continuously learns from this feedback

Key indicators include:

Model Accuracy: Build on existing prototype. Demonstrate ≥81% precision, ≥.76 recall, and ≥0.78 F1‑score for HPO term extraction using the CHU‑50 benchmark dataset and synthetic rare disease cases.

Clinician Feedback Loop: Deliver a functional prototype where clinicians can accept, reject, or edit AI suggestions, with measurable feedback on usability and trust.

Learning from Feedback: Show evidence that the model improves with iterative clinician input (e.g., reduction in edit rates across evaluation rounds).

Multi‑modal Support: Successful extraction of HPO terms from at least two input types (free text + clinician notes or dictation).","Google Cloud Platform (GCP), Amazon Web Services (AWS), Internal on-premise or virtual private servers, Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Microsoft Azure","The ideal candidate for this project would have experience with NLP, LLMs, and RAG pipelines, ideally applied to biomedical text, along with skills in prompt engineering, model evaluation, clinician-in-the-loop systems, and prototype UI development.","Our technical team consists of 6 engineers and 1 product lead. The engineering team includes expertise in machine learning, NLP/LLMs, backend development, and clinical genomics integration, ensuring coverage of both AI pipeline development and system interoperability.",This project builds on the successful outcomes of our previous Vector Fastlane cohort.
Company 13,"We aim to add a conversational AI layer to our system of record platform for enterprise communications teams. This will open up more sales avenues - particularly with smaller organizations who will be able to get more utility out of the platform than they could currently. We sell to communications teams in all sectors, but our primary focus has been the public sector so far.","Re-envision user interaction with communications data, enabling language oriented professionals to have a powerful, data-secure LLM embedded in their workflow. If this project is not pursued, we risk falling behind competitors who are building out AI tooling. We have a product lead in this area and need to retain it. Much of our marketing ties to the company's AI story.","We envision the deliverable as a functional prototype, though the MLA will not be responsible for developing the front-end components, just the NLP and how it interfaces with our backend. Specifically, this will be a conversational agent that can help access different views of our CRM software (e.g. ""show me all media interactions from 2025 from Forestry that had to do with wildfires""), our analytics (e.g. ""show me a piechart of media interactions with Canadian outlets versus international in 2024""), and will help with drafting documents based on contents of our CRM (e.g. ""make me a media release for this story with a similar style to prior releases about similar topics"").",External users - Communications directors and their media relations teams,Enhancement to an existing system,"1) What the dataset contains and its relevance to the use case.

The dataset will be any relevant data within our web application, including all CRM data collected over the past five years. We may restrict the data to a certain client (tenant). 

2) Source(s) of the dataset (e.g., internal CRM, third-party APIs, web scraping, user-generated input, etc.).

Data is either user-generated/entered, or scraped from emails by our existing AI tooling. 

3) How the data was collected and its current format (structured, unstructured, tabular, text, etc.).

Most data is structured, within MongoDB. There are also stored unstructured documents, including emails and docx/pdf that represent media relation specialist outputs. 

4) Indicate the key features or fields present in the dataset.


5) Approximate size (e.g., number of records, file size, time range).

Entries in our CRM (Services, Media Interactions, Engagements) number >10,000. These are primarily from the last 3 years, though some entries go back 5 years in total.

This is the data point to correct from last submission: We have over 100,000 client records that we can use for AI and data analysis",Partially annotated,FALSE,,"For all tasks, we will create expected question-answer pairs that will include scenarios, example user input, and the expected output (or more specifically, the expected filtering that should be applied to the data). For the CRM data filtering tasks, this can be a somewhat quantitative/automated, as we can set up an evaluation system that can look at the input and the filter output. This will also be the case for applying analytics e.g. building a certain pie chart. 

For document generation, this will be a bit more complicated. As it will be a RAG system, we could quantitatively evaluate the document retrieval e.g. if with a certain user input, we can check that certain relevant documents are being retrieved to use for drafting the output document. We will also want to evaluate the final drafted document, which will be qualitatively assessed by a human expert - we would have such raters readily available to help measure performance, and are happy to help develop an evaluation scheme (correctness, relevance, safety, etc).",We have a current initial AI system that uses LLaMA 3 with some prompting and few shot learning to scrape structured data from emails. This is deployed on Amazon Bedrock. This system is working quite well and could be used as a foundation/influence for this much more complex project.,Regular data privacy concerns that any enterprise has with cloud based systems,Utilization rate by clients,Amazon Web Services (AWS),"The ideal candidate would have experience and interest in NLP pipelines used for predictive, generative, and extractive tasks, and be comfortable working with a mix of both structured and unstructured data. They would enjoy collaborating with our front end/backend developers to integrate the NLP pipeline into out software so that their work could be integrated promptly. A nice-to-have would be familiarity with AWS or a similar system as that’s how we’ve currently deployed our NLP tool, but we would be happy to introduce them to our system.","Three software developers plus our CTO, who is an an AI researcher with a background in NLP. Occasionally we engage co-op students also","1) What the dataset contains and its relevance to the use case.

The dataset will be any relevant data within our web application, including all CRM data collected over the past five years. We may restrict the data to a certain client (tenant). 

2) Source(s) of the dataset (e.g., internal CRM, third-party APIs, web scraping, user-generated input, etc.).

Data is either user-generated/entered, or scraped from emails by our existing AI tooling. 

3) How the data was collected and its current format (structured, unstructured, tabular, text, etc.).

Most data is structured, within MongoDB. There are also stored unstructured documents, including emails and docx/pdf that represent media relation specialist outputs. 

4) Indicate the key features or fields present in the dataset.


5) Approximate size (e.g., number of records, file size, time range).

Entries in our CRM (Services, Media Interactions, Engagements) number >10,000. These are primarily from the last 3 years, though some entries go back 5 years in total.

This is the data point to correct from last submission: We have over 100,000 client records that we can use for AI and data analysis"
Company 14,"The Cognitive Coach AI addresses a critical challenge in the eldercare and healthcare industry: the rising need for effective, personalized cognitive support for older adults experiencing cognitive decline, particularly mild cognitive impairment (MCI). As the aging population grows, caregivers—often family members—face mounting stress and uncertainty in providing consistent, meaningful engagement for their loved ones. This AI-driven agent offers an opportunity to transform caregiving by enabling the creation of daily, individualized cognitive stimulation plans, including memory games, conversations, and adaptive exercises.

Targeting both informal caregivers and clinical professionals, the solution not only reduces caregiver burden but also enhances the autonomy and cognitive engagement of older adults. In addition, it delivers valuable insights to clinicians, supporting earlier interventions and potentially delaying the progression of cognitive decline. Operating within the eldercare and digital health industries—markets that are expanding rapidly and increasingly influenced by regulatory oversight—the Cognitive Coach AI offers a scalable, tech-enabled approach to improving quality of life and care outcomes.","Objectives:

Build a New Product (Cognitive Coach AI MVP):
Develop a functional minimum viable product that enables users (caregivers) to generate personalized cognitive stimulation plans tailored to an older adult’s cognitive profile.

Automate a Manual and Stressful Process:
Replace the current ad-hoc or manual creation of activity plans with an AI-driven system that automates plan generation based on cognitive profiles, preferences, and progress tracking.

Enable Exporting to PDF for Easy Sharing:
Provide a user-friendly feature to export cognitive stimulation plans in PDF format, making it easy to share with family members or healthcare professionals.

Improve Decision Support for Caregivers and Clinicians:
Offer actionable insights based on user data to help caregivers make better daily decisions and assist clinicians in monitoring and adjusting care strategies.

Success Milestones:

User Onboarding Flow Implemented:
Caregivers can create a user profile and input cognitive characteristics of the older adult.

Plan Generation Engine Functional:
AI-generated daily cognitive stimulation plans (including games, exercises, and conversation topics) based on the profile.

PDF Export Capability Working:
Users can export the generated plan in a clean, professional PDF format suitable for offline use and sharing.

User Testing with Initial Feedback:
MVP is tested with a sample group of caregivers; feedback is collected for usability, usefulness, and quality of generated plans.

Clinician Review Functionality (Optional for MVP):
Basic functionality for clinicians to view or receive reports from caregivers, potentially guiding future iterations.

Importance of the Project:

Consequences of Not Solving the Problem:

Caregivers will continue to experience high levels of stress, uncertainty, and time burden when supporting older adults with cognitive decline.

Older adults may receive inconsistent or suboptimal cognitive engagement, potentially accelerating cognitive deterioration.

Clinicians will lack timely, structured insights into patient engagement and cognitive changes outside clinical settings.

Impact of Delay:

Missing the opportunity to establish early user adoption and feedback in a rapidly growing eldercare tech market.

Delayed validation of the solution’s value proposition, potentially slowing down funding, partnerships, or regulatory approval processes.

Competitive risk as other solutions emerge in the cognitive health support space.","Within the scope of the cohort, the goal is to deliver a functional and testable MVP that integrates seamlessly into the existing Company 14 Core UI, enabling users (caregivers) to generate personalized cognitive stimulation plans. The key deliverables fall into two main categories: AI Agent capabilities and integration components.

1. AI Agent (Core Deliverable) - Delivered as an API or MCP server

A. 360° Cognitive Profile Generator

Input: User-provided data (e.g., observed behaviors, existing diagnosis, self-assessments).
Output: Structured cognitive profile graph capturing key dimensions such as memory, attention, language, executive function, mood, etc.

Capabilities:
Adaptable to varying levels of input detail.
May include a scoring or tagging system to define cognitive strengths and deficits.
Stores profiles for future personalization.

B. Cognitive Stimulation Plan Generator

Input: Cognitive profile from (A), user preferences, and context (e.g., daily schedule, caregiver goals).
Output: A personalized daily or weekly plan including:
Memory games
Cognitive exercises (e.g., logic puzzles, sequencing tasks)
Conversation prompts (aligned with personal history or interests)

Output Format: Structured JSON or other standard format for rendering in UI and exporting as PDF.

2. API or MCP Server - To expose the AI Agent for use by the front-end and external services

RESTful or gRPC API endpoints to:
Generate a cognitive profile graph.
Generate or regenerate stimulation plans.
Retrieve/export plans in PDF format.

Scalable, stateless service (ideally containerized for deployment).

Optional or Stretch Deliverables:

If time/resources allow:
Basic Feedback Loop: Allow caregivers to rate effectiveness of activities, feeding back into plan generation.
Insights Dashboard (internal tool): For clinicians or product team to monitor cognitive trends or usage patterns.","Family and Unpaid Caregivers (Primary Target Audience) These individuals are the main users of the solution, directly benefiting from the AI-generated cognitive profiles and stimulation plans for older adults with mild cognitive impairment (MCI) or early dementia.  Who they are:  Spouses, adult children, friends, and neighbours.  Majority are middle-aged (45–65), with many also being older adults (65+) themselves.  ~4 million unpaid caregivers in Ontario support family and friends. Source: https://ontariocaregiver.ca/wp-content/uploads/2024/03/OCO_Impact_Report_English_2022_June29_FINAL-s.pdf   Key stats:  97% of adults receiving long-stay home care in Ontario have at least one unpaid caregiver. Source: https://www.hqontario.ca/portals/0/documents/system-performance/reality-caring-report-en.pdf   Over 58% of caregivers report feeling burnt out; more than half report worsening mental health. Source: https://ontariocaregiver.ca/wp-content/uploads/2024/03/OCO_Impact_Report_English_2022_June29_FINAL-s.pdf   Caregivers typically provide 11–30 hours/week of support. Source: https://ontariocaregiver.ca/wp-content/uploads/2024/03/Spotlight-on-ontarios-caregivers-2019_EN.pdf   Needs this solution addresses:  Reduce emotional and cognitive load through personalized daily activity planning.  Improve quality of care and maintain older adults’ independence.  Provide structured documentation (PDFs) for sharing with clinicians or other family members.",New standalone tool or feature,"A) Datasets for 360° Cognitive Profiles
1. NeuroCognitive Performance Test (NCPT)

What it contains & relevance:

Over 5.5M subtest scores from ~750,000 adults, across cognitive domains: memory, reasoning, attention, flexibility.

Relevant for building domain-level cognitive profiles (e.g., “memory moderate impairment, language intact”).

Source:

Proprietary test by Lumosity (Lumos Labs), released as open dataset via Nature Scientific Data.

Collection & format:

Collected from users completing web-based cognitive tasks.

Structured, tabular data with subtest scores, demographics, metadata.

Key features/fields:

Participant ID, age, gender, education.

Cognitive domain test results (reaction time, accuracy).

Timestamp (longitudinal aspect possible).

Size:

5.5M subtests, ~750k participants.

2. CoSoWELL (Cognitive & Social Well-Being in Older Adults)

What it contains & relevance:

Data from ~2,000 adults (55–84). Combines cognitive, social, emotional measures, and personal narratives.

Relevant for “360°” profiles beyond cognition (social engagement, loneliness, well-being).

Source:

Research project, open science dataset hosted via Open Science Framework / PMC.

Collection & format:

Surveys and narrative prompts completed repeatedly over years.

Mixed: structured survey data + unstructured narrative text.

Key features/fields:

Demographics (age, gender, education).

Self-rated memory, loneliness, well-being.

Narrative texts (used for linguistic/cognitive markers).

Size:

~2,000 participants, multiple survey/narrative waves.

Dataset covers several years (2017–2023 approx.).

3. Harmonized Cognitive Aging Protocol (NIA)

What it contains & relevance:

Standardized batteries of cognitive aging measures across multiple cohorts (e.g., MMSE, memory recall, executive function).

Useful for unifying different sources into a comparable profile schema.

Source:

U.S. National Institute on Aging (NIA).

Collection & format:

Derived from harmonization of survey & clinical data across major longitudinal studies.

Structured tabular format.

Key features/fields:

Test scores (MMSE, Trails, Digit Span, etc.).

Demographics and health info.

Longitudinal follow-up indicators.

Size:

Varies by contributing dataset (some >10,000 records).

Covers decades of data (1990s–present).

4. English Longitudinal Study of Ageing (ELSA)

What it contains & relevance:

Biannual study of adults 50+ in England. Includes cognitive tests, health, psychosocial factors, economics.

Directly supports 360° profiling with multidomain measures.

Source:

UK Data Service / NatCen.

Collection & format:

Survey interviews, nurse assessments, linked admin records.

Structured (SPSS/Stata datasets).

Key features/fields:

Demographics, memory tests, verbal fluency, numeracy.

Health conditions, ADLs/IADLs.

Psychosocial well-being, income/education.

Size:

~18,000 participants, repeated waves since 2002.

5. NACDA (National Archive of Computerized Data on Aging)

What it contains & relevance:

Large repository of U.S. and international aging datasets. Covers cognition, functional health, caregiving, demographics.

Useful as a “meta-source” to pull different cognitive/aging datasets.

Source:

ICPSR, University of Michigan.

Collection & format:

Varies by study — includes surveys, clinical, longitudinal.

Structured (SPSS, Stata, CSV).

Key features/fields:

Depends on dataset (often MMSE, demographics, ADLs, etc.).

Size:

Hundreds of datasets; many with 1,000–20,000 participants.

6. Singapore Longitudinal Study of VCI & Dementia

What it contains & relevance:

Cohort with vascular cognitive impairment, MCI, and dementia diagnoses.

Relevant for distinguishing diagnostic categories in profiles.

Source:

NIAGADS (National Institute on Aging Genetics of Alzheimer’s Disease Data Storage Site).

Collection & format:

Clinical diagnoses, neuropsychological testing.

Structured medical data.

Key features/fields:

Diagnosis (normal, MCI, dementia).

Cognitive test scores.

Demographics.

Size:

Few thousand participants, longitudinal.

B) Datasets / Sources for Cognitive Stimulation Plans
1. Cochrane Review – Cognitive Stimulation for Dementia

What it contains & relevance:

Meta-analysis of dozens of randomized controlled trials. Activities: word games, puzzles, discussions, reminiscence therapy.

Relevant as evidence-based library of activity types.

Source:

Cochrane Database of Systematic Reviews.

Collection & format:

Literature synthesis.

Semi-structured, textual descriptions of interventions.

Key features/fields:

Intervention type, cognitive domain targeted, trial size/outcomes.

Size:

Dozens of trials synthesized; thousands of participants collectively.

2. Serious Game for Cognitive Stimulation (Morán et al.)

What it contains & relevance:

A digital serious game tested with older adults with MCI. Includes daily-life simulation activities (shopping, organizing).

Relevant as activity templates linked to deficits.

Source:

ResearchGate preprint / usability study.

Collection & format:

Experimental pilot with older adults.

Semi-structured text + app/game interface.

Key features/fields:

Task descriptions, targeted cognitive functions, usability outcomes.

Size:

Pilot study (~30 participants).

3. Cognitive Interventions for Healthy Older Adults (Review, Velloso et al.)

What it contains & relevance:

Review of unimodal (single domain) and multimodal (combined) interventions in healthy older adults.

Useful for preventative plans for pre-MCI users.

Source:

ScienceDirect / Elsevier.

Collection & format:

Literature review (structured summaries).

Key features/fields:

Intervention type, delivery mode, cognitive domain improved.

Size:

Dozens of interventions covered.

4. Scoping Review: Cognitive Stimulation + ADLs (Ryan et al.)

What it contains & relevance:

Integration of cognitive stimulation with daily activities (e.g., cooking, dressing).

Useful for embedding plans into real-world routines.

Source:

SAGE Journals.

Collection & format:

Literature scoping review.

Key features/fields:

Activity descriptions, ADL context, cognitive function targeted.

Size:

Dozens of studies synthesized.

5. Brain Stimulating Games & Activities (The CareSide)

What it contains & relevance:

Practical list of games (puzzles, music, creative activities).

Useful for generating friendly, caregiver-facing activity suggestions.

Source:

Caregiving website (The CareSide).

Collection & format:

Curated article, semi-structured list format.

Key features/fields:

Activity name, description, domain targeted (implicit).

Size:

Tens of activities listed (small but usable as seed content).

In summary:

For 360° profiles: NCPT, CoSoWELL, ELSA, NACDA, Harmonized Cognitive Aging Protocol are the richest, structured sources.

For stimulation plans: Cochrane Review, Serious Game studies, ADL reviews, and curated caregiver resources provide activity templates.",Raw/unprocessed,FALSE,,"Evaluation Plan for Cognitive Coach AI MVP
1. Golden Dataset / Benchmark Set

Approach:

Build a small, curated benchmark dataset of cognitive profiles + expert-designed stimulation plans.

Sourced from clinician input, published activity guides (e.g., Cochrane Review, Serious Game studies), and adapted caregiver manuals.

Use:

Fine-tuned model outputs compared against “gold-standard” plans for relevance, appropriateness, and personalization.

Metrics: BLEU/ROUGE (surface similarity), semantic similarity scores, and domain-specific scoring (e.g., coverage of cognitive deficits).

2. Human-in-the-Loop Validation

Approach:

Involve clinicians (geriatric specialists, occupational therapists) and family caregivers in reviewing generated plans.

Collect ratings on dimensions like:

Clinical appropriateness (does it match the profile?)

Practical usability (is it doable for caregivers/older adults?)

Engagement potential (is it enjoyable and motivating?)

Use:

Continuous feedback loop to refine prompts, rules, and fine-tuning data.

Score threshold (e.g., ≥4/5 rating on usefulness) set as success metric.

3. Heuristics & Business Rules

Approach:

Layer simple rules on top of model output to ensure safety and usability. Examples:

No activity exceeds safe physical limits for older adults.

Always balance 2+ activity types (memory + conversation + light exercise).

PDF output must contain structured sections (morning / afternoon / evening).

Use:

Rule-based checks run automatically before presenting plan to end users.

Helps maintain reliability in MVP stage.

4. Pre/Post Comparisons & Metrics Tracking

Approach:

Pre/post caregiver stress surveys (e.g., Zarit Burden Interview short form).

Engagement tracking: Caregivers log which activities were completed vs skipped.

Clinical proxy metrics: Self-reported observations of mood, memory, or independence.

Use:

Compare outcomes over a 4–6 week pilot.

Success if caregivers report reduced stress and older adults show stable or improved engagement.

5. Dashboards & Continuous Monitoring

Approach:

Develop internal dashboard showing:

Frequency of generated plans.

Distribution of activity types suggested.

Caregiver feedback ratings.

Error or override cases (where caregiver rejects or modifies plan).

Use:

Provides visibility into adoption, satisfaction, and model consistency.

6. Qualitative Indicators of Success

Caregiver testimonials (reduced uncertainty, easier planning).

Clinician endorsements (plans align with standard of care).

Adoption metrics: repeat use, number of exported PDFs per week.

Caregiver retention: willingness to continue using after MVP trial.","Yes — we are actively considering generative AI and LLM techniques for this use case. Specifically:

Instruction-tuned LLMs (GPT-4, LLaMA-2, Mistral) for personalized plan generation.

Domain-specific fine-tuning with cognitive profile + activity datasets.

Retrieval-Augmented Generation (RAG) to ground outputs in evidence-based caregiver resources.

Multi-agent design to separate profiling, plan generation, and formatting.

Safety guardrails + human-in-the-loop validation to ensure trustworthiness.

Explored / Under Consideration

Instruction-tuned Large Language Models (LLMs):

Candidates: GPT-4, LLaMA-2, Mistral-7B/13B, Falcon, and fine-tuned variants on healthcare/eldercare data.

Relevance: These models are effective at generating structured text (e.g., personalized daily plans, conversation prompts, caregiver summaries).

Approach: Use a base model, then fine-tune or prompt-engineer with domain-specific datasets (e.g., cognitive assessment data, evidence-based activity libraries).

Domain-specific Fine-Tuning / Instruction Alignment:

Build a synthetic dataset combining (A) cognitive profiles (from NCPT, ELSA, CoSoWELL, etc.) with (B) cognitive stimulation plan examples (from Cochrane reviews, serious games, caregiver manuals).

Fine-tune model to learn the mapping: profile → personalized plan.

Use instruction-response pairs:

Instruction: Generate a cognitive plan for an 80-year-old with mild memory impairment, who enjoys music and gardening.  
Response: [plan with memory game, music therapy, gardening conversation topic].  


Retrieval-Augmented Generation (RAG):

Store a knowledge base of clinically validated activity templates (from caregiver orgs, Cochrane, dementia activity guides).

LLM generates plans by retrieving relevant activities for a given profile, ensuring evidence-based grounding and avoiding hallucinations.

Multi-Agent / Modular Design:

Agent 1: Profile Builder – generates structured 360° cognitive profile from caregiver inputs.

Agent 2: Plan Generator – maps profile → tailored stimulation plan.

Agent 3: Output Formatter – packages plan into a clean PDF for caregiver/clinician use.

Each agent could be implemented as an LLM prompt pipeline or microservice behind an API/MCP server.

Evaluation / Safety Enhancements:

Use rule-based guardrails (heuristics layered on top of LLM output) to ensure suggested activities are safe, appropriate, and balanced (e.g., no physically demanding activities for frail users).

Optionally integrate human-in-the-loop review for clinician validation during pilot phase.","Yes, there are important considerations in deploying generative AI in this space:

Privacy & compliance (PHIPA/HIPAA).

Bias & fairness (cultural, linguistic inclusivity).

Safety guardrails to prevent harmful suggestions.

Transparency & trust (explainable recommendations, caregiver control).

Legal framing as a supportive tool, not diagnostic.

Addressing these upfront will ensure trust, adoption, and compliance, while aligning with the project’s mission of supporting caregivers and older adults ethically.

Ethical, Legal, and Societal Considerations
1. Data Privacy & Security

Issue: Cognitive health data (e.g., memory scores, daily activities) is highly sensitive and may qualify as personal health information (PHI) under HIPAA (U.S.) or PHIPA (Ontario).

Mitigation:

Encrypt all stored and transmitted data.

Use anonymization/pseudonymization where possible.

Store data in compliance with local regulations (e.g., Canadian servers for Ontario users).

Explicit caregiver and patient consent before data use.

2. Fairness & Bias

Issue: LLMs may embed cultural, language, or demographic biases that could produce irrelevant or insensitive activity suggestions (e.g., Western-centric conversation topics not suited for multicultural Ontario).

Mitigation:

Curate training/fine-tuning data with cultural and linguistic diversity in mind.

Incorporate user customization fields (e.g., preferred language, cultural background, hobbies).

Periodic audits of output to detect systemic biases.

3. Safety & Clinical Appropriateness

Issue: AI-generated activities must not put older adults at risk (e.g., suggesting strenuous exercises for someone with mobility issues).

Mitigation:

Apply rule-based guardrails (e.g., only low-impact activities for frail or mobility-limited users).

Human-in-the-loop validation by clinicians during pilot phase.

Clear disclaimers that this is a support tool, not a replacement for medical advice.

4. Transparency & Explainability

Issue: Caregivers and clinicians may distrust “black-box” AI recommendations if they cannot see why an activity was suggested.

Mitigation:

Provide explanations alongside plans (e.g., “This activity was suggested because it supports memory recall and matches the user’s interest in gardening”).

Show links to validated sources (Cochrane reviews, caregiver guides) when applicable.

5. User Trust & Autonomy

Issue: Caregivers must feel the AI supports, not replaces, their judgment. Older adults must not feel infantilized or stigmatized.

Mitigation:

Position AI as a care partner tool that reduces caregiver stress.

Allow caregivers to edit, reject, or customize plans easily.

Involve caregivers and older adults in co-design to ensure usability and dignity.

6. Legal & Regulatory Compliance

Issue: Depending on positioning, the solution may fall under digital health tool regulations.

Mitigation:

MVP framed as a care support tool, not a diagnostic device.

Monitor evolving guidance on AI in healthcare (e.g., FDA’s digital health framework, Health Canada’s SaMD regulations).","1. Baseline Metrics (Before MVP Deployment)

Caregiver Stress Levels: Use validated tools such as the Zarit Burden Interview (short form) or OCO survey benchmarks (e.g., 58% of Ontario caregivers report burnout).

Engagement Gaps: % of caregivers reporting uncertainty about how to keep loved ones cognitively engaged.

Clinician Insight Gap: % of clinicians reporting limited visibility into daily cognitive activities outside of appointments.

2. Key Performance Indicators (KPIs)
User Adoption & Engagement

% of caregivers who successfully generate a 360° cognitive profile.

% of caregivers who generate and export at least one personalized stimulation plan (PDF).

Retention rate: # of caregivers using the tool weekly over 4–6 weeks.

Plan Quality & Appropriateness

Caregiver ratings (Likert scale 1–5) of plan usefulness, relevance, and feasibility.

% of plans rated ≥4/5 by caregivers.

Clinician validation: % of plans judged clinically appropriate in pilot reviews.

Caregiver & Older Adult Outcomes

Change in caregiver stress score (pre vs. post pilot).

Caregiver-reported reduction in time spent planning activities.

Caregiver perception of older adult engagement and independence (self-reported improvement).

System Reliability

% of plan generations that meet business rules (balanced activity mix, safe suggestions).

Latency of plan generation (e.g., <5 seconds).

PDF export success rate (% error-free exports).

3. Qualitative Outcomes

Caregiver testimonials: Evidence of reduced uncertainty, improved confidence, and reduced stress.

Older adult feedback: Reports of enjoyment, sense of independence, or emotional well-being.

Clinician feedback: Evidence that plans provide actionable insights between visits.

4. Success Targets (MVP Phase)

80%+ of caregivers report plans are “useful” or “very useful.”

50%+ reduction in time caregivers spend planning daily activities.

20% improvement in caregiver stress score (baseline vs. post).

≥75% of clinicians validate plans as appropriate for patient’s cognitive profile.

≥70% caregiver retention after 4 weeks of MVP use.","Vector-provided infrastructure (if applicable), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Amazon Web Services (AWS), Google Cloud Platform (GCP)","The ideal candidate for this project would have hands-on experience with LLMs, prompt engineering, and fine-tuning, coupled with strong skills in NLP, data cleaning, and retrieval-augmented generation (RAG). Extra expertise in building graphs and knowledge graphs for representing cognitive profiles, activity mappings, and caregiver-clinician insights would be highly valuable.","Our team consists of three core technical members supported by a product manager:

2 AI/ML Engineer – Leads development of the cognitive profiling and stimulation plan generation agent, including LLM fine-tuning, prompt engineering, and integration of retrieval/knowledge graph components.

2 Full-Stack Developers – Responsible for building and maintaining the backend services (API/MCP server), integrating with the Company 14 Core UI, implementing PDF export functionality, and ensuring security and scalability.

1 Product Manager – Manages priorities, defines requirements in collaboration with caregivers and clinicians, and ensures alignment between technical delivery and user needs.","The Cognitive Coach AI project addresses an urgent and growing need: in Ontario alone, there are ~4 million unpaid caregivers, and over 58% report burnout, while the prevalence of cognitive impairment in home care patients has risen from 38% to 62% in just five years. Family caregivers often struggle with uncertainty and stress when designing meaningful daily activities, while clinicians have limited visibility into cognitive engagement outside of clinical settings.

This project leverages Generative AI and LLMs to bridge that gap by:

Automating the generation of personalized cognitive profiles and evidence-based stimulation plans.

Reducing caregiver stress and supporting older adults’ independence.

Providing clinicians with structured insights to inform care decisions.

We believe this use case is particularly well-suited for generative AI because it involves pattern recognition across cognitive domains and creative, yet safe, generation of activity plans. With the MVP, we aim to validate feasibility, demonstrate user value, and set the foundation for scaling into a clinical-grade support system."
Company 15,"Company 15 is tackling one of the most fundamental challenges in scaling computer vision: inconsistent, low-quality image capture. Without high-quality, standardized data, AI models fail to deliver accurate results, slowing adoption across industries that depend on visual workflows.

Our solution is an agentic, browser-native capture co-pilot that runs directly in a user’s browser (via WASM + Rust) and leverages quantized language models (QLMs) for on-device reasoning. The system detects issues such as blur, poor lighting, framing errors, and missing coverage, then generates adaptive, context-aware prompts in real time to guide users toward valid captures.

In parallel, Company 15 is developing 3D-to-2D alignment capabilities that transform sequential image captures into standardized inspection-ready views. This ensures every session yields a structured dataset suitable for downstream AI pipelines.

Target industries and audience:

- Fleets & logistics (drivers, fleet managers)
- Automotive remarketing (auction operators, dealerships)
- Insurance & warranty (claims adjusters, inspectors)
- Marketplaces & digital retail (merchants, operations teams)

The global automotive and logistics markets alone exceed $5T in value, with billions of images captured annually for inspections, resale, claims, and compliance. Today, 20–40% of captures require rework. By embedding agentic AI directly at the point of capture, Company 15 addresses this bottleneck and enables faster, cheaper, and more scalable AI adoption.","The main objective is to deliver a pilot-ready, browser-native agentic capture co-pilot that ensures every vehicle inspection session achieves complete coverage and outputs standardized 2D views from a 3D-aware pipeline.

Objectives:

1) Automate capture quality and coverage assurance with an in-browser agent that detects blur, glare, angle, and coverage gaps, providing adaptive real-time guidance.

2) Integrate with Company 15’s 3D engine to pass captured images into a reconstruction pipeline, validating whether full vehicle coverage has been achieved.

3) Standardize capture outputs by generating 3D-to-2D aligned views with confidence scoring for downstream AI models.

4) Deliver a pilot-ready prototype embedded in Company 15’s WASP interface that enterprises can deploy directly for real-world testing.

Success Milestones (within cohort timeframe):

Milestone 1 – Browser-Native Agentic Guidance (Month 1–2): Deploy QLM-driven agent inside WASP to provide frame-level quality feedback; target ≥30% reduction in failed captures.

Milestone 2 – Coverage Reasoning + 3D Engine Integration (Month 2–3): Enhance the agent to reason about overall vehicle coverage and feed sequential captures into the 3D engine for validation, ensuring ≥80% body coverage with ≤10% missed zones.

Milestone 3 – 3D-to-2D Alignment Prototype (Month 3–4): Output standardized 2D inspection views from validated 3D reconstructions with pose deviation ≤10° and ≥90% coverage completeness.

Milestone 4 – Pilot-Ready Prototype & Evaluation (Final Deliverable): Fully integrated agent + 3D pipeline, benchmarked for low latency (<500 ms feedback), accompanied by metrics dashboard, validation datasets, and pilot deployment guide.

Importance and Consequences of Delay:
Without this solution, enterprises will continue to suffer from incomplete and inconsistent capture datasets, with 20–40% of inspections requiring rework. This undermines trust in AI inspection workflows, increases costs, and slows deployment across fleets, automotive remarketing, and insurance. Failure to integrate coverage reasoning and 3D validation now would leave a critical gap, preventing Company 15 from achieving pilot readiness and ceding early leadership in agentic capture orchestration.","The goal is to deliver a pilot-ready, browser-native capture co-pilot that ensures complete, high-quality vehicle coverage and generates standardized 2D views from a 3D-aware pipeline.

Objectives:

1) Automate capture quality and coverage assurance by embedding a lightweight agent in Company 15’s WASP interface.

2) Pass captured images into Company 15’s 3D engine to validate coverage and reconstruct standardized views.

3) Prove feasibility of running quantized language models (QLMs) in-browser via WASM for low-latency reasoning.

4) Deliver a deployable prototype that enterprises can pilot immediately.","Frontline capture users (drivers, agents, technicians) and operational stakeholders (fleet managers, claims and remarketing teams) who rely on standardized, high-quality visual datasets.",Enhancement to an existing system,"Company 15 will leverage a combination of proprietary capture validation data and publicly available vehicle specifications to power the agentic capture co-pilot and 3D-to-2D alignment pipeline.

Proprietary data: A labeled seed dataset of vehicle images tagged as acceptable vs rejected, with metadata for side, angle, and capture conditions. This provides immediate training signals for distinguishing valid vs invalid captures.

Public data: Vehicle specifications and dimensional attributes (length, width, height, wheelbase, panel layouts) from OEM registries, open datasets, and government sources. These references enable reasoning about full coverage and accurate pose alignment.

Format & Features:

Proprietary: High-resolution images (JPEG/PNG) with structured JSON metadata (timestamp, capture angle, vehicle side, pass/fail flag).

Public: Structured tabular data (CSV/JSON) containing vehicle make/model/year and dimensional attributes.

Scale:

Proprietary: Tens of thousands of labeled captures across multiple vehicle types (~1–2 TB including images and metadata).

Public: Thousands of vehicle entries spanning 10+ years of makes/models (<1 GB structured).

Together, these datasets provide the foundation for an agent that can both evaluate capture quality in real time and validate complete vehicle coverage against reliable dimensional references, ensuring the system is cohort-ready and pilot-ready.",Partially annotated,FALSE,,"We will combine quantitative benchmarks with side-by-side testing against existing capture tools:

Golden dataset: Accuracy, precision, and recall on labeled acceptable vs rejected images.

Coverage metrics: Pose deviation (°) and % completeness from 3D-to-2D validation.

Human-in-the-loop validation: Expert reviewers confirm outputs and flag edge cases.

Side-by-side benchmark: Matched objects, devices, and routes to compare Company 15’s agentic capture against baseline tools. Metrics: reject rate, coverage completeness, latency, and time-to-complete capture.

Efficiency target: Demonstrate ≥50% reduction in capture time while also cutting reject rates by ≥50%.","Yes. We are exploring quantized LLMs (QLMs) deployed in-browser via WASM for low-latency reasoning, adaptive feedback, and real-time guidance. Models like Mistral-7B and LLaMA 3.2 are under consideration, with RAG and prompt engineering supporting agentic decision-making and coverage validation.","Yes. Our system is designed with responsible AI practices:

Data privacy: All capture data excludes personally identifiable information (PII). The 3D engine automatically applies license plate and face blurring to protect individuals before data is used for training or validation.

Fairness & bias: Models are trained on diverse datasets to avoid bias toward specific vehicles, environments, or conditions.

Transparency & trust: The agent provides clear, explainable feedback (e.g., why a capture is rejected) rather than opaque decisions.

Human oversight: Edge cases are escalated for human review to ensure accountability.

These measures ensure Company 15’s agentic guidance system is safe, privacy-preserving, and trusted by end users.","Success will be measured by:

≥50% reduction in rejected captures.

≥90% coverage completeness with ≤10° pose deviation.

≥50% faster capture time per session.

Real-time guidance (<500 ms latency) that works reliably offline or in low-connectivity environments.","Google Cloud Platform (GCP), Amazon Web Services (AWS), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Vector-provided infrastructure (if applicable), Company-hosted development environment (e.g., GitLab, JupyterHub)","The ideal candidate would have hands-on experience with LLMs and generative AI, plus familiarity with prompt engineering and lightweight model deployment. Strong skills in computer vision or multimodal AI, along with comfort using open-source tools (e.g., Hugging Face, PyTorch) for dataset preparation, evaluation, and experimentation, would be highly valuable.","Company 15’s team includes 3 core members:

Founder & Product Lead (myself): Driving technical strategy, product roadmap, and integration of agentic AI into the WASP interface; experienced in leading applied AI projects from prototype to enterprise deployment.

1 Full-Stack Developer: Skilled in both backend and frontend development, specializing in browser-native engineering (WASM/Rust), system integration, and user interface design.

1 Data Analyst: Responsible for dataset preparation, annotation pipelines, and performance evaluation.","Company 15’s focus is on solving one of the most critical bottlenecks in applied computer vision: reliable, standardized image capture. Our approach combines browser-native agentic guidance with 3D-to-2D alignment, which is both technically ambitious and immediately practical for enterprise deployment. Our founder and head of technology have been working on capture workflows for over eight years, scaling to thousands of captures daily across three continents. This gives us unique insight into the challenges and opportunities in this domain. Participation in this program would accelerate Company 15’s path to a pilot-ready solution while creating broader learnings for agentic AI at the edge."
Company 16,"Company 16 addresses the critical challenge organizations face in efficiently creating, personalizing, and managing complex customer communications in highly regulated industries such as healthcare, insurance, finance, and the public sector. Traditional customer communications management (CCM) platforms rely heavily on manual processes and rigid workflows, resulting in slow turnaround times, high operational costs, increased risk of compliance errors, and limited ability to adapt to changing business and regulatory requirements.


Our project focuses on developing an agentic AI-first platform that will automate content generation, quality assurance, and policy drafting, enabling real-time, multi-lingual, and multi-format communications. Operating in the $400B+ global communications management market, we serve mid-sized to enterprise clients in regulated sectors where compliance accuracy and personalization at scale are critical business requirements.


This transformation will significantly reduce manual effort, improve compliance accuracy, and empower organizations to rapidly scale and personalize their communications to better serve Canadian and global clients.","Primary Objective: Develop and deploy agentic AI capabilities that enable autonomous content management workflows for regulated industries.


Key Success Milestones:


1. Foundation Model Training & Validation (Month 1-2): Successfully train Company 16's generative language model on 500+ million proprietary CCM tokens with validated performance against target accuracy metrics

2. Agentic AI Prototype Development (Month 3): Create a functional AI agent that can autonomously generate, review, and optimize customer communications while adhering to regulatory constraints

3. Integration & Testing (Month 4): Integrate the agentic AI system into Company 16's existing workflow with demonstrated 80%+ reduction in manual policy authoring time

4. Compliance Validation (Month 4): Validate the system meets regulatory requirements for at least one target industry vertical

Critical Importance: Delay in this project would impact our competitive position in the rapidly evolving AI-powered CCM market, potentially allowing competitors to capture first-mover advantage in agentic AI applications. Without autonomous capabilities, our clients will continue facing inefficiencies that could cost millions in operational overhead and compliance risks.","We envision delivering a comprehensive agentic AI system consisting of:

Core Deliverables:

- Autonomous Content Agent: An AI system capable of generating, reviewing, and optimizing customer communications without human intervention

- Policy Drafting Assistant: An intelligent agent that can automatically generate policy documents based on regulatory requirements and business rules

- Quality Assurance Pipeline: Automated validation system ensuring all AI-generated content meets compliance standards

- API Integration Layer: RESTful APIs enabling seamless integration with existing Company 16 workflow infrastructure

- Multi-modal Communication Engine: System supporting generation across print, digital, and interactive formats

Technical Infrastructure:

- Fine-tuned language models optimized for CCM domain
- Agent orchestration framework with memory and planning capabilities
- Real-time inference pipeline for production deployment
- Comprehensive testing and validation framework","Internal Users:   - Content Authors & Editors: Will leverage AI agents to accelerate content creation and ensure consistency across communications  - Compliance Officers: Will use automated QA systems to validate regulatory adherence  - Product Managers: Will configure and monitor agentic workflows   External Client Users:   - Communications Managers at insurance companies, healthcare organizations, banks, and public sector entities  - Regulatory Affairs Teams requiring compliant, auditable content generation  - Customer Experience Teams needing personalized, multi-channel communications at scale",Enhancement to an existing system,"Dataset Contents & Relevance: Our primary dataset consists of over 500 million proprietary Customer Communications Management (CCM) tokens encompassing:


- Insurance policy documents, claims communications, and regulatory notices

- Healthcare patient communications, treatment plans, and compliance documentation

- Financial services statements, disclosures, and regulatory filings

- Public sector citizen communications and policy documents


Data Sources:


- Internal Company 16 content repositories from 10+ years of client implementations

- Anonymized client communication templates and approved content libraries

- Industry-specific regulatory guidelines and compliance documentation

- Multi-lingual communication samples across English, French, and other languages


Collection Method & Format: Data was collected through Company 16's platform implementations across regulated industries. Current formats include:


- Structured: JSON-formatted templates, metadata, and workflow definitions

- Unstructured: Rich text documents, PDFs, and HTML communications

- Tabular: Content performance metrics, usage statistics, and compliance audit trails


Key Features:


- Content type classifications (policy, notice, statement, etc.)

- Industry vertical tags (insurance, healthcare, finance, public sector)

- Regulatory compliance markers and approval workflows

- Multi-format variations (print, digital, interactive)

- Personalization variables and dynamic content rules


Approximate Size: Hundreds of gigabytes (100s of GB) of proprietary CCM content and templates",Partially annotated and includes derived or engineered features,FALSE,,"Evaluation Methods:


- Golden Dataset Validation: Curated benchmark set of 10,000+ expert-validated communications for accuracy assessment

- Human-in-the-Loop Testing: Subject matter experts from each industry vertical will validate AI-generated content quality

- Compliance Simulation: Automated testing against regulatory requirements using rule-based validation systems

- A/B Testing Framework: Real-world performance comparison between AI-generated and human-authored content

- Business Metrics Tracking:

- Content generation time reduction (target: 80%+)

- Regulatory compliance accuracy (target: 99%+)

- Client satisfaction scores (target: 85%+)

- Error rate reduction in communications workflows","Yes, we have extensive experience with Generative AI and LLMs:


Current Implementation:


- GPT-4 Integration: Company 16 was the first CCM platform to integrate OpenAI's GPT-4 for content rewriting and optimization (launched May 2023)

- Assisted Authoring: Our MARCIE AI system currently provides intelligent content suggestions, tone adjustments, and reading level optimization

- Multi-lingual Generation: Active use of transformer models for content translation and localization


Planned Advanced Techniques:


- Fine-tuning: Domain-specific model training on our 500M+ token CCM dataset

- Retrieval-Augmented Generation (RAG): Integration with regulatory knowledge bases for compliant content generation

- Agentic AI Workflows: Multi-step reasoning agents with memory and planning capabilities

- Tool-using LLMs: Agents capable of calling Company 16 APIs and external compliance validation services

- Prompt Engineering: Sophisticated prompt chains for complex policy document generation","Critical considerations for our agentic AI system:


Regulatory Compliance:


- Content generation must maintain strict adherence to industry-specific regulations (HIPAA, SOX, insurance regulations)

- Audit trails required for all AI-generated content in regulated industries

- Data privacy and sovereignty requirements for Canadian and international clients


Bias and Fairness:


- Ensuring AI-generated communications are free from discriminatory language or bias

- Consistent treatment across demographic groups in personalized content

- Transparent decision-making processes for compliance officers


Human Oversight:


- Maintaining human-in-the-loop validation for high-stakes communications

- Clear boundaries on autonomous decision-making vs. human approval requirements

- Fail-safe mechanisms when AI confidence levels are insufficient


Transparency and Explainability:


- Providing clear reasoning for AI-generated content recommendations

- Maintaining client trust through explainable AI outputs

- Documentation of AI involvement for regulatory audit purposes","Baseline Metrics:


- Current manual policy authoring time: 4-8 hours per document

- Content approval cycles: 5-10 business days

- Compliance error rate: 2-3% requiring rework

- Client satisfaction with content turnaround: 65%


Target KPIs:


- Efficiency: 80%+ reduction in manual authoring time (target: <1 hour per document)

- Quality: 99%+ regulatory compliance accuracy with automated validation

- Speed: Sub-24 hour content approval cycles for standard communications

- Satisfaction: 85%+ client satisfaction scores in pilot surveys

- Scalability: Support for 10x increase in communication volume without proportional resource increase

Qualitative Outcomes:

- Improved client competitive positioning through faster regulatory response

- Enhanced personalization capabilities enabling better customer experiences

- Reduced operational risk through automated compliance validation",Amazon Web Services (AWS),"The ideal candidate for this project would possess deep expertise in large language model fine-tuning and agentic AI architectures, with demonstrated experience in building production-scale NLP systems for regulated industries. They should have hands-on knowledge of transformer model optimization, retrieval-augmented generation, and multi-agent system design, along with understanding of compliance requirements and audit trail generation for enterprise AI applications.","Current Technical Team Structure:


- AI/ML Engineers (4): Specialized in natural language processing, model training, and AI system integration

- Software Engineers (8): Platform development, API design, and system architecture

- Data Scientists (2): Analytics, model validation, and performance optimization

- Product Managers (2): AI feature strategy and client requirements analysis

- DevOps Engineers (2): Cloud infrastructure, deployment, and monitoring


AI-Specific Expertise: Our team has successfully deployed GPT-4 integration, built custom NLP models for content analysis, and maintains production AI systems serving enterprise clients. Team members have experience with transformer architectures, prompt engineering, and regulatory compliance in AI systems.

b. Technical Point of Contact


- Title: Executive Vice President, R&D


This individual will serve as the primary liaison with Machine Learning Associates and will dedicate 2-4 hours per week during the execution phase.","Strategic Alignment with Vector Institute Mission: Company 16's participation in the FastLane MLA program aligns perfectly with Vector's goals of advancing Canadian AI leadership and supporting innovative applications of generative AI and autonomous agents. Our project addresses real-world challenges in regulated industries while developing cutting-edge agentic AI capabilities.


Economic Impact: This project will create at least 15 new Canadian AI/ML engineering positions and establish Company 16 as a global leader in agentic AI for customer communications. The technology developed will serve Canadian organizations across healthcare, finance, insurance, and public sectors, improving their competitive position globally.


Innovation Potential: By combining domain-specific language models with agentic AI capabilities, we're pioneering a new category of intelligent content management systems. This work will generate Canadian-owned intellectual property and patents in the rapidly growing field of autonomous AI agents for enterprise applications.


Risk Mitigation: Our experienced team, existing AI infrastructure, and deep understanding of regulatory requirements position us to successfully deliver project objectives while maintaining the highest standards of responsible AI development."
Company 17,"Company 17 addresses the emerging challenge that large language models (LLMs) are becoming the new Google. Consumers increasingly discover and evaluate products directly inside systems like ChatGPT, Gemini, and Perplexity, with Shopify integrations and AI-native browsers accelerating a shift toward LLM-driven purchasing. Yet brands have no visibility or control over how they are represented. Outputs are often inaccurate, omit differentiators, or surface competitors — creating reputational, compliance, and revenue risks.

Our solution is the AEO Scorecard™, the first platform for AI Journey Optimization. It evaluates how brands are described across LLMs, detects hallucinations, measures visibility and differentiation, and generates actionable recommendations to improve representation. This includes compliance-aware monitoring for industries like healthcare, life sciences, and financial services, where misinformation can lead to regulatory and safety concerns.

The global marketing technology sector is valued at over $500B, and regulated industries in particular face urgent needs for visibility and governance in AI-driven discovery. By providing enterprises with transparent, auditable insights and recommendations, Company 17 enables organizations to build trust, visibility, and competitive advantage in the AI-first economy.","Key Objectives & Success Milestones
The objective of this project is to move Company 17 from prototype to a scalable, compliance-ready platform for AI Journey Optimization. Specifically, we aim to:

Automate brand audits across LLMs by running standardized prompts, capturing outputs, and structuring them into measurable AEO Scores.

Detect and flag hallucinations, omissions, and compliance risks in AI outputs, particularly for regulated industries such as healthcare and finance.

Generate actionable recommendations (content, schema, or data fixes) through agent-to-agent workflows, enabling brands to actively improve their representation in generative AI.

Milestones (6–12 months):

Convert co-design pilots into first paying customers.

Launch SaaS platform on GCP with automated scoring dashboards.

Integrate vector-based retrieval and evaluation pipelines to support compliance monitoring at scale.

Importance
If this problem is not solved, enterprises will remain blind to how LLMs are shaping consumer decisions. They risk reputational damage, regulatory non-compliance, and lost revenue as purchasing increasingly shifts into LLM-native platforms. Without Company 17, brands will lack the tools to monitor, measure, or influence the most important discovery channel of the future.","Specific Outputs & Deliverables
Within the scope of this project, we expect the following outputs:

Productized AEO Scorecard™ Dashboard – transition from prototype to a scalable, web-based dashboard on GCP that automates prompt testing, scoring, and reporting.

Evaluation & Compliance Pipelines – structured pipelines to detect hallucinations, omissions, and compliance risks in LLM outputs, with benchmarks and audit logs for enterprise adoption.

Action Layer / Recommendation Engine – early-stage agent-to-agent workflow that not only scores outputs but also generates actionable recommendations (e.g., content or schema updates) to improve brand visibility and compliance.

Data Infrastructure – scalable storage and retrieval system (e.g., vector-based search) to manage prompt–response datasets and support longitudinal tracking of AI visibility.

Healthcare-Specific MVP – rapid prototyping of a tailored solution through a co-design agreement with a healthcare marketing agency, ensuring our MVP build strategy is validated against the unique compliance and visibility needs of the healthcare sector.

Role of MLAs
We expect the MLAs to contribute to pipeline development, orchestration frameworks, and evaluation tooling, while supporting the rapid prototyping workstream with our healthcare partner to ensure sector-specific alignment.","Enterprise Marketing & Communications Leaders – CMOs, VPs of Brand, and digital marketing teams who need visibility into how their organizations are represented in generative AI platforms.  Regulated Industry Stakeholders – Compliance officers and brand managers in healthcare, life sciences, and financial services, where hallucinations or misstatements from LLMs create regulatory, safety, and reputational risks.  Agency Partners – Digital marketing and healthcare-specific agencies who will use Company 17 to deliver AI visibility audits and optimization services to their clients, often as a white-labeled solution.  Internal Analysts & Product Teams – Data and insights teams within enterprises who will use the dashboards and reports to track visibility over time, benchmark competitors, and guide content/SEO strategies adapted for LLMs.",New standalone tool or feature,"What it contains and relevance
A structured corpus of LLM prompt–response pairs per brand and category. Each record is evaluated against our AEO Scorecard to measure visibility, differentiation, hallucinations, and compliance risk. This dataset is the evidence base for AI Journey Optimization.

Sources
a) Programmatic calls to LLM APIs (OpenAI GPT) for standardized prompts
b) Client-provided public reference materials for grounding (no PII/PHI)
c) Public product pages and documentation for context checks

Collection method and current format
Executed from Replit. Responses are captured as plain text logs and lightweight JSON snippets inside the Replit project workspace. There is no external database yet. We plan to migrate to Google Cloud (e.g., GCS for raw logs and BigQuery/Firestore for structured records) during productization.

Key features or fields
prompt_id, brand_id, category, model_version, parameters, timestamp, response_text, citations_present, competitor_mentions, missing_differentiators, hallucination_flags, compliance_flags, visibility_score, differentiation_score, sentiment_label, evaluator_id or pipeline_version, run_id.

Approximate size
Current: under 50 MB across pilots since June 2025
Per engagement: roughly 1,000 to 5,000 records depending on prompt coverage and competitor set
12-month projection: 1 to 5 GB as pilots expand and longitudinal tracking increases",Raw/unprocessed,FALSE,,"At present, evaluation is conducted entirely through human-in-the-loop validation. Each prompt–response record is manually reviewed against the AEO Scorecard to check for hallucinations, competitor mentions, missing differentiators, and compliance risks. This process ensures early pilots generate meaningful insights, but it also highlights limitations: the data can be inconsistent, the model outputs are not fully reliable, and Replit is not a production-grade environment for trusted storage or evaluation.

As we move from prototype to product, we plan to introduce structured evaluation pipelines to improve consistency and trust. These include building a small golden dataset of validated responses per pilot brand, automated heuristics and business rules (e.g., compliance flags, sentiment checks), and pre/post comparisons to measure the impact of recommended fixes. We also plan to migrate to Google Cloud (BigQuery/Firestore) to support persistent storage, dashboards, and auditable performance tracking.","Used to date

OpenAI GPT-4 and GPT-5 APIs for prompt testing and analysis

Prompt engineering and structured, human-in-the-loop evaluation for visibility, differentiation, hallucinations, and compliance checks

Lightweight app-layer orchestration inside Replit to run standardized prompt suites

Planned / under consideration

Retrieval-augmented generation with a vector store (Pinecone or BigQuery Vector) to ground outputs in verified brand and compliance data

Supervised classification heuristics for hallucination detection, competitor mentions, and missing differentiators

Evaluation pipelines and golden sets per brand for repeatable benchmarking and pre/post impact tracking

Early multi-agent orchestration where one agent generates, another validates compliance, and another proposes content or schema fixes

Select fine-tuning or parameter-efficient tuning of smaller open-source models like Mistral or LLaMA for cost control and domain performance, contingent on data readiness and ROI","Yes. There are several important ethical, legal, and societal considerations tied to our use case:

Transparency & Trust: LLMs often generate outputs that are opaque to users. Company 17’s goal is to provide enterprises with transparent, auditable insights into how their brand is being represented, so results can be trusted and verified.

Bias & Fairness: Generative models may misrepresent certain products or industries due to training data biases. Our evaluation pipelines are designed to flag omissions, hallucinations, and unfair competitor positioning.

Compliance & Regulation: In healthcare, life sciences, and finance, hallucinations or off-label recommendations could have safety and regulatory consequences. Our solution incorporates compliance checks and is being co-designed with a healthcare-specific marketing agency to align with industry requirements.

Data Privacy: We do not process PII or PHI in our workflows. All datasets are limited to public or client-approved content.

Societal Impact: As LLMs become the “new Google,” lack of oversight could amplify misinformation. Company 17 is designed to mitigate this risk by creating visibility, governance, and accountability.","Data Consistency & Accuracy

Baseline: manual, human-in-the-loop validation with inconsistent outputs.

Target: >80% repeatability across prompt–response runs with automated parsing pipelines.

Hallucination & Compliance Detection

Baseline: qualitative/manual detection.

Target: ≥75% precision/recall on hallucination and compliance flags.

Evaluation & Scoring Frameworks

Creation of a golden dataset per pilot brand.

Dashboards tracking hallucinations, visibility, differentiation, and sentiment trends.

Productization Milestones

Migration from Replit to GCP with persistent dataset storage.

Delivery of a working AEO Scorecard™ dashboard that automates testing and evaluation.

Innovation Readiness

Ability to integrate at least one emerging AI infrastructure component (e.g., Pinecone vector search, orchestration framework) into our pipeline during the cohort.

Demonstrated flexibility to adapt new tools without major rework, ensuring the platform stays aligned with rapid LLM innovation.","Google Cloud Platform (GCP), Vector-provided infrastructure (if applicable), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases)","The ideal candidate for this project would have hands-on experience with LLMs, prompt engineering, and evaluation pipelines, and be comfortable building structured workflows to improve the accuracy and consistency of generative AI outputs. They should also bring knowledge of data parsing, vector-based search, and compliance-aware NLP methods, with the ability to translate research techniques into practical, production-ready prototypes.","Our team is lean but supported by strong technical expertise.

CEO (Full-time): Leads product vision and development, with a Master’s in Analytics & Data and direct experience designing and testing our AEO Scorecard prototype.

Technical Co-Founder (Part-time): Chief Data Officer at Colliers, recognized on DataIQ’s Top 100 Most Influential People in AI. Contributes capital, technical oversight, and expertise in large-scale data systems and AI strategy.

Chief Growth Officer (Full-time): Focused on partnerships, pilots, and customer engagement.

Operations & Partnerships (Part-time): Oversees internal processes, partner coordination, and early-stage scaling.

Head of Product (Joining): Being onboarded to drive the productization of the prototype into a SaaS platform.",We are currently working with Perpetual Motion Patents to develop a formal IP strategy. We have also just signed an NDA with our potential co-design partner (our first client) and are moving fast to validate and productize our solution.
Company 18,"North American healthcare faces two urgent problems: provider shortages that delay access to basic care, and medication overload in seniors that drives costly adverse drug events (ADEs). In Canada, nearly 35% of avoidable ER visits could be managed by pharmacists prescribing for minor ailments, while in the U.S., ADEs cause over 600,000 ER visits among older adults each year, costing >$7.5B annually.

Company 18 operates in digital health and pharmacy technology, serving community pharmacies, pharma companies, and long-term care (LTC) operators. Our platform automates minor ailment prescribing, enabling pharmacists to practice at the top of their scope, save time, and divert patients from ER visits. Early traction includes 14 live pharmacies in British Columbia, with demonstrated ROI.

Our next step is a deprescribing platform for LTC, integrating medication lists, nursing notes, and family input to surface deprescribing opportunities. This addresses regulatory pressure, reduces hospital transfers, and supports a $552M SAM in North American care homes.","The key objectives of our solution are to automate manual intake and documentation processes, improve clinical decision support for pharmacists, and build a deprescribing platform that integrates families, nurses, and providers in long-term care. By doing so, we enable pharmacists to practice at the top of their scope, reduce avoidable ER visits, and improve patient safety.

Success milestones include:

Scaling our minor ailment automation from 14 to 50+ pharmacies within 12 months.

Launching and validating deprescribing pilots in 2–3 LTC chains, proving time savings, regulatory compliance, and reduced hospital transfers.

Establishing pharma and payor partnerships to expand impact across the healthcare system.

If this problem is not solved, patients will continue to flood ERs for basic conditions, costing billions in avoidable spending, while seniors in LTC remain vulnerable to medication overload, leading to unnecessary hospitalizations and family distress. Delaying implementation means perpetuating inefficiency, regulatory risk for operators, and missed opportunities to empower pharmacists as front-line providers.","For this project, we envision a set of deliverables that demonstrate how AI can streamline deprescribing and pharmacist workflows in long-term care:

Functional prototype of a deprescribing platform that integrates with sample EHR/medication data, showcasing how resident medication lists can be ingested and processed.

AI-powered decision support module that extracts key insights from nursing reports and family input, then generates deprescribing opportunities for pharmacists.

Dual-facing dashboards: one for care staff/pharmacists to view flagged risks and recommendations, and one for families to contribute observations.

APIs and backend pipelines that support secure data ingestion (medication lists, text inputs) and structured outputs for the pharmacist-facing interface.

Documentation and hand-off package describing the architecture, data pipeline, and model selection so it can be integrated into Company 18’s broader platform.

We expect the MLAs to deliver a working proof-of-concept demonstrating this workflow end-to-end, with clear metrics on feasibility and usability that we can build on for pilot deployment in care homes.","The primary users of this solution are:  Pharmacists (external): receive structured deprescribing recommendations, supported by nursing/family input, to improve medication safety and reduce review time.  Nurses and care staff in long-term care homes (external): enter shift reports and observations into the platform, ensuring frontline insights are captured.  Family caregivers (external): provide feedback on resident behavior or side effects, increasing transparency and engagement in care decisions.  Care home administrators (external): access compliance-ready reports that demonstrate proactive medication management and reduce regulatory risk.  Company 18 internal team (internal): product, engineering, and analytics teams who will monitor performance, refine models, and support integration with partner facilities.  Together, these groups ensure the platform drives value across the care ecosystem: safer prescribing for residents, efficiency for providers, and trust for families and regulators.",Enhancement to an existing system,"Content & Relevance:
The dataset will include resident medication lists, nursing shift notes, and family caregiver observations from long-term care (LTC) settings. This information is directly relevant to deprescribing, as it captures polypharmacy profiles, side-effect observations, and care staff input that inform pharmacist decision-making.

Sources:

Medication lists from LTC electronic health record (EHR) systems (e.g., PointClickCare).

Nursing/care staff reports entered during shifts.

Family-generated input via patient-facing portals.

Supplementary synthetic datasets (publicly available deprescribing trials, MedSafer, Deprescribing.org guidelines) for training and validation.

Collection & Format:

Medication data: structured, tabular (drug name, dose, frequency, start/end date).

Nursing and family reports: unstructured text (symptom notes, side-effect reports).

Integrated into CSV/JSON files for ingestion.

Key Features/Fields:

Resident demographics (age, sex, diagnoses).

Active medications (drug, class, indication, dose, frequency).

Adverse event markers (falls, confusion, sedation, appetite change).

Free-text nursing and caregiver observations.

Outcome labels (e.g., ER visit, hospitalization, deprescribed medication).

Size:

Initial pilot dataset: ~500–1,000 resident profiles across 2–3 LTC homes.

Each resident profile: ~10–15 active meds, multiple nursing/family notes per week.

Approx. 50,000–100,000 medication records and 10,000+ text notes over a 6–12 month range.",Raw/unprocessed,FALSE,,"We plan to evaluate the system’s performance using a combination of quantitative and qualitative methods tailored to healthcare AI:

Human-in-the-loop validation: Pharmacists will review deprescribing recommendations generated by the system. Acceptance rates, modifications, and rejections will provide a direct measure of clinical relevance and safety.

Golden dataset/benchmarking: We will test against curated datasets from published deprescribing trials and guidelines (e.g., MedSafer, Deprescribing.org algorithms) to benchmark system accuracy in identifying potentially inappropriate medications.

Heuristics & business rules: Built-in rule checks (e.g., Beers Criteria, drug–drug interaction lists) will validate that high-risk medications are consistently flagged.

Pre/post comparisons: We will measure time required for comprehensive medication reviews before vs. after tool use, number of medications deprescribed per review, and changes in avoidable ER transfer rates in pilot sites.

Metrics dashboards: Dashboards will track usage (active users, number of med reviews), performance (recommendations generated vs. accepted), and outcomes (admissions avoided, compliance reports generated).

Qualitative feedback: Surveys and interviews with pharmacists, nurses, and families will capture usability, trust, and satisfaction, ensuring the platform is not only accurate but also adopted in practice.

This blended evaluation strategy ensures both clinical safety and operational impact are validated before scaling.",No,"Yes - deploying Generative AI in healthcare requires addressing several ethical, legal, and societal considerations:

Data privacy & security: Patient data must be protected under HIPAA/PHIPA. All datasets used for training or inference will be de-identified, encrypted, and access-controlled to prevent misuse.

Transparency & user trust: AI outputs (e.g., deprescribing suggestions or patient summaries) will be clearly labeled as recommendations, with pharmacists retaining final clinical judgment. This avoids over-reliance on autonomous agents.

Fairness & bias: Older adults in long-term care are a vulnerable population. We will evaluate models to ensure recommendations are not biased against certain demographic groups (e.g., women, ethnic minorities) and are consistent with evidence-based guidelines.

Clinical safety: All generative outputs are human-in-the-loop — pharmacists validate recommendations before any action is taken. This ensures that the system augments, not replaces, professional expertise.

Regulatory compliance: We will align with emerging standards for AI in healthcare (FDA guidance in the U.S., Health Canada digital health frameworks) and document model behavior for auditability.

Societal impact: By improving deprescribing, we aim to reduce unnecessary hospitalizations and build trust with families by increasing transparency in care decisions. However, safeguards will be in place to ensure families understand the system is an aid, not an autonomous decision-maker.

These considerations are central to our design philosophy, ensuring that AI enhances patient safety, empowers clinicians, and builds confidence among families and regulators.","We will measure success through a mix of baseline metrics, quantitative KPIs, and qualitative feedback:

Baseline metrics:

Average time spent by pharmacists on a comprehensive medication review.

Current rate of deprescribing interventions in long-term care homes.

Frequency of ER transfers related to adverse drug events.

Key Performance Indicators (KPIs):

Efficiency: ≥30% reduction in time required for pharmacist-led medication reviews.

Clinical impact: ≥20% increase in identification of deprescribing opportunities compared to baseline.

Adoption: % of nursing staff and family members actively contributing observations through the platform (target ≥60%).

Decision support accuracy: Proportion of AI-generated suggestions accepted or partially accepted by pharmacists (target ≥70%).

Qualitative outcomes:

Improved confidence and trust among families, as measured through caregiver satisfaction surveys.

Positive usability feedback from nurses and pharmacists (ease of integration into workflow).

Demonstrated value to care home administrators through compliance-ready reports and reduced regulatory risk.

Success will be defined by showing that the platform not only augments pharmacist efficiency but also leads to safer prescribing decisions, higher stakeholder engagement, and evidence of reduced avoidable hospital transfers.","Amazon Web Services (AWS), Vector-provided infrastructure (if applicable), Not yet determined","The ideal candidate for this project would have experience with LLMs and NLP (summarization, extraction, clinical text processing), strong skills in data cleaning and pipeline engineering, and familiarity with healthcare or clinical datasets. They should be comfortable with human-in-the-loop AI design, ensuring outputs are accurate, interpretable, and aligned with pharmacist decision support workflows.",We have a CTO and Software engineer. No ML or data analysts.,No
Company 19,"Luxury is a 1.5 trillion dollar global industry where brand equity is the most valuable and fragile asset. Yet luxury brands spend an average of 4.8 million dollars annually on AI, and 73 percent report damage to brand perception from misaligned tools. No standards currently exist to determine whether AI systems are luxury ready, leaving adoption both risky and urgent.

AI Company 19 solves this problem by building the intelligence infrastructure luxury brands will rely on to adopt AI safely. Our proprietary scoring system applies IQ test principles to evaluate how well AI tools think at the luxury level. Separate frameworks for Emerging and Established brands provide precise insights, while certification ensures tools meet strict equity preserving standards.

Our target audience is global luxury houses such as LVMH, Kering, and Richemont, as well as emerging luxury brands and AI developers seeking access to the luxury market. Market forces including the EU AI Act, competitive pressure from early adopters, and rising demand for personalization among ultra high net worth clients create immediate urgency.

This project establishes Company 19 as the decision layer for luxury AI adoption, enabling risk controlled deployment and shaping how AI is built for the world’s most valuable brands.","Our primary objective is to automate a currently manual evaluation process and build the SaaS platform that becomes the intelligence layer for luxury AI adoption. Success will be measured by:

Automation of evaluations: Transition from manual scoring to an automated pipeline capable of scaling assessments from hundreds to thousands of tools.

Certification launch: Operationalize Company 19 Scores as the entry standard for AI developers seeking luxury market access.

Alliance: Formalize an invite only Alliance of luxury brands and developers to maintain standards and accelerate adoption.

Marketplace: Launch the certified Marketplace that connects brands with AI tools proven luxury ready.

Without this project, luxury brands face unmanaged AI adoption, leaving brand equity worth 1.5 trillion dollars globally at risk. Misaligned AI can dilute exclusivity, damage heritage, and erode trust among ultra high net worth clients.

Delay would also weaken competitive positioning. Conglomerates are already adopting AI, regulatory compliance under the EU AI Act is becoming mandatory, and developers need certification pathways. If Company 19 is not implemented now, luxury brands will either adopt unsafe tools or delay innovation, both outcomes undermining long term competitiveness and consumer trust.

This project ensures that luxury AI adoption is safe, standardized, and scalable, safeguarding brand equity while enabling growth.","We expect the MLAs to help us build the foundation of our automated evaluation system. Specific deliverables include:
Evaluation pipeline prototype: A backend process that automates scoring of AI tools across selected categories using our proprietary methodology.


API layer: An interface that connects the evaluation pipeline to future dashboards and certification workflows.


Functional dashboard: An internal tool for visualizing evaluation results, comparison across tools, and generation of Company 19 Scores.


Data handling framework: Initial system for organizing and processing text, image, and video datasets that support AI scoring.


Within the scope of the cohort, our priority is to move from manual evaluations to a working prototype of an automated pipeline with a simple dashboard and API connections. This will demonstrate technical feasibility, reduce manual load, and create the base infrastructure for our SaaS platform.
By the end of the program, success means we can automatically process and score a set of AI tools in at least one category, with results displayed in a dashboard that can be expanded into our certification marketplace.","The primary external users of this solution are: Established luxury houses (e.g., LVMH, Kering, Richemont) adopting AI but requiring validated, heritage preserving tools.   Emerging luxury brands seeking scalable credibility and risk controlled AI adoption.   AI developers who need certification to access the luxury market and prove compliance with brand equity standards.   Key internal users include: Company 19 analysts who run evaluations and generate Company 19 Scores.   Alliance members (luxury executives, academics, and developers) who review insights and guide standard setting.   Together, these groups form the ecosystem that ensures evaluations are credible, certifications are trusted, and the SaaS platform delivers actionable intelligence for adoption.",New standalone tool or feature,"Dataset Description

Contents and Relevance
Our dataset contains 132 comprehensive AI tool evaluations across 10 luxury categories (copy generation, image creation, personalization, etc.). Each evaluation includes Company 19 Scores (0–1000), parameter-level ratings, compliance assessments, and cross-tool comparisons. This dataset directly enables automated scoring algorithms and pattern recognition for luxury-appropriate AI outputs.


Sources
Data is generated internally through systematic AI tool testing using our proprietary methodology. Sources include direct API interactions, structured prompt testing, and brand alignment tests across defined luxury scenarios.


Collection Method and Format
Evaluations are conducted using controlled luxury brand scenarios and stored securely in Notion as the source of truth. From there, structured data is selectively synced into Webflow, where only visualized scores are displayed. This ensures public transparency while keeping our scoring methodology protected as a trade secret. Formats include structured tabular data with JSON metadata for complex evaluations.


Key Features


Tool identification and category classification


Company 19 Scores by parameter (brand voice precision, UHNW segmentation, heritage preservation)


Emerging vs Established brand performance differentials


Output samples with quality ratings


Compliance indicators for luxury standards


Temporal data showing tool evolution


Size
132 evaluations spanning 18 months of testing. 500 MB of structured data, including scoring matrices, output samples, and metadata. Targeting 250+ evaluations within 6 months to expand pattern recognition.",Partially annotated,FALSE,,"We will evaluate performance using a combination of benchmark datasets, human validation, and dashboard tracking:
Golden dataset: Use our existing library of 132 evaluations across 10 luxury categories as a controlled benchmark set. Each tool has parameter-level scores and outputs for cross-comparison.


Human in the loop: Luxury analysts review automated outputs to validate alignment with brand tone, exclusivity, and heritage preservation. This ensures subjective qualities are captured with accuracy.


Business rules and heuristics: Scoring thresholds (e.g., 900+ Company 19 Score required for certification) function as clear success indicators.


Pre and post comparisons: Track improvements in evaluation accuracy as we transition from manual to automated pipelines.


Dashboards and metrics: Internal dashboards will monitor scoring consistency, variance across categories, and time-to-completion metrics to measure efficiency gains.


Success will be defined by achieving consistent alignment between automated scores and analyst reviews, reduced variance in scoring outputs, and demonstrable scalability beyond manual capacity.","Yes. We actively explore Generative AI and LLMs as part of our evaluation process. Our work includes systematic testing of models such as OpenAI GPT-4/4o, Anthropic Claude, Google Gemini, and open-source LLMs fine-tuned for enterprise contexts. We also assess multi-modal models like Midjourney, Stable Diffusion, and Runway for image and video generation.

Our current approach focuses on prompt engineering, controlled scenario testing, and comparative scoring to measure how these models perform in luxury-specific contexts such as brand voice precision, ultra-high-net-worth segmentation, cultural nuance, and exclusivity preservation.

For future development, we are considering building automated scoring pipelines using retrieval-augmented generation (RAG), supervised classification, and explainability layers to benchmark LLM outputs against luxury standards.","Yes. In luxury contexts, the risks are amplified because brand equity is fragile and consumer trust is paramount. Generative AI outputs often carry bias, cultural insensitivity, and lack of transparency that can damage heritage, alienate high-value clients, or misrepresent exclusivity. Legal considerations include compliance with the EU AI Act, which requires independent evaluation of high-risk systems. Ethical concerns focus on ensuring that AI-driven personalization does not compromise fairness or privacy for ultra-high-net-worth individuals, whose expectations for discretion are non-negotiable.

Our methodology addresses these issues by introducing a third-party standard of evaluation that enforces accountability, transparency, and explainability before AI tools are deployed in luxury environments. By codifying evaluation benchmarks, we safeguard against bias, ensure compliance, and provide brands with the confidence that AI adoption will not undermine societal trust in their exclusivity and cultural heritage.","Success will be measured by transitioning our manual evaluation process into an automated prototype that demonstrates scalability without compromising luxury-specific precision.

Baseline Metrics:

132 completed AI tool evaluations across 10 categories (established benchmark).

Current dataset size: ~500MB of structured evaluation data in Notion.

KPIs:

Expand dataset to 250+ evaluations within 6 months.

Achieve 90% consistency between manual scoring and automated system outputs.

Onboard at least 5 luxury brand executives into the Alliance during project period.

Deliver functional scoring pipeline integrated with secure Notion-to-Webflow workflow.

Qualitative Outcomes:

Validation that luxury-specific parameters (heritage preservation, UHNW segmentation, cultural nuance) can be automated reliably.

Increased executive trust in AI Company 19™ as the independent standard for brand-safe AI adoption.

Clear pathway toward SaaS platform development and certification marketplace.","Amazon Web Services (AWS), Vector-provided infrastructure (if applicable), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases), Company-hosted development environment (e.g., GitLab, JupyterHub)","The ideal candidate for this project would have strong experience with large language models, prompt engineering, and NLP evaluation, combined with skills in data structuring and automation pipelines. Familiarity with explainability methods, compliance frameworks, and applying AI to nuanced domains like brand voice or cultural sensitivity would be a strong asset.","Our current technical team is lean, led by the founders. (CEO) serves as technical founder, having built the Company 19™ methodology and evaluation system. (President) brings 15 years of growth and data-driven strategy. We do not yet employ in-house ML engineers or data scientists, but we are actively engaging external AI consultants and planning to expand with ML engineers, data analysts, and full-stack developers as we scale toward the SaaS platform.","The AI Company 19™ is the first intelligence layer built to safeguard $1.5T in global luxury brand equity during AI adoption. Over the past 18 months, we have completed 132 systematic AI tool evaluations using our proprietary dual-framework methodology for emerging and established luxury brands. This work has created the foundation for an automated SaaS platform that will serve as the decision standard for luxury AI adoption.

Our approach is uniquely protected through trademark, copyright, and trade secret measures, ensuring defensibility as we scale. While current evaluations are manual, the data collected provides the depth needed to build automated pipelines and certification systems that meet regulatory requirements such as the EU AI Act.

Participation in this program will accelerate our transition from validated methodology to technical infrastructure, enabling us to deliver trusted, equity-preserving standards to luxury brands and certified market access to AI developers."
Company 20,"Company 20 operates in the innovation funding and government grants sector, where Canadian SMEs struggle to access the billions of dollars available each year through federal and provincial programs. The key business problem is that the process of identifying programs, preparing applications, and managing claims is highly fragmented, slow, and costly. Innovators spend scarce time navigating administrative complexity rather than building their businesses, while governments struggle with oversight and under-deployment of allocated funds

Our target audience includes Canadian SMEs in technology, manufacturing, and life sciences — companies that are innovation-driven but resource-constrained. These firms rely on timely non-dilutive funding to scale, yet lack the internal expertise to keep pace with changing program requirements and compliance obligations.

The market context is large and highly regulated: Canada allocates more than $5 billion annually in innovation-related grants and incentives, but much of this remains underutilized or delayed. At the same time, regulatory pressures are rising around data sovereignty and privacy (PIPEDA), and governments are seeking more efficient, accountable mechanisms for fund distribution.

The opportunity is for Company 20 to deliver an AI-powered funding operating system: a platform that reduces application preparation time from weeks to hours, automates compliance and claims, and creates a transparent, data-rich infrastructure for funders and applicants alike. By addressing inefficiency at both the applicant and government levels, Company 20 can unlock significant economic value and position Canada as a leader in AI-enabled innovation funding.","[Objectives and Success Milestones]
The proposed solution aims to accelerate and improve the accuracy of grant preparation and compliance by creating a hybrid AI system that combines the efficiency of large language models with the precision of symbolic reasoning methods.
[Key objectives:]
•	Automate document-heavy workflows: Ingest and process client reports, spreadsheets, emails, and meeting transcripts to automatically generate structured company profiles and draft applications.
•	Improve decision support with knowledge graphs: Build a RAG system organized by handcrafted semantic hierarchies, enabling consistent retrieval and contextualization of program and company information.
•	Develop a hybrid neurosymbolic architecture: Innovate on how symbolic reasoning modules (for temporal, financial, and logical inference) are integrated with adapted LLM components (improved embeddings and retrieval pipelines) so that each reinforces the other.
•	Advance hierarchical organization methods: Explore probabilistic clustering approaches (e.g., Gaussian mixture–based) to improve the structuring of knowledge graphs, enabling better scalability and precision.
•	Streamline compliance and oversight: Automate expense categorization, compliance validation, and reporting to reduce costs for both applicants and funders.
Milestones:
•	Near-term (6–12 months): Deploy baseline KG-RAG prototype with handcrafted semantic hierarchy; demonstrate reduction of application preparation time from a week to a day.
•	Mid-term (12–15 months): Deliver a hybrid symbolic–neural RAG prototype integrating symbolic inference with enhanced embedding and retrieval models; demonstrate measurable accuracy and reasoning improvements, along with improved methods for hierarchical organization.
•	Long-term (3+ years): Deploy a unified, production-grade platform capable of serving 500+ SMEs annually, managing $500M+ in applications, and achieving a 70%+ success rate.
[Importance:]
If this project is delayed, Canadian SMEs will continue to face costly, slow, and error-prone access to funding, and governments will continue to suffer from inefficient fund deployment and oversight. Timely implementation establishes Company 20 as a leader in responsible AI for funding infrastructure and ensures Canada builds competitive advantage in hybrid neurosymbolic approaches to applied AI.","The project will generate concrete tools that support both our internal consultants and our longer-term R&D program. The expected deliverables within the MLA cohort include:
•	Functional prototype of a knowledge-graph-based RAG system with our handcrafted semantic hierarchy implemented in the knowledge graph construction process.
•	Internal backend system and pipeline for ingesting diverse document types (reports, spreadsheets, PDFs, emails, transcripts) into Postgres + Neo4j, ensuring compliance with PIPEDA and Canadian data sovereignty requirements.
•	APIs for integration with Company 20’s workflow tools to allow dynamic retrieval of client knowledge for grant application preparation.
•	Research-facing interface (internal only) built on OpenWebUI, supporting consultants and collaborators in testing retrieval results and knowledge graph performance.
•	Evaluation reports and benchmarks measuring performance gains over traditional RAG (speed, accuracy, and consistency), forming the foundation for mid-term neurosymbolic enhancements.
The MLAs will be expected to help design and implement the prototype KG-RAG system and ingestion pipeline, ensure APIs are functional for workflow integration, and support benchmarking experiments that validate the system’s effectiveness.","In the near term, the primary users will be Company 20’s internal consultants and grant writers, who rely on the system to streamline document intake, application drafting, and claims management. For them, the platform directly reduces preparation time and improves accuracy, enabling higher throughput and better outcomes for clients.  A second internal group of users will be research collaborators and student interns, who will interact with the research-facing interface to test and refine knowledge graph construction, neurosymbolic RAG methods, and evaluation benchmarks.  Strategically, the solution is also intended to serve government funding agencies as a future user group. By adapting the same infrastructure for the funder side, agencies could automate compliance checks, streamline oversight, and improve transparency across billions in annual funding. This dual-sided relevance — supporting both applicants and agencies — amplifies the system’s long-term value and positions it as a national infrastructure opportunity.",Enhancement to an existing system,"1. Contents and Relevance:
The dataset consists of company documentation and communications directly relevant to preparing and submitting grant applications. It includes financial reports, spreadsheets, tax filings, technical reports, funding documents, meeting transcripts, and email correspondence. This information is essential because grant preparation requires synthesizing details from diverse and often incomplete sources into a coherent, compliant application.

2. Sources:
- Internal company documents provided by clients (DOCX, XLSX, PPTX, PDF, Google Docs).
- Client communications via email threads (Google Workspace / Gmail).
- Meeting transcripts (captured through the Fathom API.)
- Supplemental materials obtained via web scraping (e.g., public program information).

3. Collection and Format:
Data is provided directly by clients through secure uploads, by integration with Google Workspace APIs, or via API-based services (Fathom). The data is currently unstructured, stored as raw documents, text transcripts, and email threads.

4. Key Features / Fields:
- Company identifiers (name, sector, size, location).
- Financial data (revenue, expenses, balance sheets).
- Project descriptions and technical details.
- Program requirements and deadlines.
- Communication records (meeting notes, email Q&A with clients).

5. Approximate Size:
Currently ~800 MB of unstructured data, covering ~150 client companies, and growing rapidly as new clients onboard. We anticipate significant growth in both volume and diversity of documents as the platform scales.",Raw/unprocessed,TRUE,"PII is handled under PIPEDA compliance, with all data stored on Canadian servers and accessed only via Google Workspace authentication. For anonymization, we apply a combination of:
•	Redaction of direct identifiers (names, emails, phone numbers, addresses).
•	Pseudonymization (replacing entities with consistent placeholders to preserve context).
•	Selective removal of sensitive fields in structured documents (e.g., tax ID numbers).
•	Use of synthetic or masked data when sharing datasets externally (e.g., with students or collaborators).
This ensures that client-identifying information is stripped out or substituted before data is used outside controlled Company 20 operations","While internal anonymized datasets will be used for system development and compliance testing, public benchmark datasets will be required for research evaluation and publication. These provide a common ground for measuring progress and ensuring results are generalizable beyond our client base.
We will pursue a dual evaluation strategy:
1.	Internal Evaluation (Use-Case Driven):
The system will be applied directly to Company 20’s workflows (grant application preparation, compliance checks) to confirm that it meets our operational needs. We will measure attributes of user experience (e.g., speed, usability, accuracy) and collect user stories from our consultants. This ensures the tool is valuable in practice, but we acknowledge that this is not a systematic research evaluation.
2.	External Evaluation (Research Benchmarks):
For systematic and publishable evaluation, we will rely on public datasets, such as MSRS (Phanse et al., 2025, arXiv:2508.20867). These benchmarks provide structured, multi-source reasoning tasks that allow us to measure performance improvements in knowledge-graph-based RAG and neurosymbolic methods in a way that is comparable to academic baselines.
This combination ensures our solution is both practically validated for our use case and rigorously benchmarked for the research community.","Yes. Our consultants currently make active use of Generative AI and LLMs in day-to-day work. This includes:
•	Google AI Studio (Gemini 2.5 Pro) for drafting and refining grant-related text. Some assistance from GPT 5-based models.
•	OpenWebUI connected to multiple open-source and commercial models via OpenRouter, giving access to a wide range of LLMs for experimentation. Our consultants experimented with a variety of models, such as LLaMA 3 (Meta), Command R+ (Cohere), and Mistral09x22B (Mistral AI). 
For this project, we are considering approaches that combine these existing LLM tools with knowledge-graph-based RAG and ultimately a neurosymbolic hybrid architecture that integrates symbolic reasoning (temporal, financial, logical) with improved LLM embeddings and retrieval methods.","Yes. Several considerations are relevant:
•	Data privacy and sovereignty: All client data must be handled under PIPEDA and stored on Canadian servers to maintain compliance and trust.
•	Transparency and user trust: Our consultants need clear visibility into how outputs are generated, especially in high-stakes contexts like financial reasoning and eligibility criteria. We aim to avoid “black box” outputs by integrating symbolic reasoning alongside LLMs.
•	Accuracy and reliability: Errors or hallucinations in generated text could directly affect funding outcomes. We mitigate this risk by keeping humans in the loop: consultants always verify completions before use and review final applications with clients. In parallel, we aim to reduce errors by incorporating symbolic methods and external verification (e.g., spreadsheets, logic engines).
•	Fairness and bias: While secondary to privacy, we remain attentive to bias, particularly since many LLMs are trained on corpora dominated by non-Canadian content. In the future, we may explore fine-tuning open-source models on Canadian data to mitigate this risk.
Our design philosophy emphasizes privacy first, compliance by default, and explainability through neurosymbolic methods, ensuring that ethical and legal standards are integral to the system.","We will measure success across three dimensions: efficiency, accuracy, and usability.
•	Baseline Metrics:
o	Current application preparation time: ~1 week per grant.
o	Current dataset size: ~800 MB across ~150 clients, growing.
o	Current success rate: ~20% of submitted applications.
•	Target KPIs:
o	Efficiency: Reduce grant preparation time from 1 week → 1 day (near-term) and toward 1 hour (long-term).
o	Accuracy: Lower the rate of factual/financial errors in draft applications by ≥50% compared to current LLM-only baselines.
o	Usability: Positive consultant feedback (≥80% report the tool makes their work faster/easier); collection of internal user stories validating fit-for-purpose.
•	Research Outcomes:
o	Demonstrate that our baseline knowledge-graph RAG improves retrieval precision/recall compared to vanilla RAG.
o	Show that mid-term neurosymbolic hybrid RAG further improves reasoning accuracy on tasks such as temporal alignment and financial calculations.
o	Benchmark results against public datasets (e.g., MSRS, Phanse et al. 2025) to provide systematic, publishable evaluation.
Success will be defined as meeting near-term KPIs while also producing validated research evidence that our approach advances the state of RAG for real-world applications","Internal on-premise or virtual private servers, Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases)","The ideal MLA candidate has a strong foundation in NLP and Python, with familiarity in knowledge-graph-based RAG methods. Background in classical AI and symbolic reasoning (e.g., logical systems, temporal reasoning, planning algorithms) is highly valuable, as it enables them to contribute to our neurosymbolic approach that combines retrieval, reasoning, and LLMs","The current team consists of:

-  (CTO): Software developer with 15 years of applied R&D experience in computer vision and robotics for industrial automation. MSc in Computer Science (ABD status for PhD), specialized in computer vision and robot control (planning and execution). Inventor on 6 patents, with extensive experience bridging applied research and industrial deployment.

-  (CEO): Scientist and entrepreneur with a PhD in Chemical Biology, 11 peer-reviewed publications, and founder of GreenLid, a CleanTech startup with a successful exit. Brings deep expertise in commercialization, innovation funding, and client engagement, providing strategic oversight and business leadership.

Over the next year, we plan to expand the technical team with three roles: backend engineer, frontend engineer, and devops engineer. In parallel, we expect to involve student interns and academic collaborators in research prototypes focused on knowledge-graph-based RAG and neurosymbolic reasoning.","Company 20 is uniquely positioned to both deliver immediate impact and contribute to cutting-edge AI research. In the near term, our solution accelerates access to non-dilutive funding for Canadian SMEs by making grant preparation faster, cheaper, and more reliable. In the longer term, our neurosymbolic approach to RAG represents a novel contribution to AI research with applications beyond our sector.

Our team combines technical depth (15+ years of applied R&D, 6 patents) with entrepreneurial and commercialization expertise (PhD, successful CleanTech exit). This balance ensures both feasibility and scalability.

We see strong alignment with Vector’s mission: our work supports Canadian data sovereignty and PIPEDA compliance, advances responsible AI methods, and strengthens Canada’s innovation funding infrastructure. FASTLANE MLA support would accelerate not only our business outcomes but also our research contributions to the broader AI ecosystem."
Company 21,"Problem/Opportunity:

Agriculture, one of the world’s largest and most vital industries, faces increasing pressure to adapt to climate change, regulatory demands, and market volatility. Traditional farming practices often lead to inefficient resource use, over-fertilization, high greenhouse gas (GHG) emissions, and poor environmental outcomes. Moreover, farmers struggle to optimize crop yields while reducing costs and complying with environmental regulations. This project aims to solve these challenges by using advanced agritech solutions to collect real-time data and create predictive financial models that enhance farm profitability, sustainability, and compliance.

Key Business Opportunity:

The project leverages soil sensor data and GHG emissions data to forecast commodity prices and optimize farm operations. By accurately predicting future market trends, crop yields, and carbon credit opportunities, farmers can make more informed decisions about fertilizer use, irrigation, livestock management, and emissions reduction strategies. The result is a win-win: reducing environmental impact while enhancing farm profitability through smarter, data-driven practices.

Target Audience:

Farmers (particularly those in crops, dairy, and livestock sectors) who are looking to improve yield, reduce input costs, and enhance sustainability.

Agribusinesses and agritech companies interested in adopting new technologies to drive growth and optimize farm management.

Regulators seeking tools to ensure sustainability and compliance with environmental standards, such as GHG emissions limits and water quality control.

Carbon Markets that can utilize the data for carbon credit trading and verifying emissions reductions from agricultural activities.

Relevant Market Context:

Industry: The global agricultural technology (agritech) market is projected to reach $42.5 billion by 2027, driven by technological advancements in sensors, machine learning, and climate-smart farming solutions. The precision agriculture segment, in particular, is experiencing rapid growth as farmers seek efficient, cost-effective solutions for improving yields and reducing resource consumption.

Size of the Market:

Global agricultural market is worth over $3.3 trillion (2019), with farming accounting for a significant portion of this.

The carbon market (where emissions reductions translate into tradable carbon credits) is expected to grow substantially, reaching an estimated $100 billion by 2030.

The precision agriculture market is expected to grow at a CAGR of 12.8% between 2020 and 2027, fueled by increased adoption of IoT sensors and AI-powered analytics.

Regulatory Environment:

Governments are tightening emissions regulations for agriculture, with more stringent limits on GHG emissions (e.g., methane and nitrous oxide from livestock and soil).

There is increasing emphasis on sustainable farming practices and carbon sequestration, with many countries offering financial incentives for farms that reduce their environmental footprint and participate in carbon credit programs.

Environmental regulations are becoming more complex, creating a need for solutions that help farms stay compliant while maintaining profitability.

Business Context:

Farming Practices: Traditional farming still relies heavily on trial-and-error and seasonal practices. This project moves away from guesswork to data-driven decisions, using real-time data to model future scenarios.

Market Volatility: The commodities market is volatile, with crop prices fluctuating due to climate events, trade policies, and changing consumer preferences. Predicting commodity prices with high accuracy will give farmers a competitive edge in pricing and market entry.

Carbon Credits & Sustainability: Increasing pressure to meet climate goals and mitigate carbon emissions opens up new revenue opportunities for farmers who adopt sustainable farming practices. Participating in carbon markets by reducing emissions could provide an additional stream of income.","1. Key Objective: Automate Data Collection and Analysis

Description: The first objective is to automate the real-time collection of critical agricultural data, including soil quality, water nutrients, GHG emissions from livestock, and environmental conditions. The data will be captured through IoT sensors and integrated into a unified platform for seamless monitoring and analysis.

Success Milestone: Successfully deploy and integrate a network of IoT sensors across pilot farms, with real-time data flowing into an analytics platform. Achieve a 90%+ data accuracy rate in capturing soil nutrient levels, water quality, GHG emissions, and environmental conditions.

Importance:

Without automation, farmers must rely on manual processes for collecting soil, emissions, and climate data, which is time-consuming, inaccurate, and costly.

Manual data collection creates significant barriers to understanding the full scope of farm operations, leading to inefficiencies, potential regulatory non-compliance, and missed opportunities for profitability.

2. Key Objective: Develop Predictive Financial and Yield Forecasting Models

Description: The project will leverage the data collected to develop advanced predictive financial models that forecast crop yields, commodity prices, input optimization (fertilizer, feed, water), and emissions reductions. These models will also include risk forecasting for climate events (droughts, floods), market volatility, and carbon credit calculations for farms looking to monetize emissions reductions.

Success Milestone: Develop and deploy a working prototype of the predictive financial model, including at least three key metrics—crop yield forecasts, carbon credit eligibility, and input optimization strategies—that can be used to simulate scenarios and make actionable farm decisions.

Importance:

If predictive models are not developed, farmers will continue to make decisions based on intuition rather than accurate forecasts, leading to over-fertilization, missed opportunities for carbon credits, and suboptimal pricing of crops in volatile markets.

Lack of forecasting tools hinders the ability to identify growth opportunities, resulting in missed cost-saving and revenue-generating possibilities, which may undermine long-term profitability.

3. Key Objective: Improve Decision Support for Sustainability and Compliance

Description: Build a comprehensive decision support system (DSS) that integrates the data and predictive models into actionable insights, helping farmers make data-driven decisions about resource use, emissions reductions, sustainability practices, and compliance with environmental regulations (e.g., GHG emission limits, water quality standards).

Success Milestone: Deliver a functional user dashboard with real-time insights on farm performance, including resource consumption, environmental impact, and regulatory compliance. Achieve user satisfaction ratings of at least 85% for ease of use and effectiveness in decision-making.

Importance:

Without a decision support system, farmers will struggle to comply with rapidly evolving environmental regulations, leading to potential penalties or fines. Additionally, they risk falling behind in adopting sustainability practices that are becoming crucial for market access and brand reputation.

A lack of decision support could also mean missing out on carbon credit trading or other sustainability-related financial incentives that can help offset farming costs.

4. Key Objective: Enable Real-Time Monitoring of Financial Performance and Carbon Credits

Description: Create a module within the platform for farmers to monitor their financial performance (e.g., cost savings on inputs, increased crop yield) alongside carbon credit performance (GHG reductions and carbon credit eligibility). This will allow farmers to track their environmental and financial progress in one unified system.

Success Milestone: The platform should enable farmers to track at least two key financial indicators (e.g., fertilizer cost savings, carbon credit revenue) in real time, with automated calculations for carbon credit eligibility and potential revenue.

Importance:

If the financial and carbon credit monitoring feature is delayed or not implemented, farmers will be unable to quantify the return on investment (ROI) of their sustainable practices. This would undermine efforts to optimize farm operations and capitalize on carbon credit opportunities, which are increasingly important revenue streams.

The inability to track financial performance and carbon credits may reduce farmers' willingness to invest in climate-smart technologies due to a lack of clear value propositions.

5. Key Objective: Develop Scalable Technology for Widespread Adoption

Description: Ensure that the solution is scalable across different farm sizes, climates, and crop types. The platform and sensor technology should be adaptable to various use cases and should include easy-to-deploy options for small, medium, and large farms.

Success Milestone: Achieve a successful pilot test of the solution on farms of varying sizes and types (e.g., crop-focused, livestock-focused, mixed operations) with scalability benchmarks set to deploy on at least 100 farms within the first year.

Importance:

Without scalability, the project will face adoption barriers, limiting the potential impact on global farming practices. If the technology is too expensive or complex to deploy at scale, its potential for transformation in the agriculture industry will be drastically reduced.

Delaying or failing to make the solution scalable would also prevent farmers from accessing affordable agritech tools, particularly those in regions with smaller or less tech-savvy farms.

Consequences of Not Solving the Problem / Delaying Implementation:

Economic Losses for Farmers: Without the ability to predict crop yields, commodity prices, and input needs, farmers will face inefficient resource use, increasing operational costs and leading to lower profitability. If farmers cannot accurately assess their carbon credit eligibility, they miss out on additional revenue from carbon markets.

Environmental Damage: The continued reliance on traditional farming methods (e.g., over-fertilization, inefficient water use) will exacerbate soil degradation, GHG emissions, and water pollution, undermining long-term farm productivity and contributing to climate change. Failure to adopt climate-smart practices could also result in farmers failing to meet regulatory standards, risking penalties and legal consequences.

Missed Opportunities for Compliance and Carbon Trading: Without a solution for emissions tracking and carbon credit management, farmers could miss out on the growing carbon credit market, which is an essential revenue stream as environmental regulations become more stringent globally. Additionally, failure to comply with evolving emissions standards may cause them to lose market access or face penalties.

Stagnant Industry Growth: Agritech adoption is already lagging behind in many areas. If this project is delayed, it will hinder the progress of precision agriculture, which is necessary to improve efficiency, environmental stewardship, and profitability in the sector. The agriculture industry would continue to be highly vulnerable to climate risks without the necessary tools to enhance resilience and sustainability.

Increased Risk from Climate Change: As climate change accelerates, farmers will face more frequent and severe weather events, impacting crop yield and livestock health. Without advanced forecasting models, farmers cannot predict or mitigate these risks effectively, leaving them exposed to financial instability and production losses.","1. Predictive Financial Models and Forecasting Tools

Description: Development of AI/ML-powered predictive models for:

Crop Yield Forecasting: Predict future crop yields based on nutrient data, soil health, and environmental conditions.

Commodity Price Prediction: Predict price fluctuations based on market trends and farm data.

Input Optimization: Estimate the reduction in input costs (e.g., fertilizer, water) from implementing precision agriculture practices.

Emissions Reduction: Calculate potential carbon credits and associated revenue from reducing GHG emissions in farming practices.

Expected Outcome: A set of working machine learning models that can be deployed to predict future outcomes and help farmers make more informed decisions on inputs, pricing, and sustainability efforts.

Relevance for MLAs: They will assist in developing the ML models, including selecting the right algorithms for regression, classification, and time-series analysis, and fine-tuning them to maximize prediction accuracy.

2. Decision Support System (DSS) Dashboard

Description: An interactive dashboard that consolidates all the real-time data and predictive model outputs, providing actionable insights on farm operations. This system will include:

Financial Performance Metrics: Fertilizer cost savings, crop yield forecasts, emissions reductions, and carbon credit revenue.

Environmental and Compliance Monitoring: Real-time updates on GHG emissions, water quality, and other sustainability KPIs to ensure regulatory compliance.

Scenario Analysis Tools: Allow users to simulate various farming scenarios (e.g., reducing fertilizer by 20%, increasing irrigation) and see their impact on costs, yields, and emissions.

Expected Outcome: A functional, user-friendly dashboard where farmers can track the impact of decisions in real-time, monitor environmental metrics, and simulate different farming strategies.

Relevance for MLAs: Assist in building the backend infrastructure for the dashboard, ensuring that data flows correctly and predictions are integrated smoothly into the interface. They will also contribute to designing the UI/UX for farmers to interact with the data.

3. Carbon Credit Calculation and Monetization Tool

Description: A tool integrated into the platform that calculates carbon credit eligibility based on the farm's GHG emissions (CO₂, CH₄, N₂O) and allows farmers to explore how to monetize these reductions through carbon credit trading.

Expected Outcome: A calculator that automatically converts farm emissions into CO₂ equivalents (using IPCC protocols) and provides farmers with an estimate of their potential revenue from carbon credits. The tool will also include real-time updates to reflect changes in emissions or carbon credit market prices.

Relevance for MLAs: Develop the algorithms for carbon credit conversion and integration with the platform’s emission-tracking system, and contribute to the development of data models that allow real-time monitoring of carbon credits.

4. Backend System for Data Processing and Storage

Description: A robust backend system to handle the large-scale storage, processing, and analysis of agricultural data. The backend will be responsible for collecting data from IoT sensors, running predictive models, storing historical data for analysis, and providing this information to the front-end user interface.

Expected Outcome: A secure and scalable backend system capable of storing and processing massive amounts of farm data (e.g., soil, water, emissions, climate data). The system will allow for efficient querying, real-time analytics, and fast data retrieval.

Relevance for MLAs: Work on the database architecture, ensure data pipeline efficiency, and design a cloud infrastructure that can handle the large-scale datasets generated by the IoT sensors and predictive models.

5. API for Data Sharing and Integration

Description: An API that allows external systems or third-party applications to access the data collected by Agrobotic’s platform. This will enable integration with other farm management systems (e.g., CRM tools, market pricing platforms, or regulatory compliance tools) and third-party carbon credit marketplaces.

Expected Outcome: A fully functional API that provides access to key data sets (e.g., farm performance, emissions data, carbon credit calculations) in a secure, structured manner. It should support real-time data push/pull and secure authentication methods.

Relevance for MLAs: Assist in API design and development, ensuring efficient data sharing and integration with other systems, as well as implementing security protocols for data access.

6. Reporting and Compliance Module

Description: A reporting tool that generates reports aligned with environmental regulations, showcasing the farm’s performance in areas like GHG emissions, nutrient management, and water use. The tool will also include auditable data for regulatory bodies or sustainability certification programs.

Expected Outcome: A reporting tool that automates the generation of regulatory-compliant documents, including carbon credit reports, emissions inventories, and sustainability reports. It will also support custom report generation based on farm-specific needs.

Relevance for MLAs: Help develop the reporting engine, ensuring that data is compiled correctly, aligned with regulatory standards, and formatted appropriately for submission to authorities or certification bodies.

Conclusion:

The primary deliverables of the project are:

Functional prototype integrating IoT sensor data.

Predictive financial models for crop yields, commodity prices, emissions reduction, and carbon credit monetization.

Interactive DSS dashboard to track financial performance, emissions, and environmental sustainability.

Carbon credit calculation and monetization tool for farm emissions.

Scalable backend system for data storage and processing.

API for external integration and data sharing.

Reporting module for compliance and sustainability reporting.","1. Farmers (Primary End-Users)  Role: The primary users of the solution are farmers who will directly benefit from the platform’s tools and insights.  Needs/Goals:  Precision Agriculture: Optimize fertilizer, water, and pesticide use based on real-time data, thereby reducing costs and increasing crop yields.  Emissions Management: Monitor and reduce greenhouse gas (GHG) emissions from their operations to comply with environmental regulations and take part in carbon credit programs.  Sustainability and Compliance: Ensure their operations meet environmental and regulatory standards and track the farm’s sustainability efforts.  Financial Optimization: Use predictive models to forecast crop yields, commodity pricing, and input cost savings, and generate insights into long-term financial health.  Expected Interaction: They will primarily use the dashboard for real-time monitoring, engage with predictive models for financial forecasting, and rely on the carbon credit tool for emissions reductions.  2. Agronomists and Farm Advisors  Role: Experts who advise farmers on crop management, soil health, and environmental sustainability.  Needs/Goals:  Data-Driven Recommendations: Access to detailed, actionable insights based on sensor data to advise on soil health, irrigation, and nutrient management.  Customization of Models: Ability to adjust models to farm-specific conditions and provide personalized recommendations for optimizing farm operations.  Sustainability Guidance: Help farmers comply with regulations and optimize emissions reductions to maximize carbon credit revenue.  Expected Interaction: Agronomists will use the predictive models to provide customized advice and rely on the compliance and sustainability reports for regulatory guidance.  3. Agricultural Business Analysts  Role: Analysts working within agribusinesses or on behalf of agribusinesses to assess farm performance and market conditions.  Needs/Goals:  Market Forecasting: Use predictive models and commodity pricing forecasts to assess market trends and prepare for future pricing.  Farm Performance Monitoring: Monitor multiple farms’ performance through the DSS dashboard, focusing on operational efficiency, sustainability metrics, and financial performance.  Supply Chain Optimization: Assess how farm data (e.g., yield forecasts, input costs) impacts the broader supply chain.  Expected Interaction: Analysts will use the dashboard for ongoing monitoring, API for integrating farm data into internal business systems, and scenario analysis tools to evaluate potential market scenarios.  4. Regulatory Bodies and Compliance Auditors  Role: Government agencies or third-party auditors that oversee environmental regulations and sustainability reporting.  Needs/Goals:  Data Auditing: Ensure farms are adhering to environmental regulations related to emissions, nutrient use, and water quality.  Compliance Verification: Validate that farms are meeting industry standards for emissions reductions and sustainability.  Carbon Credit Verification: Verify carbon credits claimed by farms and ensure compliance with carbon offset programs.  Expected Interaction: Regulatory bodies will rely on the reporting and compliance module for auditable GHG inventories, carbon credit eligibility, and overall sustainability performance.  5. Carbon Credit Brokers or Marketplaces  Role: Organizations or individuals that facilitate the trading of carbon credits on behalf of farmers or agribusinesses.  Needs/Goals:  Carbon Credit Valuation: Evaluate the potential value of carbon credits generated from farming operations based on emissions reductions.  Market Access: Facilitate the monetization of carbon credits by linking farms to trading platforms.  Expected Interaction: They will use the carbon credit monetization tool to assess emissions reductions and estimate potential revenue, and may also interact with the API to integrate data into trading platforms.  6. Farm Management Software Developers  Role: Third-party developers who may integrate the platform's functionality into other farm management or ERP systems.  Needs/Goals:  Data Integration: Integrate data from the platform into their own farm management systems to enrich the farm management process with real-time sensor data, predictions, and financial insights.  Expected Interaction: They will primarily work with the API to pull data from Agrobotic’s platform and integrate it with their software solutions.  7. Research Institutions/Environmental Scientists  Role: Academic or research-based organizations that focus on agricultural sustainability, environmental science, or climate change.  Needs/Goals:  Data Access for Research: Access large datasets on soil health, water quality, livestock emissions, and environmental conditions for academic research.  Validation of Agricultural Practices: Validate the effectiveness of various sustainable farming practices based on real-world data.  Expected Interaction: Researchers may use the API to access and analyze farm data or interact with the dashboard to track environmental trends.  8. Investors & Financial Institutions (Sustainability-Focused)  Role: Investment firms or banks that focus on sustainability and agri-finance.  Needs/Goals:  Risk Assessment: Assess the financial health and sustainability risks of agricultural ventures or investments.  Investment Opportunities: Identify investment opportunities in sustainable agriculture based on data-driven insights into farm performance and emissions reductions.  Carbon Credit Portfolio: Invest in carbon credit markets or support farmers in monetizing their carbon reductions.  Expected Interaction: Investors will interact with financial dashboards, predictive models for yield forecasting and market risk, and the carbon credit module to assess potential returns.  Summary of Primary Users:  Farmers (end-users for real-time monitoring, optimization, and decision support)  Agronomists/Farm Advisors (provide expert recommendations and customizations)  Agricultural Business Analysts (monitor farm performance, market forecasting, and supply chain optimization)  Regulatory Bodies (verify compliance with environmental standards and carbon credit programs)  Carbon Credit Brokers/Marketplaces (facilitate the trade of carbon credits)  Farm Management Software Developers (integrate Agrobotic data with other systems)  Research Institutions/Environmental Scientists (access data for academic research and validation)  Investors & Financial Institutions (assess financial health and sustainability risks for investment purposes)",Enhancement to an existing system,"Datasets for the Project:
1) Soil & Water Nutrient Data (NitroSense & Field Sensors)

Dataset Contents & Relevance:

Soil Nutrients: Data on the levels of Nitrogen (N), Phosphorus (P), and Potassium (K) in the soil. This is crucial for precision farming as it helps optimize fertilizer use, leading to cost savings and improved crop yields.

Soil pH: Indicates the acidity or alkalinity of the soil, affecting nutrient availability to plants.

Soil Organic Carbon: Important for assessing soil health, carbon sequestration potential, and sustainability.

Nitrate Concentrations in Soil and Runoff Water: Measures nitrogen contamination in soil and water, important for nutrient management and reducing environmental pollution.

Soil Moisture & Temperature: Helps optimize irrigation and manage water use efficiently.

Data Use Case: These sensors will enable real-time monitoring of soil conditions, helping farmers make data-driven decisions about fertilizer, water usage, and crop management. It is also essential for carbon credit calculations based on emissions from fertilizer application and soil health.

Source(s):

Internal IoT Sensors: Agrobotic's proprietary field sensors (e.g., NitroSense, Field Sensors) deployed on farms.

Third-Party APIs: Weather data APIs for additional context like temperature, humidity, and rainfall.

Data Collection Method:

Sensors deployed in the field collect data continuously at fixed intervals (e.g., every 15 minutes).

The data is transmitted to a central cloud-based system via low-power wireless communication (e.g., LoRaWAN, NB-IoT).

Data Format:

Structured tabular format (CSV, JSON, or similar).

Data points include timestamp, sensor ID, soil temperature, moisture level, pH, nutrient levels (NPK), nitrate levels, organic carbon, etc.

Key Features/Fields:

Timestamp: Date and time of the reading.

Sensor ID: Unique identifier for each sensor deployed in the field.

Soil Moisture (%): Percentage of soil moisture.

Soil Temperature (°C): Temperature of the soil.

NPK Levels: Nitrogen, Phosphorus, Potassium content in ppm (parts per million).

Soil pH: pH level of the soil.

Nitrate Concentration (ppm): Concentration of nitrates in the soil or runoff water.

Organic Carbon (%): Amount of organic carbon in the soil.

Size & Time Range:

Approximate size: 100,000 to 500,000 records per farm per year.

Data recorded for a typical growing season (3-9 months) for crops.

Data size: 10MB – 100MB per farm annually, depending on sensor density and sampling rate.

2) Greenhouse Gas (GHG) Emissions Data (DairyGHG & AgroTrace-Livestock)

Dataset Contents & Relevance:

Methane (CH₄) Emissions: Measurements of methane emissions from enteric fermentation in dairy cows, which is a major contributor to agricultural GHG emissions.

Nitrous Oxide (N₂O) Emissions: Emissions from manure storage and field application of manure, critical for understanding and mitigating farm-related GHG emissions.

Ammonia (NH₃): Measurement of ammonia buildup in livestock barns and pens.

Ventilation, Temperature, and Humidity: Environmental data in livestock barns that affect emission levels.

Data Use Case: These datasets will be used to calculate emissions reductions, help farms comply with environmental regulations, and generate carbon credits.

Source(s):

Internal Sensors: IoT-based sensors (e.g., DairyGHG, AgroTrace-Livestock) deployed within barns and pens to measure GHG emissions.

Third-Party Weather Data: APIs to collect data on external climate conditions that affect barn emissions (e.g., temperature, humidity).

Data Collection Method:

Sensors collect real-time emissions data in the livestock barns.

Data is captured at regular intervals (e.g., every minute) for methane, nitrous oxide, ammonia levels, and environmental factors (ventilation, temperature, humidity).

Data Format:

Structured tabular format (CSV, JSON).

Features include timestamp, sensor ID, methane concentration, nitrous oxide levels, ammonia levels, temperature, humidity, etc.

Key Features/Fields:

Timestamp: Date and time of the reading.

Sensor ID: Unique identifier for each sensor.

Methane Concentration (ppm): Methane emissions from livestock.

Nitrous Oxide (N₂O) Concentration (ppm): Nitrous oxide emissions from manure.

Ammonia (NH₃) Concentration (ppm): Ammonia emissions in the barn.

Temperature (°C): Temperature inside the barn.

Humidity (%): Humidity inside the barn.

Ventilation Rate: Airflow or ventilation rate within the barn.

Size & Time Range:

Approximate size: 50,000 – 200,000 records per farm per year.

Data collected continuously during barn operation periods (typically 6-12 months/year for livestock).

Data size: 5MB – 30MB per farm annually.

3) Weather & Microclimate Data

Dataset Contents & Relevance:

Temperature, Humidity, and Rainfall: Daily weather parameters that influence both soil conditions and livestock environment.

Microclimate Data: Local climate data, including wind speed, barometric pressure, and solar radiation, which are important for both soil and livestock management.

Data Use Case: This data will support yield forecasting, irrigation scheduling, and emissions modeling by providing external environmental context to the farm data.

Source(s):

Third-Party Weather APIs: e.g., OpenWeatherMap, Weatherstack, or AgriMet.

On-Site Sensors: Weather stations deployed on-site for localized readings.

Data Collection Method:

Data collected via external weather stations or APIs at regular intervals (typically hourly or daily).

Data Format:

Structured tabular format (CSV, JSON).

Features include timestamp, temperature, humidity, rainfall, wind speed, solar radiation, etc.

Key Features/Fields:

Timestamp: Date and time of the reading.

Temperature (°C): Ambient air temperature.

Humidity (%): Humidity level in the air.

Rainfall (mm): Precipitation amount in mm.

Wind Speed (km/h): Wind speed at the site.

Solar Radiation (W/m²): Solar exposure at the farm.

Size & Time Range:

Approximate size: 1,000 – 10,000 records per farm per year.

Time range: Data collected on a daily or hourly basis over a 1-3 year period.

Data size: 1MB – 5MB per farm per year.

4) Financial & Operational Data (Input Optimization, Yield Forecasting, etc.)

Dataset Contents & Relevance:

Input Costs: Data related to input costs such as fertilizer, water, and feed.

Yield Data: Historical crop yield data to forecast future yields and assess financial performance.

Emission Reductions: Data for calculating potential carbon credits based on GHG reductions.

Data Use Case: This dataset will help forecast financial outcomes, calculate savings from optimized inputs, and model the ROI of sustainability practices (e.g., fertilizer reduction, water conservation).

Source(s):

Internal Farm Records: Input costs, historical yields, and operational metadata provided by farmers or through farm management systems.

Third-Party Financial Data: Agricultural commodity pricing data sourced from market APIs (e.g., CME Group, NASDAQ).

Data Collection Method:

Input costs and yields are collected through surveys, interviews, or integration with farm management software.

Carbon credit data may be calculated based on emissions data and external benchmarks.

Data Format:

Structured tabular format (CSV, JSON).

Features include timestamp, input costs (e.g., fertilizer, water), yield data (e.g., tons/acre), and carbon credits.

Key Features/Fields:

Timestamp: Date and time of the transaction or data point.

Input Costs: Cost of fertilizer, pesticides, feed, and water.

Yield Data: Historical crop yield per field/acre.

Carbon Credits: Number of carbon credits eligible for trade.

Size & Time Range:

Approximate size: 10,000 – 50,000 records per farm per year.

Time range: 1-5 years of historical data.

Data size: 1MB – 10MB per farm annually.

Summary of Data Sources and Format:

Soil & Water Data: Internal IoT sensors, structured tabular format (NPK, pH, moisture, nitrate, etc.).

GHG Emissions Data: Internal sensors, structured tabular format (methane, nitrous oxide, ammonia, etc.).

Weather Data: Third-party weather APIs, structured tabular format (temperature, humidity, rainfall, etc.).

Financial Data: Internal farm records or third-party APIs, structured tabular format (input costs, yield, carbon credits).

Approximate Size:

1-10MB per farm annually for each dataset.

100,000 – 500,000 records per farm per year depending on the sensor density and time range.",Annotation pipeline is in place,FALSE,,"Evaluating the Model/System Performance:

To ensure the system delivers actionable, reliable insights for farmers and agribusinesses, we'll employ a multi-faceted evaluation strategy that includes both quantitative and qualitative metrics. The evaluation process will combine traditional performance metrics, domain-specific business rules, and continuous monitoring to ensure alignment with business goals. Here’s a detailed breakdown of how we plan to evaluate the model/system’s performance:

1. Golden Dataset & Benchmarking

Golden Dataset:

A golden dataset will be created from historical, manually validated data from select pilot farms. This dataset will serve as a ""truth"" set to compare the model’s predictions (e.g., fertilizer savings, yield forecasts, GHG reductions, carbon credits).

The golden dataset will include ground-truth data from historical soil nutrient levels, water usage, emissions data, crop yields, and market prices (for financial forecasting).

Benchmark Set:

We will also use industry benchmark datasets where available, including national soil health standards, regional fertilizer efficiency benchmarks, and carbon credit eligibility guidelines. These benchmarks will serve to assess the regulatory compliance of the model's recommendations and results.

Cross-Validation:

We will employ k-fold cross-validation on training data to assess the robustness of the model across various farm environments, soil types, and crop varieties.

Success Indicator: Model performance should match or exceed performance thresholds defined by these benchmark sets and golden datasets (e.g., 80%-90% accuracy in predictions of fertilizer savings and crop yield forecasting).

2. Human-in-the-Loop Validation

Expert Review:

In certain cases, particularly for complex predictions (e.g., emissions accounting, fertilizer optimization), agronomists or agriculture experts will manually validate the model's recommendations for individual farms. This will serve as a quality control process before recommendations are sent to farmers.

Farmer Feedback:

After deploying the model in a real-world environment, we will regularly gather feedback from farmers on the usability, accuracy, and practicality of the recommendations provided. This could include farmer surveys or direct interviews to understand how well the model's suggestions align with real-world farming practices.

Success Indicator: Positive feedback from at least 80% of participating farmers on model accuracy and usability within the first 3-6 months of deployment.

3. Heuristics and Business Rules

Business Rules Integration:

The model will be designed to incorporate specific business rules and heuristics to ensure that the recommendations make sense in the context of real-world farming economics. For example:

Cost-Benefit Analysis: The system will calculate the ROI of reducing fertilizer inputs based on a farmer’s historical yields and input costs. It will reject recommendations that result in unfeasible or negative economic outcomes.

Regulatory Compliance: For GHG emissions or fertilizer reductions, we will ensure that recommendations align with local environmental regulations (e.g., reducing nitrate runoff, adhering to carbon credit calculation methods).

Threshold-based Alerts:

The system will set thresholds for certain key metrics (e.g., soil moisture levels, emissions) that trigger alerts or notifications if exceeded. These thresholds will be based on industry standards and internal model validations.

Success Indicator: The model should consistently produce economically feasible recommendations that adhere to business rules (e.g., fertilizer savings should be in the range of 10-30% without sacrificing yield).

4. Pre/Post Comparisons & Dashboards

Pre/Post Comparison:

One of the most important indicators of success is comparing pre-deployment and post-deployment farm data. Specifically, we will compare metrics such as:

Fertilizer use: Before vs. after model adoption to see if we’ve achieved input savings.

Crop yield: Comparing actual yields vs. forecasted yields from the model.

Emissions reductions: Pre/post carbon footprint analysis to assess whether GHG reductions are being achieved.

Financial Metrics:

ROI: Calculate the return on investment (ROI) from implementing the model’s recommendations. For example, the savings from reduced fertilizer inputs should ideally exceed the cost of implementing the model, leading to a positive ROI.

Carbon Credit Revenue: If the system is successful in helping farms reduce emissions, we’ll track the number of carbon credits generated and their financial value.

Dashboards:

A real-time dashboard will be used to track key performance indicators (KPIs) such as:

Fertilizer cost vs. savings

Yield forecasting accuracy

GHG emissions levels and carbon credit eligibility

Water usage efficiency and soil health

Dashboards will also allow farm managers to make quick decisions based on current data.

Success Indicator: Positive pre/post comparisons, with at least 10-20% improvement in fertilizer use efficiency and crop yield accuracy within the first 6-12 months.

5. Quantitative & Qualitative Metrics Tracking

Quantitative Metrics:

Prediction Accuracy: For metrics like yield forecasting, fertilizer savings, and emissions reductions, we will track the mean absolute error (MAE), root mean squared error (RMSE), or R-squared values for each prediction.

Cost Savings: We will calculate and track cost savings (e.g., in fertilizer and feed) in monetary terms as a percentage of the farm’s total annual input costs.

GHG Reductions: Quantifying CO2e reductions based on fertilizer and livestock management changes.

Qualitative Metrics:

Farmer Satisfaction: Regular surveys or feedback loops with farmers to gauge how well the system is meeting their needs. Metrics such as satisfaction with recommendations and ease of use will be tracked.

Adoption Rate: The percentage of farmers who actively use and trust the system after the initial deployment period.

Success Indicator: A positive balance of quantitative (e.g., cost savings, yield improvement) and qualitative feedback (e.g., farmer satisfaction > 80%, easy adoption rate).

6. Continuous Monitoring and Retraining

Model Drift Detection:

Over time, the model will be continuously monitored for model drift (i.e., the performance degrading as farming conditions change). For example, if weather patterns shift or soil conditions change drastically, the model will need to be retrained to account for these new conditions.

Retraining Process:

The model will undergo regular retraining with new data, either periodically (e.g., quarterly) or when significant changes (e.g., weather anomalies) are detected. This ensures that the model adapts to changes in farming practices and environmental conditions.","Yes, we are considering techniques like Generative AI for forecasting agricultural outputs and LLMs for automating insights generation from farm data. Specifically, we are exploring:

GPT-based models (for text-based predictions and generating financial insights)

BERT (for extracting key patterns from large agricultural datasets)

Time-series forecasting models (for predicting commodity prices and crop yield trends)","Yes, there are several ethical, legal, and societal considerations:

Data Privacy: Ensuring that all agricultural data, especially from farms, is anonymized and does not violate any privacy regulations (e.g., GDPR, CCPA) is crucial.

Transparency: Clear communication is needed about how AI models generate forecasts and predictions, especially when making financial or environmental decisions.

Bias: We must ensure that the models do not favor specific crops, regions, or practices, which could lead to biased financial recommendations or uneven impacts on small-scale farmers.

Data Integrity: Accuracy of data inputs is critical, as errors in sensor data or model predictions could result in financial losses for farmers or incorrect sustainability reporting.

Accountability: Users should understand who is accountable for model decisions, especially when AI-driven recommendations are applied in regulatory or financial contexts.","Success for this project will be measured through the following metrics:

Quantitative Metrics:

Forecast Accuracy: Achieving at least 85% accuracy in commodity price forecasts and crop yield predictions.

Reduction in Input Costs: Demonstrating a 10-15% reduction in fertilizer and feed costs through optimized recommendations.

GHG Emissions Reduction: Quantifying at least a 10% reduction in methane (CH₄) and nitrous oxide (N₂O) emissions per farm, based on the AI-driven recommendations.

KPIs:

Farmer Adoption Rate: Targeting at least 50% adoption rate of the financial model by our current client base.

Revenue from Carbon Credits: Help farmers generate at least $1 million in carbon credit revenue annually within the first 12 months.

Model Efficiency: Reducing computational time for predictions by 30%, enabling real-time or near-real-time forecasting.

Qualitative Outcomes:

Farmer Satisfaction: Achieving an NPS score of 40+ from users of the predictive platform.

Operational Feedback: Receiving positive feedback from farmers about the usability and trustworthiness of the AI-driven insights.","Microsoft Azure, Company-hosted development environment (e.g., GitLab, JupyterHub), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases)","The ideal candidate for this project would have strong expertise in machine learning, particularly with time-series forecasting and predictive models, along with experience in agricultural data analysis, sensor data integration, and financial modeling. Knowledge of environmental data, GHG emissions, and optimization algorithms would be highly valuable.","Our technical team consists of:

2 ML Engineer: Specializes in building predictive models, particularly time-series forecasting and regression models for financial predictions based on sensor data.

3 Data Analysts: Responsible for data cleaning, transformation, and exploration. They also handle sensor data integration, quality checks, and feature engineering.

1 Product Manager: Oversees project execution, ensures alignment with business goals, and facilitates communication between the technical team and external stakeholders.","This project aligns with our broader goal of enhancing farm profitability and sustainability by leveraging advanced data analytics. We aim to provide farmers with actionable financial insights based on real-time environmental and sensor data, enabling them to make informed decisions about nutrient application, GHG emissions reduction, and carbon credit opportunities. This solution will fill a critical gap in the agricultural sector by combining cutting-edge AI models with practical, farm-scale data, directly supporting climate-smart farming practices and helping the industry transition to more sustainable operations."
Company 22,"Manufacturing operations and technical support of complex equipment face significant challenges in identifying and resolving production issues, quality defects, and equipment failures. Current root cause analysis (RCA) processes are time-consuming, require deep expertise, and often rely on manual investigation across disparate data sources. This leads to extended downtime, repeated failures, and lost production capacity.

Our target markets for this system would be manufacturing in Aerospace & Defense, Automotive, Industrial equipment and Energy & Utilities. The solution should ideally be capable of performing as an agentic reasoning system capable of accessing local or online tools but also able to be deployed locally (on-prem with up to two DGX Spark units or similar level of hardware) due to internet connectivity and security constraints.","The primary objective of this project is to transform problem solving and root cause analysis in manufacturing from a time-intensive, expert-dependent manual process into an AI-augmented system that delivers rapid, evidence-based diagnostics accessible to operators at all skill levels utilizing an reasoning, agentic system that is capable of on-premise deployment.

Accelerate Root Cause Identification: Reduce time-to-diagnosis from hours to minutes by automating initial analysis across multiple data sources and providing further instructions/recommendations for troubleshooting
Enhance Decision Support: Provide evidence-based recommendations with confidence scores and similar historical cases
Democratize Expertise: Enable less-experienced operators to perform expert-level troubleshooting through guided AI assistance
Improve Fix Effectiveness: Recommend proven solutions based on historical success rates and contextual factors","The core deliverable is a functional conversational AI assistant that frontline manufacturing workers can interact with through natural language to diagnose production incidents and receive guided troubleshooting support. The system should preferably include, as a foundation model, an open-source reasoning model capable of visual reasoning (such as NVidia Cosmos Reason1 56B or Magistral).","The primary users for this system will be manufacturing engineers, maintenance technicians, and quality control specialists.",Enhancement to an existing system,"For this project, we'll be utilizing manufacturing datasets from our partners and videos we have taken/collected. These data sets combine videos of operators performing procedures from a first-person perspective, sensors and robots, maintenance records, incident reports, QA/QC reports, machine manuals and technical data sheets on equipment. The datasets provide the diversity needed to build an AI assistant that can perform across different manufacturing environments rather than being narrowly optimized for a single facility.

The sensor and equipment data includes time-series measurements of process parameters like temperature, pressure, flow rates, vibration, speed, and power consumption, along with equipment status indicators, alarm states, and operating modes.

Maintenance records provide historical context essential for pattern recognition. These include work order histories with problem descriptions written by technicians, completed maintenance activities and parts replaced, preventive maintenance schedules and compliance records, and most valuable, unstructured technician notes capturing observations and troubleshooting steps.

This data comes in different forms, that are either structured or unstructured, multimodal or tabular. The approximate dataset size contains up to two years of maintenance records, with over 1 million of recorded error codes for machines. Which should be more than sufficient to train a robust model.",Annotation pipeline is in place,FALSE,,"We will develop a crude benchmark that may be limited in scope, consisting of “known” true resolutions and reasonable possible resolutions. We will also implement a real-world evaluation approach that combines technical performance metrics, user experience assessment, and real-world impact measurement, recognizing that our success depends on whether frontline workers actually find the AI assistant useful in practice, not just whether it achieves high accuracy on benchmarks.","Yes, we are currently using mostly the Gemini family of models, and have investigated a variety of models for various use cases. We also have a RAG system that may be ported to support an on-prem solution (since it currently utilizes an online embedding service).","This use case utilizes zero data that contains PII or has fairness concerns or implications. As a ‘debugging’ tool, the model will suggest steps that may be taken to resolve an issue, and while there may be machine bias in following these suggestions, the user will be able to see if the steps do resolve the issue.","There will be two outcomes that are measured:

1. Accuracy (% of suggestions that would correctly fix a problem).

2. False positives (unreasonable possible resolutions that are suggested that would not result in a fix)","Google Cloud Platform (GCP), Internal on-premise or virtual private servers","The ideal candidate for this project would have a good understanding of LLM concepts, RAG systems, fine-tuning approaches and computer vision. The candidate would be a master or PhD student in Engineering, with strong skills in Python and knowledge of SQL.","This project will be led by Company 22’s experienced and multidisciplinary team of 15 people, bringing deep expertise in AI, machine learning, and technical industry applications. Company 22’s core team includes engineers, data scientists, and project managers from top institutions such as Stanford, UC Berkeley, and the University of Toronto, ensuring world-class technical and operational execution. 

The team is led by CEO, an AI veteran with nearly two decades of experience in building successful AI-driven solutions, including founding and leading startups acquired by global tech giants. Our CTO, holds a PhD in Natural Language Processing (NLP) with multiple peer-reviewed research papers on AI and award-winning AI innovations.",
Company 23,"About Company 23's platform: Company 23's platform is an AI career navigation platform that helps people make better, faster career decisions. It unifies skills discovery, job matching, resume/cover-letter support, interview prep, and learning guidance into one conversational experience. We solve two chronic problems: (1) career services and employers can’t personalize guidance at scale; (2) learners lack a clear, adaptive path from goal → skill gap → practice → proof.

The opportunity. With Vector’s MLA support, we will build an agentic micro-learning engine that generates personalized, outcomes-aligned learning “sprints” from a learner’s career goal and skill gaps. Each sprint assembles bite-sized lessons with flashcards, audio snippets, quizzes, and practical tasks, then adapts based on performance and feedback. Content will be created and orchestrated by AI using a mixture of vetted web sources, licensed materials, and model knowledge, with safeguards for privacy, copyright, and safety.

Industry & audience. We operate at the intersection of workforce development, edtech, and HR tech. Primary customers: career hubs (postsecondary, newcomer/settlement and employment agencies), training providers, and employers; end users: students, newcomers, career changers, and displaced workers who need rapid, practical upskilling.

Market context. Canada faces simultaneous skills shortages and skills mismatches while services are capacity-constrained. Organizations are under pressure to demonstrate measurable outcomes and comply with PIPEDA/GDPR-style privacy, accessibility standards, and AI governance expectations. Generative/agentic AI now makes adaptive, mastery-based learning viable at population scale—if we can make it accurate, aligned, and safe.

What we’ll build with MLA.

An agentic workflow that maps goals → competencies → curated micro-content → assessments → mastery signals, with automatic course packaging (cards, audio, quizzes).

Evaluation loops (RLHF/RLAIF): capture learner and practitioner feedback to fine-tune prompts/policies and improve relevance, cultural fit, and accessibility.

Measurement & telemetry: item-level learning analytics, pre/post skill delta, engagement, and employability proxies (portfolio artifacts, interview pass-throughs).

Guardrails: source attribution, deduplication, bias checks, and IP filters; privacy by design (data minimization, 2FA, encryption in transit/at rest).

Business impact. For institutions and employers: lower cost per learner served, higher placement/readiness, and new evidence for funding and QA. For individuals: shorter time-to-competency, clearer paths to in-demand roles, and tangible artifacts that signal skills to employers. The MLA placement accelerates the science and engineering needed to move from static content to continuously improving, personalized learning at scale.","Key Objectives, Success Milestones, and Importance

Objectives (what this project will achieve)

Automate personalization at scale: Convert user goals and skill gaps into adaptive micro-courses (lessons, flashcards, audio, quizzes) with minimal human effort.

Close the loop on learning efficacy: Instrument pre/post assessment and mastery signals so courses adapt in real time and continuously improve via RLHF/RLAIF.

Enhance practitioner decision support: Surface clear readiness scores, top gaps, and suggested interventions to free practitioner capacity.

Harden for trust & compliance: Bake in privacy-by-design, source attribution/IP filters, accessibility, and auditability suitable for public-sector/postsecondary use.

Launch a GTM-ready capability: Package the micro-learning engine as a deployable Company 23's platform module with APIs, partner playbooks, and pricing.

Success Milestones (high level)

Phase 1 — Foundation & Prototype: Goal→skills mapping, automated course assembly, basic adaptivity and telemetry proven in internal trials.

Phase 2 — Pilot & Validation: Limited partner rollout demonstrating meaningful learning gains, practitioner time savings, and positive user/practitioner satisfaction.

Phase 3 — Scale & Governance: Production hardening (security, IP controls, accessibility), partner implementation toolkit, and initial revenue-bearing deployments.

Importance: consequences if delayed

For Company 23's platform

Missed market window: Competitors ship agentic learning first; harder and costlier to differentiate later.

Partner momentum stalls: Current interest from institutions/employers loses steam without a tangible, validated module.

Higher unit costs: Manual content design persists; cost-per-learner remains too high to scale sustainably.

Fragmented product narrative: Company 23's platform remains guidance-heavy without the proof of learning impact funders increasingly require.

Technical debt accrues: Ad-hoc content features proliferate without a unified adaptive/feedback architecture.

For learners, practitioners, and the ecosystem

Capacity crunch persists: Career services can’t personalize at scale; longer waitlists and lower completion.

Equity gap widens: Youth, newcomers, and displaced workers wait longer for relevant upskilling tied to in-demand roles.

Outcomes plateau: Fewer measurable skill gains and weaker employability signals (artifacts, badges, interview readiness).

Duplicated effort & higher public spend: Agencies keep commissioning static content rather than leveraging reusable, adaptive modules.

Slower workforce adaptation: Employers face prolonged skill mismatches, dampening productivity and growth.

This project moves Company 23's platform—and our partners—from static guidance to trusted, adaptive learning at scale, improving outcomes while reducing delivery costs.","Expected Outputs & Deliverables (within MLA cohort scope)

1) Functional prototype (end-to-end)

An agentic micro-learning flow that takes a user’s goal + skill gaps and auto-generates a short course “sprint” (5–10 lessons) with flashcards, audio (TTS), and quizzes.

In-product experience for a learner to take the sprint (cards → practice → quiz), plus basic adaptivity (easier/harder items based on performance).

2) Developer-ready APIs & backend services

Course Assembly API: input (goals, skills, gaps) → output (lesson objects, flashcards, quiz items, audio asset refs).

Assessment API: pre/post quiz generation, scoring, and item metadata.

Telemetry API: capture events (time on task, attempts, confidence ratings) for analytics and RLHF.

Content Validation API: source attribution, duplication checks, and IP/quality filters.

3) Practitioner & admin dashboards (v1)

Practitioner view: top gaps, readiness score, assigned sprints, learner progress, suggested interventions.

Admin view: content/source audit, safety flags, usage analytics, and configuration of guardrails.

4) Evaluation & RLHF/RLAIF harness

Feedback widgets (thumbs up/down + rationale) tied to items and lessons.

Data pipeline to aggregate signals and trigger prompt/policy updates; side-by-side A/B evaluation UI for human reviewers.

Metrics bundle: pre→post deltas, completion, satisfaction, flagged-content rate.

5) Trust, safety, and governance artifacts

Privacy-by-design controls (PII redaction, role-based access), source logs, IP filters, accessibility checks.

Red-team test set and automated checks for factuality, bias, and prohibited content categories.

6) Reusable content objects & schemas

JSON schemas for lessons, flashcards, quizzes, hints, and rubrics.

Templated, brand-ready flashcard and quiz renderers; audio generation pipeline with storage references.

7) Integration kit for Company 23's platform

Adapter layer to plug the micro-learning module into existing Company 23's platform user profiles and skills models.

Webhooks/events for “assign sprint,” “complete sprint,” and “update readiness.”

8) Documentation & handoff

System architecture and deployment guide; API reference; runbooks for monitoring and incident response.

GTM-support packet: pilot playbook, sample KPIs dashboard, and a short demo script/video.

What the MLAs will help deliver

Lead the design and implementation of the course assembly, assessment, telemetry, and validation APIs.

Build the evaluation/RLHF harness and the A/B review UI.

Stand up the first functional prototype (learner flow + practitioner/admin views) and baseline guardrails.

Produce the schemas, docs, and runbooks needed for Company 23's platform to maintain and scale the module after the cohort.","Primary users include: (1) learners—students, newcomers, career changers, and displaced workers who complete adaptive micro-courses (lessons, flashcards, audio, quizzes) to close job-relevant skill gaps; (2) career practitioners/coaches at postsecondary and employment agencies who review diagnostics, assign learning sprints, and monitor progress; (3) program managers/administrators at partner institutions who oversee outcomes, configure guardrails/content sources, and report KPIs; (4) select employer partners who consume learner artifacts (badges, assessments, work samples) as signals of readiness; and (5) Company 23’s product/ML engineering team, which integrates the module, maintains telemetry, and operates the RLHF/RLAIF improvement loop and safety/compliance controls.",Enhancement to an existing system,"Datasets

1) Company 23's platform user profiles & goals (current, internal)

What/Relevance: Pseudonymous learner profiles with target roles, declared skills, experience summaries—seed for goal→skills→content mapping.

Source: User-generated in Company 23's platform.

Collection/Format: Onboarding forms + chat; structured JSON.

Key fields: user_id, target_role(s), industries, seniority, declared skills, languages, learning constraints.

Approx. size/time range: ~1,200 learner profiles; 2024–2025; ~50–150 KB/profile.

2) Interaction & learning telemetry (current, internal)

What/Relevance: Session events used for adaptivity and efficacy analytics.

Source: App instrumentation.

Collection/Format: Structured time-series (JSON/Parquet).

Key fields: session_id, event_type (view/answer/hint/retry), timestamps, dwell time, confidence, outcome.

Approx. size/time range: ~200k–500k events across 5k–10k sessions; 2024–2025; ~3–8 GB compressed.

3) Conversational transcripts (current, internal)

What/Relevance: Anonymized user–assistant turns about roles/skills; supports content assembly and human evaluation.

Source: Company 23's platform logs.

Collection/Format: Text with role/turn metadata (JSON).

Key fields: turn_id, prompt_text, model_output, topic tags, safety flags.

Approx. size/time range: ~20k–40k turns; 1–3 GB; 2024–2025.

4) Skills/occupational taxonomy (current, internal + public)

What/Relevance: Canonical roles/skills for gap analysis and tagging.

Source: Public frameworks (e.g., ESCO/O*NET) plus our internal mappings.

Collection/Format: Tables/JSON graphs.

Key fields: occupation_code, skill_id, skill_name, related_skills, proficiency levels.

Approx. size: ~50k–100k records; ~300–600 MB.

5) Labour-market demand data (current, partner-provided)

What/Relevance: Skill/role demand signals by region to prioritize learning content and keep recommendations current.

Source: Licensed partner feed (e.g., Vicinity Jobs).

Collection/Format: API/feed; CSV/JSON snapshots.

Key fields: role title, NOC/ONET code, skill frequency, employer demand, location, trend indices.

Approx. size/time range: Rolling 12–24 months; ~100–500 MB for the cohort slice.

6) Assessment data & item bank (to be generated during cohort)

What/Relevance: Pre/post quizzes, item metadata, and learner results to measure gains and drive adaptivity.

Source: Model-generated items curated by staff; learner completions.

Collection/Format: Structured JSON (items/results); CSV exports.

Key fields: item_id, skill_tag, difficulty, distractors, attempts, score, time-to-answer.

Approx. size (target): 3k–6k items; results across 1k–3k assessments; ~0.5–1 GB.

7) Learning content corpus (to curate/expand)

What/Relevance: Short readings, definitions, examples to assemble lessons/flashcards/quizzes.

Source: Curated open-education resources and allow-listed web pages; select licensed micro-content.

Collection/Format: Text/HTML + metadata; embeddings for retrieval.

Key fields: source_id/url, license, topic/skill tags, reading level, snippet text, embedding_ref.

Approx. size (target): 10k–20k documents/snippets; ~5–10 GB text + embeddings.

Potential new datasets (to be confirmed during discovery)

We may identify additional needs during the cohort; examples include:

Sector-specific competency mappings (e.g., healthcare, advanced manufacturing) and deeper Canadian NOC alignments.

Accessibility/readability annotations (reading-level datasets, alt-text patterns) to improve inclusive content generation.

Licensed micro-content libraries (short videos, interactive items) where open content is insufficient.

Safety/red-team test sets tailored to our domains (IP/attribution checks, PII patterns, hallucination/bias probes).

Outcome signals from employer partners (e.g., internship screening results) to strengthen external validation, where feasible and consented.

Governance (applies across datasets): Pseudonymization by default, data minimization, opt-in research consent, encryption in transit/at rest, role-based access, audit logs; strict IP attribution and license compliance for external/partner content.",Raw/unprocessed,FALSE,,"To ensure the agentic micro-learning module is both effective and trustworthy, we’ll use a layered evaluation strategy that combines learning-science measures (pre/post gains), human review, small “golden” benchmark sets, product telemetry, and safety/IP guardrails. Success = measurable skill improvement for learners and reduced practitioner effort—without compromising privacy, accessibility, or attribution. We’ll lean on MLA guidance for benchmark design, difficulty calibration, and label quality to keep the approach statistically sound, equitable, and scalable.

Evaluation Plan

1) Golden sets & offline benchmarks (with MLA guidance)
Create small, versioned “golden” sets for 8–12 priority roles/skills (e.g., junior data analyst, customer success). Each includes source snippets, expected lesson/flashcard/quiz outputs, and common failure cases. Metrics: rubric scores (clarity, correctness, level fit, accessibility), semantic similarity to references, and citation checks.

2) Human-in-the-loop validation
Practitioners and internal reviewers rate generated items (1–5) on accuracy, level match, sector fit, usefulness; flag IP/attribution issues. Side-by-side A/B reviews of prompt/model/policy variants; track inter-rater agreement and sampling coverage.

3) Pre/post learning efficacy
Auto-generated pre/post quizzes aligned to the same objectives. Metrics: mean score delta (target ≥15%), effect size (Cohen’s d), two-week retention checks, and transfer proxies (short applied tasks). MLA input requested on blueprinting and light IRT calibration.

4) Personalization & adaptivity quality
Telemetry-based indicators: reduction in off-level items (target ≤10%), time-to-mastery curves, hint usage, abandon rates, and “stuck” loops. Segment by cohort (youth, newcomers, career changers) and by skill family to ensure equity.

5) Safety, privacy & IP guardrails
Maintain a red-team test suite for hallucinations, PII leakage, unsafe content, and license/attribution compliance. Track flagged-content rate (≤1%), time-to-remediation SLA, and zero-tolerance categories (0 incidents). Combine rule-based checks (regex/patterns) with model-based classifiers.

6) Business & operational KPIs
Time to generate a course sprint (<2 minutes, automated), practitioner time saved on plan creation (≥30%), course completion (≥70%), CSAT/effort scores (≥4.3/5, ≤2.5 CES), active learners per site, and cost-per-learner. Exposed via a live partner dashboard.

7) Heuristics & business rules
Minimum/maximum reading levels per cohort; objective coverage checks before publish; required citations for definitional content; automatic blocks if reviewer scores fall below threshold or attribution is missing.

8) Online experiments (as traffic permits)
Gradual rollouts with A/B/n tests on prompts/models/policies. Primary outcomes: score delta uplift, completion, satisfaction; use sequential testing or CUPED-style variance reduction. Guardrails to prevent regressions on safety/IP metrics.

9) Continuous improvement & governance
Weekly eval run on golden sets plus sampled live items; regression reports; change-control that promotes only variants beating baselines on efficacy, safety, and cost. Versioned prompts, datasets, and metrics for auditability and reproducibility.","GPT-4o/4.1 (JSON-mode, function calling) for planning and structured generation; evaluated Claude 3.5 Sonnet/Haiku for long-context reasoning and safer drafting; and tested open models (e.g., Llama 3.1-70B, Mistral Large/Mixtral) behind a RAG layer when data residency/cost control are priorities. For retrieval we use pgvector/Supabase and Pinecone with text-embedding-3-large (and are evaluating Cohere/Voyage embeddings) plus a reranker (e.g., Cohere Rerank-3). The planned agentic pipeline will use multi-agent orchestration (LangGraph/LangChain): (1) Curriculum Planner → (2) Retriever (allow-listed/OER + partner LMI) → (3) Lesson/Flashcard/Quiz Generator (strict JSON schemas) → (4) Validator/Guardrails (citation/IP/reading-level/PII checks) → (5) Assessor/Adaptive Tuner (updates difficulty), all observable via LangSmith-style traces. For audio we’ll attach TTS (e.g., Azure Neural TTS/ElevenLabs) and optional ASR for spoken responses. Fine-tuning is scoped to light preference optimization (RLHF/RLAIF/DPO on our labels) rather than full model retrains; most gains will come from prompting, constrained decoding, tool use, and RAG. Hosting will be a mix of Azure OpenAI/Anthropic APIs and self/managed OSS where required for privacy, with policy enforcement (PII redaction, license filters) at the middleware layer. Overall maturity: production LLM features in Company 23's platform today; the agentic micro-learning system is at design/prototype stage, and the MLA cohort will help us harden retrieval, validation, and adaptive policies while selecting the best model mix for accuracy, cost, and compliance.","Yes—there are important ethical, legal, and societal considerations, and we’re designing the system to meet them.

Fairness & bias: Risk that content, assessments, or recommendations skew by gender, ethnicity, disability, immigration status, or language. Mitigations: allow-listed sources and sector-relevant OER; reviewer rubrics that score for bias/cultural fit; golden-set checks across user segments; difficulty/readability controls; ongoing disparate-impact monitoring (outcomes by cohort) with corrective actions.

Privacy & consent (PIPEDA/GDPR-aligned): We process personal data and learning telemetry. Mitigations: data minimization; opt-in consent for research; pseudonymization by default; encryption in transit/at rest; role-based access and audit logs; retention limits; configurable Canadian data residency via vendors where required.

Transparency & explainability: Users and practitioners must understand why a course, item, or recommendation was generated. Mitigations: “Why this content?” disclosures; visible source attributions and licenses; model/version badges; practitioner-readable rationales for skill-gap and difficulty choices.

IP & licensing: Assembling lessons from web content risks copyright violations. Mitigations: crawl from allow-listed, licensed, or open sources only; store license metadata; mandatory attribution; automated duplication/citation checks; hard blocks on unlicensed text in published items.

Accessibility & inclusion (WCAG/AODA): Risk of excluding users with disabilities or lower literacy levels. Mitigations: WCAG 2.2 AA targets; TTS/audio alternatives; adjustable reading levels; captions/alt text; keyboard navigation; multilingual support in prioritized languages.

Autonomy boundaries & user control: Agentic flows could act beyond user intent. Mitigations: explicit user approval gates for key actions; reversible changes; rate limits; event logs; easy “pause/undo”; no external communications or data writes without consent.

Human-in-the-loop & accountability: Over-automation can reduce trust and miss context. Mitigations: practitioners can review/override content, set guardrails, and flag issues; escalation paths; clear ownership for resolving safety/IP flags.

Evaluation integrity & misuse: Generative tools could enable shortcutting or cheating. Mitigations: assessment variation pools; integrity checks (time-to-answer, item rotations); guidance that distinguishes practice aids vs. graded tasks; practitioner dashboards to spot anomalies.

Youth & vulnerable populations: Additional safeguarding for minors and at-risk learners. Mitigations: age-appropriate filters, crisis-content safety checks, and referral guidance to human support when needed.

Security & reliability: Model or pipeline failures can produce harmful or incorrect outputs. Mitigations: staged deployment, red-team test suites (hallucinations/PII/bias), automated guards with human review, incident response runbooks, and rollback policies.

Commitment: We will document these controls (data flows, model cards, evaluation results), measure outcomes by cohort for equity, and involve practitioners in continuous review. Where trade-offs arise (e.g., personalization vs. privacy), we will default to user safety, consent, and transparency.","Learning efficacy: Measure pre→post quiz gains per sprint (baseline TBD during pilot setup; target ≥15% mean score delta, Cohen’s d ≥0.5), 2-week retention quiz (baseline TBD; target ≥60% correct), and transfer proxies via short applied tasks (target ≥70% pass).
Personalization quality: Off-level item rate (baseline ~25–35% from current non-adaptive flows; target ≤10%); content relevance/fit ratings from learners/practitioners (baseline ~4.0/5; target ≥4.3/5); time-to-mastery curves improving across cohorts.
Practitioner efficiency: Time to create a learning plan/micro-course (baseline ~45–60 min manually; target ≤5 min automated, ≤2 min operator time); perceived time saved (target ≥30%) and plan revision rate down (target −25%).
Adoption & engagement: Sprint completion rate (baseline ~45–55% for comparable learning tasks; target ≥70%); active learners per site (target 150–300 in pilot), repeat sprint uptake (target ≥40%), weekly active practitioners (target ≥70% of assigned staff).
Safety, IP, and privacy: Flagged-content rate (baseline TBD; target ≤1% with 100% remediated within SLA), zero known IP/license violations in sampled outputs, privacy test suite pass rate ≥95%, accessibility checks (WCAG 2.2 AA) ≥95% pass.
Equity & inclusion: Outcome parity across key cohorts (youth, newcomers, career changers): gap in pre→post gains ≤5 pts; differential completion ≤7 pts; reading-level alignment ≥90%.
Cost & performance: Automated course generation time (target <2 min end-to-end), average inference cost per sprint (baseline TBD; target −30% vs. naive generation), system reliability ≥99.5% successful run rate.
Qualitative outcomes: Practitioner interviews indicating higher confidence in recommendations; learner comments citing clearer pathways and “right level” difficulty; partner testimonials suitable for case studies.
Process & governance: Weekly evaluation runs completed ≥90% of weeks; regression gates prevent shipping models that fail efficacy/safety thresholds; audit trail completeness ≥99%.

Baseline notes: For measures marked “TBD,” we will capture a two-week baseline during Phase 1 using existing non-adaptive content flows and early micro-course prototypes, then lock targets for the remainder of the cohort.","Google Cloud Platform (GCP), Internal on-premise or virtual private servers, Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases)","The ideal candidate for this project would be a hands-on ML/LLM engineer with strong experience in prompt engineering, agentic workflow orchestration (e.g., LangGraph/LangChain), retrieval/RAG and reranking, evaluation design (golden sets, A/B tests), and light preference optimization (RLHF/RLAIF/DPO), comfortable building production-grade Python backends and APIs with vector databases and embeddings. Bonus points for familiarity with learning-science/assessment design, safety & governance (privacy/IP/accessibility), and integrating TTS/ASR, telemetry dashboards, and guardrails into end-to-end systems.","Technical team: A lean, cross-functional group of ~6–7 people (mix of core staff and contractors).

Product & Delivery: 1 Product Manager (Company 23's platform lead), 1 Project/Program Coordinator.

Engineering: 2 Full-Stack Engineers (TypeScript/React, Node/Python), 1 Back-end Engineer.

ML/Data: 1 ML/LLM Engineer (prompting, RAG, evaluation)

Design & QA: 1 UX/UI Designer (accessibility, WCAG) and 1 QA Automation Contractor.

Security/Compliance (fractional): 1 advisor supporting privacy-by-design, IP/licensing workflows, and readiness for ISO/SOC controls.

The team has shipped production LLM features in Company 23's platform (RAG, Agents, etc.) and can provide some support to the MLA  engineers. We do have gaps on true ML Engineers though as our current team is largely learned this as we build Company 23's platform and need help in particular on observability, evals, data pipelines, RLHF, and agentic workflows and frameworks.","Accomplishments to date: Company 23's platform has moved from concept to production with core LLM features for job/skill discovery, resume/cover letters, and interview prep. Through our FSC-backed pilots we’ve served 1,200+ learners, 50+ practitioners, and 15 partner organizations, integrating labour-market signals and skills taxonomies to deliver personalized guidance at scale. We’ve also shipped practitioner views (gap diagnostics, plan suggestions) and baseline telemetry for learning analytics, laying the groundwork for adaptive micro-learning.

Support & partnerships. We’ve received just under $1M in non-dilutive funding and partnered with ecosystem leaders (e.g., Magnet, CCDF, Ontario Chamber of Commerce, Serco and an LMI partner for Canadian demand data). These collaborations have provided access to priority pilot sites, domain expertise, and governance alignment (privacy, accessibility, and IP practices) that de-risk broader rollout.

User & practitioner feedback. Early pilots indicate strong product-market fit signals: job seekers/learners report greater career clarity and confidence, discover new role pathways, and value the actionable next steps. Practitioners highlight meaningful time savings potential on plan creation and better visibility into top skill gaps, enabling more time for high-impact coaching. Among requested enhancements—adaptive, bite-sized learning, clearer progress/mastery signals, and exportable artifacts for employers—directly inform this MLA project’s focus on agentic micro-learning and measurable outcomes.

We’re energized by the traction so far—and this MLA partnership is the next step toward delivering truly personalized, equitable career navigation at scale."
Company 24,"Company 24s tackles the lack of high-quality, family-friendly, and culturally authentic film content for the 1.9 billion Muslims and values-aligned families worldwide. Existing platforms either promote unsafe mass content or low-budget niche alternatives. Company 24s bridges this gap using AI to power script-to-teaser generation, multilingual voice-preserving dubbing, and values-based content moderation—helping creators develop, validate, and fund films faster and more affordably.

Operating at the intersection of AI, media, and community finance, Company 24s enables creators to attract investment through compliant equity crowdfunding while giving audiences a voice in what gets produced. The global streaming market exceeds $500 billion, with the MENA region and Muslim diaspora showing high digital engagement but little culturally aligned content. By blending ethical AI innovation with scalable storytelling, Company 24s creates a new creative economy that empowers creators, investors, and audiences—redefining how authentic stories are developed, localized, and distributed globally.","Company 24s’ key objective is to build an AI-powered platform that democratizes film development, funding, and distribution for family-friendly, culturally authentic content. The solution automates and enhances traditionally manual, expensive, and biased creative processes—using AI for script-to-teaser generation, voice-preserving multilingual dubbing, and values-based content moderation. These tools drastically reduce time and cost from concept to validation while ensuring cultural and ethical integrity.

Success milestones include:

MVP launch of the AI Script→Teaser tool (validate 500+ creator projects).

Public beta of the streaming and voting platform with 10–15 greenlit films.

Full deployment of multilingual dubbing across 10+ languages.

100K+ active users and global partnerships with film funds and cultural organizations.

If this project is delayed, the gap in safe, values-aligned entertainment will continue to widen—leaving 1.9B Muslims underserved, creators underfunded, and audiences reliant on misaligned or low-quality content. Traditional studios will retain control over cultural narratives, and Canada will miss the opportunity to lead in ethical AI media innovation, a fast-emerging sector blending creative technology, community engagement, and responsible AI use.","During the Vector Institute cohort, Company 24s aims to deliver core AI and ML-powered prototypes that form the foundation of its creative and ethical media platform. The key deliverables include:

AI Script→Teaser Generator (Functional Prototype):
A pipeline that converts written scripts or loglines into short cinematic teasers (60–120 seconds) using text-to-video diffusion models, scene parsing, and emotion-aware prompt engineering. The MLA team will help optimize the model architecture, automate scene extraction, and improve visual and tonal consistency across genres.

Multilingual Voice-Preserving Dubbing API:
A speech-to-speech translation and dubbing system that maintains the actor’s original tone and emotional range across 10+ languages (Arabic, Urdu, Turkish, Bahasa, etc.). MLAs will support fine-tuning of voice embeddings, synchronization, and latency optimization for real-time localization.

Values-Based Content Moderation Model (Internal Tool):
A multimodal AI classifier that identifies inappropriate or non-compliant visual, text, and audio elements aligned with Company 24s’ ethical framework. Deliverables include a prototype moderation dashboard and human-in-the-loop feedback workflow.

Creator Analytics Dashboard:
A data visualization tool to track engagement, teaser performance, and voting insights.

Together, these outputs will establish Company 24s’ foundational AI content pipeline and moderation infrastructure, enabling scalable, ethical, and globally inclusive media production.","The primary users of Company 24s’ solution are both external creators and audience participants, supported by key internal teams managing the AI workflows.  External Users:  Creators (writers, filmmakers, producers): Use the AI Script→Teaser Generator to visualize story concepts, generate mood boards, and attract investors or community votes without expensive pre-production costs.  Audiences and Investors: Engage with AI-generated teasers, vote on projects they love, and participate in equity crowdfunding campaigns. They also experience multilingual, voice-preserved dubbing and personalized content recommendations powered by AI.  Internal Users:  Company 24s AI & Product Team: Utilize the moderation and dubbing pipelines to automate quality control, optimize workflows, and ensure cultural alignment.  Content Moderation & Scholar Review Teams: Leverage the AI ethics dashboard to monitor and validate that all produced content adheres to family-safe and Islamic values.  By integrating creators, audiences, and internal curators into one ecosystem, Company 24s ensures that AI innovation directly empowers the people shaping and consuming global, values-driven entertainment.",Enhancement to an existing system,"a. Dataset Description

Content & Relevance:
Company 24s will use a combination of text, audio, and visual datasets to train and validate its AI tools for script analysis, teaser generation, multilingual dubbing, and content moderation. These datasets are central to building AI systems that understand narrative structure, emotion, tone, and cultural context—enabling the creation of cinematic-quality previews, accurate voice translations, and ethically aligned content evaluation.

Sources:
Data originates from multiple channels:

Licensed third-party script libraries (film, TV, and animation screenplays).

Internally created short-form scripts and storyboards from Company 24s’ development team.

Publicly available multilingual speech corpora (e.g., Mozilla Common Voice, VoxPopuli, OpenSLR) and in-house audio recordings for dubbing model fine-tuning.

Open-source moderation datasets supplemented with internally labeled examples aligned with Company 24s’ Red/Amber/Green Islamic values taxonomy.

Collection & Format:
Data is a mix of structured metadata (genre, emotion, duration) and unstructured text/audio/video. Script data is stored as text and annotated JSON files; audio data as WAV/FLAC; visual data as MP4 and PNG frames.

Key Features/Fields:

Scripts: title, genre, scene description, emotional tone, character metadata.

Audio: language, speaker ID, transcription, emotional embedding.

Moderation: label, risk score, flagged keywords, visual tags.

Approximate Size:
~10,000 annotated scripts; ~1,500 hours of multilingual speech; ~50,000 moderation-labeled assets. Dataset size ≈ 4–5 TB, continuously expanding through new creator submissions.","The dataset for this project is currently in the planning and pre-collection stage. Company 24s is designing a multimodal dataset (text, audio, and visual) to train and validate its AI systems for script-to-teaser generation, multilingual dubbing, and content moderation.  We have defined the data architecture, collection strategy, and labeling framework but have not yet assembled the full dataset. Initial work will focus on sourcing publicly available, ethically licensed corpora (e.g., open film scripts, multilingual speech datasets such as Common Voice or VoxPopuli) and supplementing them with original user-generated content from early creators during the prototype phase.  The dataset will consist of:  Scripts and synopses for training language and scene-parsing models.  Multilingual voice recordings for speech-to-speech translation and dubbing.  Curated samples for developing Company 24s’ “Red/Amber/Green” content moderation taxonomy.  All data collection will follow strict ethical, copyright, and privacy standards, with proper consent and anonymization. During the Vector cohort, we aim to establish the foundational data ingestion, preprocessing, and annotation pipeline to enable future model training.",FALSE,,"Company 24s will evaluate model performance using a combination of quantitative benchmarks, human validation, and production-level feedback loops tailored to each AI component.

Script→Teaser Generation:

Quantitative: Evaluate text-to-video coherence using metrics such as BLEU and CLIPScore to measure semantic alignment between script and generated visuals.

Qualitative: Human-in-the-loop review by creative directors and test audiences to assess narrative flow, emotional accuracy, and cinematic quality.

Multilingual Dubbing & Voice Preservation:

Quantitative: Use Word Error Rate (WER), Mean Opinion Score (MOS), and lip-sync accuracy to benchmark translation fidelity and naturalness.

Qualitative: Linguists and native speakers evaluate cultural nuance, pronunciation, and tone preservation.

Values-Based Moderation Model:

Quantitative: Precision, recall, and F1-score tested against a manually labeled validation set (“golden dataset”) of Red/Amber/Green content categories.

Qualitative: Review by scholars and ethics advisors to ensure cultural and moral sensitivity.

System-Level Metrics:

User engagement dashboards (click-through rates, completion rates, voting participation).

Pre/post comparisons of manual vs. AI-assisted creative timelines to quantify productivity gains.

Together, these approaches ensure a balanced evaluation framework that measures technical accuracy, creative quality, and ethical alignment before large-scale deployment.","Yes. Company 24s’ platform fundamentally relies on Generative AI and Large Language Models (LLMs). For the Script→Teaser pipeline, we are exploring LLMs such as GPT-4 Turbo, Llama 3, and Mistral 7B Instruct for story summarization, emotional tagging, and scene-to-prompt generation feeding into text-to-video diffusion models (e.g., Sora, Runway Gen-3, Stable Video Diffusion).
For Multilingual Dubbing, we plan to use Whisper, SeamlessM4T, and OpenVoice for speech recognition, translation, and tone-preserving voice cloning.
For Values-Based Moderation, we are designing fine-tuned classifiers over multimodal transformer backbones such as CLIP, BLIP-2, and LLaVA to align content with Islamic and family-safe principles.
All models are orchestrated within a human-in-the-loop workflow to ensure narrative, linguistic, and cultural integrity.","Yes. Ethical, legal, and societal considerations are central to Company 24s’ design. We focus on fairness, cultural sensitivity, and transparency in Generative AI workflows. Models are trained and validated using diverse, multilingual datasets to minimize cultural or gender bias in storytelling and dubbing.
All content passes through a “Values & Fairness moderation layer” combining AI and human reviewers to ensure alignment with family-safe and non-discriminatory principles.
We implement data-privacy safeguards (GDPR, COPPA, PIPEDA compliant), explicit creator consent, and traceable content provenance to prevent misuse of generated media.
To build user trust, all AI-assisted outputs are labeled as such, and creators retain final editorial control. Company 24s also maintains an Ethics Advisory Board for oversight of cultural, legal, and religious sensitivities.","Success will be evaluated through a balanced framework of technical performance, creator adoption, and ethical impact metrics.

1. AI Model KPIs
– Script→Teaser Generation: BLEU ≥ 0.6, CLIPScore ≥ 0.8, and ≥ 80% approval in human creative review.
– Multilingual Dubbing: Word Error Rate (WER) ≤ 10%, Mean Opinion Score (MOS) ≥ 4.2, and ≥ 90% lip-sync accuracy.
– Values Moderation: Precision/Recall/F1 ≥ 0.9 on red/amber/green classification using a “golden” validation dataset.
– Bias & Interpretability: Continuous fairness evaluation using gender and language-balance metrics; SHAP-based explainability for moderation outputs.

2. Platform & User Metrics
– 500+ creators onboarded within 12 months; >10,000 community votes on greenlighted projects; ≥5 AI-assisted films funded through the Company 24s ecosystem.
– 40% reduction in script-to-pitch development time and 25% faster dubbing turnaround compared to manual workflows.
– Creator satisfaction ≥85%, Viewer trust and transparency score ≥80%.

3. Qualitative Impact
– Demonstrated ethical use of generative AI in culturally aligned, family-safe storytelling.
– Peer-reviewed case studies shared with Vector to contribute to research on multimodal fairness, AI explainability, and cross-lingual empathy in media generation.","Google Cloud Platform (GCP), Amazon Web Services (AWS), Company-hosted development environment (e.g., GitLab, JupyterHub), Vector-provided infrastructure (if applicable), Open-source or public tools (e.g., Colab, Hugging Face, Weights & Biases)","The ideal candidate for this project would have strong experience in multimodal AI development, particularly with Generative AI, LLMs, and text-to-video or speech-to-speech models, and a solid understanding of data curation, NLP, and multilingual model fine-tuning. Familiarity with Python, PyTorch, Hugging Face, and cloud-based ML pipelines (GCP/AWS), along with sensitivity to ethical AI, cultural bias mitigation, and human-in-the-loop systems, would be highly valuable.","Company 24s does not currently have a dedicated in-house technical team. The project is led by a group of product managers and creative producers who oversee workflow design, data collection strategy, and AI use-case definition. The team’s focus is on bridging creative and technical goals — defining requirements, curating datasets, and coordinating with external AI engineers and research partners (including Vector mentors) for implementation. This structure allows Company 24s to maintain strong creative direction and domain expertise while leveraging the Vector program’s technical mentorship and infrastructure to execute the AI components effectively.","Company 24s is building an AI-powered platform that merges creative storytelling with responsible technology. The project bridges the gap between global creators and family-friendly audiences by combining LLM-driven script analysis, AI-generated teasers, and voice-preserving multilingual dubbing. Although our current team is creative-led, we have deep production expertise and access to technical partners through SP Studios’ AI R&D network. Support from the Vector Institute would enable us to prototype our generative pipeline responsibly, establish best practices in ethical media AI, and contribute applied research on multimodal fairness and cultural alignment in AI-driven entertainment."
